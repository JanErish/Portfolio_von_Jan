{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create toggle for hiding or displaying raw code\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hiding error warnings\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hide warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"desc\"></a>\n",
    "\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>Part II: Data Preprocessing</tt></span></h1>\n",
    "</center>\n",
    "\n",
    "![Imgur](https://i.imgur.com/WADp795.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>EXTREME GRADIENT BOOSTING PIPELINE</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "![Imgur](https://i.imgur.com/dwEyicL.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:6px;font-family:Times New Roman,Times,serif\"><tt>image: Shutterstock</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Jan Erish Baluca\n",
    "[**LinkedIn**](https://www.linkedin.com/in/jan-erish-baluca-099569103/)  \n",
    "[**Portfolio on Github**](https://github.com/JanErish/Portfolio_von_Jan)  \n",
    "\n",
    "**Data**:  \n",
    "* Credit card transactions in September 2013 by european card holders that occured in two days.  \n",
    "* All the features are numeric as a result of PCA (Principal Component Analysis) transformations (except 'Time' and 'Amount').  \n",
    "- _Due to confidentiality issues, the original features and background information are not available._  \n",
    "- **'Class'** is the target variable. The value **1** is for cases of fraud and **0** for otherwise.  \n",
    "\n",
    "[_Dataset Source: Kaggle_](https://www.kaggle.com/mlg-ulb/creditcardfraud/data)\n",
    "\n",
    "# Table of Contents\n",
    "## Part 2: Extreme Gradient Boosting: Preprocessing Pipeline   \n",
    "1. [Imputation and Encoding](#impute)  \n",
    "2. [Resampling](#sample)  \n",
    "3. [Feature Scaling](#scale)  \n",
    "4. [Feature Extraction](#extract)  \n",
    "5. [Feature Selection](#select)  \n",
    "6. [Verdict](#verdict)  \n",
    "7. [Applying to test set/new data](#test)  \n",
    "\n",
    "# Introduction:\n",
    "\n",
    "* The following is a data preprocessing pipeline I authored powered by Extreme Gradient Boosting.  \n",
    "* The following preprocessing steps are meant to be housed within a custom sklearn-compatible estimator for:\n",
    "    * fitting to and transforming training data within the estimator\n",
    "    * transforming test data for .predict() and .predict_proba() using statistics learned from the training data  \n",
    "* The next step in the estimator's pipeline is **Hyperparameter Tuning**, which will be in Part 3.\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\"> The main goal is to be able to let the following algorithms handle decision-making on which methods to use for every step in pre-processing with just a few lines of code and minimal effort for maximum performance.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JanErish\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder\n",
    "\n",
    "import numbers\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis,FastICA,IncrementalPCA,KernelPCA,\\\n",
    "LatentDirichletAllocation,MiniBatchDictionaryLearning,MiniBatchSparsePCA,NMF,PCA,SparsePCA,\\\n",
    "TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from collections import Counter\n",
    "\n",
    "# impoering sklearn metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, hamming_loss,\\\n",
    "jaccard_similarity_score, log_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss,\\\n",
    "explained_variance_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score\n",
    "\n",
    "# importing imblearn resampling classes\n",
    "from imblearn.under_sampling import ClusterCentroids,CondensedNearestNeighbour,EditedNearestNeighbours,\\\n",
    "RepeatedEditedNearestNeighbours,AllKNN,InstanceHardnessThreshold,NearMiss,NeighbourhoodCleaningRule,OneSidedSelection,\\\n",
    "RandomUnderSampler,TomekLinks\n",
    "from imblearn.over_sampling import ADASYN,RandomOverSampler,SMOTE\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "from imblearn.ensemble import BalanceCascade,BalancedBaggingClassifier,EasyEnsemble\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, minmax_scale, RobustScaler, \\\n",
    "                                    MaxAbsScaler, QuantileTransformer\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "from colorama import Fore, Style\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mX with randomly generated missing values:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        V1        V2        V3        V4        V5        V6  \\\n",
       "0    NaN       NaN -0.072781       NaN  1.378155       NaN       NaN   \n",
       "1    1.0       NaN  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2    NaN       NaN -1.340163  1.773209       NaN       NaN  1.800499   \n",
       "3    3.0       NaN       NaN       NaN       NaN -0.010309  1.247203   \n",
       "4    NaN       NaN       NaN       NaN  0.403034 -0.407193  0.095921   \n",
       "5    NaN -0.425966       NaN  1.141109 -0.168252  0.420987 -0.029728   \n",
       "6    6.0       NaN  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7    7.0 -0.644269       NaN  1.074380 -0.492199       NaN  0.428118   \n",
       "8    8.0 -0.894286  0.286157 -0.113192       NaN  2.669599       NaN   \n",
       "9    9.0 -0.338262       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "         V7        V8        V9   ...         V20       V21       V22  \\\n",
       "0       NaN       NaN  0.363787   ...    0.251412       NaN       NaN   \n",
       "1       NaN  0.085102 -0.255425   ...   -0.069083 -0.225775 -0.638672   \n",
       "2       NaN       NaN -1.514654   ...         NaN       NaN  0.771679   \n",
       "3  0.237609       NaN       NaN   ...         NaN       NaN  0.005274   \n",
       "4  0.592941 -0.270533  0.817739   ...         NaN       NaN       NaN   \n",
       "5       NaN  0.260314       NaN   ...         NaN       NaN       NaN   \n",
       "6 -0.005159  0.081213  0.464960   ...   -0.219633 -0.167716 -0.270710   \n",
       "7  1.120631       NaN  0.615375   ...         NaN       NaN -1.015455   \n",
       "8  0.370145  0.851084 -0.392048   ...         NaN -0.073425 -0.268092   \n",
       "9  0.651583       NaN       NaN   ...    0.203711 -0.246914 -0.633753   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Amount  \n",
       "0       NaN       NaN  0.128539 -0.189115       NaN -0.021053     NaN  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2       NaN       NaN -0.327642       NaN       NaN       NaN  378.66  \n",
       "3       NaN -1.175575       NaN       NaN  0.062723       NaN     NaN  \n",
       "4       NaN  0.141267 -0.206010  0.502292       NaN       NaN     NaN  \n",
       "5 -0.026398       NaN -0.232794       NaN       NaN       NaN     NaN  \n",
       "6 -0.154104       NaN  0.750137       NaN  0.034507  0.005168    4.99  \n",
       "7  0.057504       NaN       NaN -0.051634 -1.206921 -1.085339   40.80  \n",
       "8 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404     NaN  \n",
       "9 -0.120794 -0.385050       NaN  0.094199       NaN  0.083076    3.68  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Importing the dataset\"\"\"\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df = df.drop('Time', axis=1)\n",
    "df.reset_index(inplace=True)\n",
    "target = 'Class'\n",
    "\n",
    "\n",
    "# Adding random missing values to both independent and dependent variables\n",
    "import random\n",
    "def add_random_na(row):\n",
    "    vals = row.values\n",
    "    for _ in range(random.randint(0,len(vals)-2)):\n",
    "        i = random.randint(0,len(vals)-1)\n",
    "        vals[i] = np.nan\n",
    "    return vals\n",
    "df = df.apply(add_random_na,axis=1)\n",
    "\n",
    "\n",
    "X = df.drop(target,axis=1)\n",
    "y = pd.DataFrame(df[target])\n",
    "print(f'\\033[1m{Fore.RED}X with randomly generated missing values:{Style.RESET_ALL}\\033[0m')\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mTarget class values:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0.,  nan,   1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Target class values:{Style.RESET_ALL}\\033[0m')\n",
    "y.iloc[:,0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\">\n",
    "class&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nullifier\n",
    "    </span> \n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Attributes:  </span>\n",
    "\n",
    "### .null_values: list\n",
    "* list of values that do not belong to a fitted array's valid values\n",
    "\n",
    "### .null_dictionary\n",
    "* a dictionary of values considered null for each column of the fitted DataFrame\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Methods:  </span>\n",
    "\n",
    "### .find/ .finder(a, valid_values)\n",
    "Lists values that do not belong to the provided valid values for an array.  \n",
    "**Parameters:**\n",
    "* a: *array-like, shape (n_samples,)*  \n",
    "    * array or DataFrame column for which null values will be found  \n",
    "* valid_values: _list_\n",
    "    * list of values considered valid.\n",
    "    * values that do not belong to this list will be included to .null_values\n",
    "**Returns:**\n",
    "* null_values: *list*  \n",
    "    * list of values to be considered null  \n",
    " \n",
    "### .nullify(a)\n",
    "Converts values in null_values into NaNs. \n",
    "**Parameters:**\n",
    "* a: *array-like, shape (n_samples,)*  \n",
    "    * array or DataFrame column for which null values will be found  \n",
    "* valid_values: _list_\n",
    "    * list of values considered valid.\n",
    "    * values that do not belong to this list will be included to .null_values\n",
    "**Returns:**\n",
    "* a: *array-like, shape (n_samples,)*  \n",
    "    * array with nullified invalid values\n",
    "\n",
    "### .find_nullify(a, valid_values)\n",
    "Efficienty finds null_values and converts them to NaNs.\n",
    "**Returns:**\n",
    "* a: *array-like, shape (n_samples,)*  \n",
    "    * array with nullified invalid values\n",
    "\n",
    "### .df_nullifier(df, valid_dictionary)\n",
    "Converts values in null_values into NaNs. \n",
    "**Parameters:**\n",
    "* df: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * DataFrame whose columns will be nullified using the dictionary of valid values  \n",
    "* valid_dictionary: _dictionary_\n",
    "    * a dictionary comprising of the following key/value pairs:  \n",
    "        **Key** =  column name  \n",
    "        **Value** = list of valid values for the column.  \n",
    "**Returns:**\n",
    "* df: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * DataFrame with null values turned into NaNs  \n",
    "    \n",
    "### .df_null_dictionary(df, valid_dictionary)\n",
    "Returns a dictionary of values considered null for each DatFrame column  \n",
    "**Parameters:**  \n",
    "* df: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * DataFrame whose columns will be nullified using the dictionary of valid values  \n",
    "* valid_dictionary: _dictionary_\n",
    "    * a dictionary comprising of the following key/value pairs:  \n",
    "        **Key** =  column name  \n",
    "        **Value** = list of valid values for the column.    \n",
    "        \n",
    "**Returns:**  \n",
    "* null_dictionary: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * a dictionary comprising of the following key/value pairs:  \n",
    "        **Key** =  column name  \n",
    "        **Value** = list of null values for the column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nullifier:\n",
    "    def __init__(self):\n",
    "        self\n",
    "\n",
    "    def find(self, a, valid_values):\n",
    "        a = pd.DataFrame(a) \n",
    "        valid_indeces = a.iloc[:,0].isin(valid_values)\n",
    "        if len(valid_indeces) != 0:\n",
    "            invalid_a = pd.DataFrame(a.drop(a.index[valid_indeces]))\n",
    "            self.null_values = list(invalid_a.iloc[:,0].unique())\n",
    "        return self\n",
    "    \n",
    "    def finder(self, a, valid_values):\n",
    "        self.find(a, valid_values)\n",
    "        return self.null_values\n",
    "    \n",
    "    def nullify(self, a):\n",
    "        a = pd.DataFrame(a)\n",
    "        if len(self.null_values) != 0:\n",
    "            for null_val in self.null_values:\n",
    "                a = a.replace([null_val], np.nan)\n",
    "        return a\n",
    "    \n",
    "    def find_nullify(self, a, valid_values):\n",
    "        self.find(a, valid_values)\n",
    "        return self.nullify(a)\n",
    "    \n",
    "    # DataFrame nullifier\n",
    "    def df_nullifier(self, df, valid_dictionary):\n",
    "        \"\"\" This method should be given a pandas DataFrame and a dictionary comprising of the following key/value pairs:\n",
    "            Key =  column name\n",
    "            Value = list of valid values for the column.\"\"\"\n",
    "        for col, valid_vals in valid_dictionary.items():\n",
    "            df[col] = self.find_nullify(df[col], valid_vals)\n",
    "        return df\n",
    "    \n",
    "    def df_null_dictionary(self, df, valid_dictionary):\n",
    "        \"\"\"Returns a dictionary of values considered null for each DatFrame column\"\"\"\n",
    "        self.null_dictionary = {}\n",
    "        for col, valid_vals in valid_dictionary.items():\n",
    "            self.null_dictionary[col] = self.finder(df[col], valid_vals)\n",
    "        return self.null_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\">\n",
    "class&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCAR_Dropper</span><i><span style=\"font-size:23px;font-weight:bold;color:#3366cc\">(null_values=None)</span></i>  \n",
    " \n",
    "This class automatically drops rows with missing data for a categorical target/dependent variable (y) under the assumption that the data is Missing Completely at Random (MCAR). Parameter Null_values can be fed a list of label values that will be considered Null and thus will be dropped\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Parameters/Attributes:  </span>\n",
    "\n",
    "### .null_values: list\n",
    "* list of values that do not belong to a y's valid values\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Methods:  </span>\n",
    "\n",
    "### .fit_transform(X, y)  \n",
    "\n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "    \n",
    "**Returns:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features whose rows with missing target values have been dropped\n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target whose rows with missing target values have been dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCAR_Dropper:\n",
    "    \"\"\"This transformer is not compatible with sklearn Pipeline.\n",
    "    \n",
    "    This class automatically drops rows with missing data for a categorical target/dependent variable\n",
    "    under the assumption that the data is Missing Completely at Random (MCAR).\n",
    "    \n",
    "    Parameter Null_values can be fed a list of label values that will be considered Null and thus will be dropped\"\"\"\n",
    "    def __init__(self, null_values=None):\n",
    "        self.null_values = null_values\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X, y):\n",
    "        X_nonull = X\n",
    "        y_nonull = y\n",
    "        null_inds = pd.isnull(y).any(1).nonzero()[0]\n",
    "        if len(null_inds) != 0:\n",
    "            X_nonull = pd.DataFrame(X.drop(X.index[null_inds])).reset_index(drop=True)\n",
    "            y_nonull = pd.DataFrame(y.drop(y.index[null_inds])).reset_index(drop=True)\n",
    "        \n",
    "        # drop values considered as null based on given list\n",
    "        if self.null_values is not None:\n",
    "            for value in self.null_values:\n",
    "                null_inds = y_nonull.iloc[:,0] == value\n",
    "                if len(null_inds) != 0:\n",
    "                    X_nonull = pd.DataFrame(X_nonull.drop(X_nonull.index[null_inds])).reset_index(drop=True)\n",
    "                    y_nonull = pd.DataFrame(y_nonull.drop(y_nonull.index[null_inds])).reset_index(drop=True)\n",
    "        \n",
    "        return X_nonull, y_nonull\n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:25px;font-weight:bold;color:#b22222\"> Nullifier + MCAR_Dropper: dropping rows with missing y values  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mChecking for null target values:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Class    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping rows which have missing y values\n",
    "X_mcar, y_mcar = MCAR_Dropper(null_values=Nullifier().finder(a=y,valid_values=[1,0])).fit_transform(X, y)\n",
    "print(f'\\033[1m{Fore.RED}Checking for null target values:{Style.RESET_ALL}\\033[0m')\n",
    "y_mcar.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mX (MCAR) missing value count:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index     57030\n",
       "V1        57210\n",
       "V2        57025\n",
       "V3        57078\n",
       "V4        57078\n",
       "V5        56949\n",
       "V6        57025\n",
       "V7        57027\n",
       "V8        56953\n",
       "V9        57032\n",
       "V10       56740\n",
       "V11       57085\n",
       "V12       57040\n",
       "V13       56821\n",
       "V14       57103\n",
       "V15       56974\n",
       "V16       57101\n",
       "V17       56842\n",
       "V18       56982\n",
       "V19       56960\n",
       "V20       56827\n",
       "V21       57136\n",
       "V22       57053\n",
       "V23       56922\n",
       "V24       56630\n",
       "V25       56732\n",
       "V26       57084\n",
       "V27       56691\n",
       "V28       56878\n",
       "Amount    56858\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}X (MCAR) missing value count:{Style.RESET_ALL}\\033[0m')\n",
    "X_mcar.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mcar, y_mcar,\n",
    "                                                test_size=0.2, random_state=69, stratify=y_mcar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"impute\"></a>\n",
    "\n",
    "![Imgur](https://i.imgur.com/gwV4Msa.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>Imputation and Encoding</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "![Imgur](https://i.imgur.com/xlClQAG.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\">\n",
    "class&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Imputation_Nation</span><i><span style=\"font-size:16px;font-weight:bold;color:#3366cc\">(impute_y=False, objective=\"reg:linear\", missing_values='NaN', num_imput_strat='mean', cat_imput_strat='most_frequent',  imputation_axis=0, Cat_mask = None, Num_cols = None, X_fillers=[], y_filler=None, classification_objectives=None)</span></i>  \n",
    " \n",
    "This class handles the imputation of missing values and the encoding and creation of dummy variables for categorical variables. If the target variable is passed as y, it will also be imputed if impute_y= True. Otherwise, y will pass through as untouched.  \n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Parameters:  </span>\n",
    "\n",
    "### impute_y: boolean, default=False\n",
    "* whether or not y will also undergo imputation and encoding\n",
    "\n",
    "### objective: str, default='reg:linear'\n",
    "* refers to the xgboost objective being used\n",
    "* if objective is a classification objective, and impute_y is True, y will be handled as categorical data\n",
    "\n",
    "### missing_values: str, default='NaN'\n",
    "* identifies what will be considered as missing values  \n",
    "\n",
    "### num_imput_strat: str, default='mean'\n",
    "* 'mean', 'median', or 'most frequent'\n",
    "* determines how numerical variables will be imputed using the sklearn Imputer module\n",
    "\n",
    "### cat_mask: list, default=None\n",
    "* allows user to pass a list of features which will be considered categorical\n",
    "* if left None, categorical nature of features will be determined through dtype\n",
    "\n",
    "### num_cols: list, default=None\n",
    "* allows user to pass a list of features which will be considered numerical\n",
    "* if left None, numerical nature of features will be determined through dtype\n",
    "\n",
    "### X_fillers: pandas DataFrame, default=[]\n",
    "* allows user to pass a DataFrame of features and their respective values used to fill in NaNs\n",
    "* sourced from previous use of fit_transform and most useful for imputing test or new data\n",
    "\n",
    "### y_filler: pandas DataFrame, default=None\n",
    "* allows user to pass a value used to fill in NaNs in the target y  \n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Attributes:  </span>\n",
    "\n",
    "### X_fillers: pandas DataFrame, default=[]\n",
    "* DataFrame of features and their respective values used to fill in NaNs\n",
    "* sourced from previous use of fit_transform and most useful for imputing test or new data\n",
    "\n",
    "### y_filler: pandas DataFrame, default=None\n",
    "* value used to fill in NaNs in the target y  \n",
    "  \n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Methods:  </span>\n",
    "\n",
    "### .fit_transform(X, y=None)  \n",
    "\n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "    \n",
    "**Returns:**\n",
    "* X: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * imputed and encoded independent variables / features\n",
    "* y: *pandas DataFrame, shape (n_samples,)*    \n",
    "    * imputed or untouched dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imputer Class\n",
    "\n",
    "class Imputation_Nation:\n",
    "        \"\"\"This class handles the imputation of missing values\n",
    "        and the encoding and creation of dummy variables for categorical variables.\n",
    "\n",
    "        If the target variable is passed as y, it will also be imputed if impute_y= True.\n",
    "        Otherwise, y will pass through as untouched.\"\"\"\n",
    "        \n",
    "        classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                            'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "        \n",
    "        missing_values = None\n",
    "        num_imput_strat = None\n",
    "        cat_imput_strat = None\n",
    "        imputation_axis = None\n",
    "        objective = None\n",
    "        impute_y = None\n",
    "        X_fillers=None\n",
    "        y_filler=None\n",
    "        Cat_mask = None\n",
    "        Num_cols = None\n",
    "        \n",
    "        def __init__(self, impute_y=False, objective=\"reg:linear\",missing_values='NaN', num_imput_strat='mean', \n",
    "                     cat_imput_strat='most_frequent', imputation_axis=0,\n",
    "                    Cat_mask = None, Num_cols = None, X_fillers=[],y_filler=None,\n",
    "                    \n",
    "                     classification_objectives=None):\n",
    "        \n",
    "            # assigning parameters as instance variables\n",
    "            varses = list(vars(Imputation_Nation).keys())\n",
    "            self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "            class_name = \"Imputation_Nation\"+\".\"\n",
    "            for v in self.variables:\n",
    "                # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "                if eval(\"%s != None\" % (class_name+v)) is True:\n",
    "                    exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "                # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "                else:\n",
    "                    exec(\"self.%s = %s\" % (v, v)) \n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        \n",
    "        def transform(self, X, y=None):\n",
    "                \n",
    "            X_full = pd.DataFrame(X)\n",
    "            if y is not None:\n",
    "                y_full = pd.DataFrame(y)\n",
    "                \n",
    "            # identify categorical columns\n",
    "            if self.Cat_mask is None:\n",
    "                Cat_mask = X_full.dtypes == object\n",
    "                Cat_cols = X_full.columns[Cat_mask].tolist()\n",
    "            else:\n",
    "                Cat_mask = self.Cat_mask\n",
    "                Cat_cols = X_full.columns[Cat_mask].tolist()\n",
    "            \n",
    "            # identify numeric columns\n",
    "            if self.Num_cols is None:\n",
    "                Num_cols = X_full.select_dtypes(exclude=['object']).columns.tolist()\n",
    "            else:\n",
    "                Num_cols = self.Num_cols\n",
    "            \n",
    "            fill_cols = []\n",
    "            fill_values = []\n",
    "            \n",
    "            # Encoding and imputing categorical variables\n",
    "            if len(Cat_cols) != 0:\n",
    "                for col in Cat_cols:\n",
    "\n",
    "                    X_full.loc[:,col] = X_full.loc[:,col].fillna(self.missing_values)\n",
    "                    missing_count = sum(X_full.loc[:,col] == self.missing_values)\n",
    " \n",
    "                    if len(self.X_fillers) != 0 and col in self.X_fillers.index:\n",
    "                        if missing_count != 0:\n",
    "                            X_full.loc[:,col]=X_full.loc[:,col].replace(self.missing_values,\n",
    "                                                                        self.X_fillers.loc[col,'Fills'])\n",
    "                    else:\n",
    "                        if missing_count != 0:\n",
    "                            values, counts = np.unique(X_full.loc[X_full.loc[:,col]!=self.missing_values,col],\n",
    "                                                       return_counts=True)\n",
    "                            m = counts.argmax()\n",
    "                            most_frequent = [values[m]][0]\n",
    "                            X_full.loc[:,col] = X_full.loc[:,col].replace(self.missing_values,most_frequent)\n",
    "\n",
    "                            fill_cols.append(col)\n",
    "                            fill_values = fill_values + [most_frequent]\n",
    "\n",
    "                X_full = pd.get_dummies(X_full, drop_first=True, columns=Cat_cols)\n",
    "            \n",
    "            # Imputing numeric columns\n",
    "            if len(Num_cols) != 0:\n",
    "                for col in Num_cols:\n",
    "                    if len(self.X_fillers) != 0 and col in self.X_fillers.index:\n",
    "                        X_full.loc[:,col] = X_full.loc[:,col].fillna(self.X_fillers.loc[col,'Fills'])\n",
    "                    else:\n",
    "                        IMP=Imputer(missing_values=self.missing_values,strategy=self.num_imput_strat, \n",
    "                                    axis=self.imputation_axis)\n",
    "                        IMP.fit(X_full.loc[:,col].values.reshape(-1, 1))\n",
    "                        X_full.loc[:,col] = IMP.transform(X_full.loc[:,col].values.reshape(-1, 1))\n",
    "                        fill_cols.append(col)\n",
    "                        fill_values = fill_values  + (list(IMP.statistics_))\n",
    "            \n",
    "            if y is not None and self.impute_y == True:\n",
    "                if self.objective in self.classification_objectives:\n",
    "                    col = y_full.columns[0]\n",
    "                    y_full.loc[:,col] =  y_full.loc[:,col].fillna(self.missing_values)\n",
    "                    \n",
    "                    # checking if values are numeric\n",
    "                    numerics = 0\n",
    "                    for value in list(y_full.loc[:,col].unique()):\n",
    "                        if isinstance(value, numbers.Number) is True:\n",
    "                            numerics += 1\n",
    "                    if numerics > 0:\n",
    "                        y_is_numeric = True\n",
    "                    else:\n",
    "                        y_is_numeric = False\n",
    "                        \n",
    "                    # convert all values to string\n",
    "                    y_full.loc[:,col] = y_full.loc[:,col].apply(str) \n",
    "                    \n",
    "                    # count missing values\n",
    "                    missing_count = sum(y_full.loc[:,col] == self.missing_values)\n",
    "                    \n",
    "                    # if filler value has been provided, use it as replacement\n",
    "                    if self.y_filler is not None and missing_count != 0:\n",
    "                        y_full.loc[:,col]=y_full.loc[:,col].replace(self.missing_values,\n",
    "                                                                    self.y_filler.loc[col,'Fills'])\n",
    "                    elif missing_count != 0:\n",
    "                        values, counts = np.unique(y_full.loc[y_full.loc[:,col]!=self.missing_values,col],\n",
    "                                                   return_counts=True)\n",
    "                        m = counts.argmax()\n",
    "                        most_frequent = [values[m]][0]\n",
    "                        y_full.loc[:,col] = y_full.loc[:,col].replace(self.missing_values,most_frequent)\n",
    "                        self.y_filler = most_frequent\n",
    "                        \n",
    "                    # if values were originally numeric, convert to int\n",
    "                    if y_is_numeric is True:\n",
    "                        y_full.loc[:,col] = [int(float(value)) for value in y_full.loc[:,col]]\n",
    "                    \n",
    "                \n",
    "                else:\n",
    "                    if len(self.y_filler) != 0:\n",
    "                        y_full.loc[:,col] = y_full.loc[:,col].fillna(self.y_filler)\n",
    "                    else:\n",
    "                        IMP = Imputer(missing_values=self.missing_values,strategy=self.num_imput_strat, \n",
    "                                    axis=self.imputation_axis)\n",
    "                        IMP.fit(y_full.loc[:,col])\n",
    "                        y_full.loc[:,col] = IMP.transform(y_full.loc[:,col])\n",
    "                        self.y_filler = list(IMP.statistics_)\n",
    "            \n",
    "            self.X_fillers = pd.DataFrame()\n",
    "            self.X_fillers['Columns'] = fill_cols\n",
    "            self.X_fillers['Fills'] = fill_values\n",
    "            self.X_fillers.set_index('Columns', inplace=True)\n",
    "            \n",
    "            if y is not None:\n",
    "                return pd.DataFrame(X_full), pd.DataFrame(y_full)\n",
    "            return pd.DataFrame(X_full)\n",
    "        \n",
    "        def fit_transform(self, X, y=None):\n",
    "            self.fit(X, y)\n",
    "            return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Imputation_Nation in action  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mImputed and Encoded X_train:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45924</th>\n",
       "      <td>71071.000000</td>\n",
       "      <td>1.112634</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>2.514559</td>\n",
       "      <td>-0.313505</td>\n",
       "      <td>-0.117546</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.056953</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175697</td>\n",
       "      <td>-0.045880</td>\n",
       "      <td>-0.127814</td>\n",
       "      <td>0.142695</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0.030464</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170525</th>\n",
       "      <td>263822.000000</td>\n",
       "      <td>-0.741988</td>\n",
       "      <td>0.724629</td>\n",
       "      <td>1.105168</td>\n",
       "      <td>-0.015564</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>-0.002993</td>\n",
       "      <td>0.874461</td>\n",
       "      <td>-0.270365</td>\n",
       "      <td>-0.782157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001480</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>-0.554227</td>\n",
       "      <td>0.067597</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.034668</td>\n",
       "      <td>0.063369</td>\n",
       "      <td>87.959687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56081</th>\n",
       "      <td>142772.993236</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.421228</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>-0.002993</td>\n",
       "      <td>-0.397842</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001480</td>\n",
       "      <td>-0.015377</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>-0.412952</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.435571</td>\n",
       "      <td>-0.020457</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>76.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120027</th>\n",
       "      <td>142772.993236</td>\n",
       "      <td>2.282386</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>-0.984139</td>\n",
       "      <td>-1.812335</td>\n",
       "      <td>-1.029801</td>\n",
       "      <td>-0.002993</td>\n",
       "      <td>-1.443545</td>\n",
       "      <td>0.089758</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.401575</td>\n",
       "      <td>-0.143415</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.207734</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.228512</td>\n",
       "      <td>-0.189058</td>\n",
       "      <td>0.013076</td>\n",
       "      <td>-0.054427</td>\n",
       "      <td>19.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117252</th>\n",
       "      <td>181424.000000</td>\n",
       "      <td>1.602153</td>\n",
       "      <td>-2.090925</td>\n",
       "      <td>-0.308115</td>\n",
       "      <td>-0.735557</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>-0.002993</td>\n",
       "      <td>-1.280689</td>\n",
       "      <td>0.204274</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425196</td>\n",
       "      <td>0.435741</td>\n",
       "      <td>0.646779</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.593891</td>\n",
       "      <td>-0.248057</td>\n",
       "      <td>-0.018549</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73480</th>\n",
       "      <td>113572.000000</td>\n",
       "      <td>-2.909004</td>\n",
       "      <td>1.469046</td>\n",
       "      <td>-0.919853</td>\n",
       "      <td>1.543391</td>\n",
       "      <td>-1.633809</td>\n",
       "      <td>2.489002</td>\n",
       "      <td>0.644544</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.786929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.651084</td>\n",
       "      <td>0.047422</td>\n",
       "      <td>1.250971</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>-0.868204</td>\n",
       "      <td>-1.286972</td>\n",
       "      <td>-0.256837</td>\n",
       "      <td>-0.614046</td>\n",
       "      <td>-0.258630</td>\n",
       "      <td>292.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173065</th>\n",
       "      <td>267768.000000</td>\n",
       "      <td>0.114134</td>\n",
       "      <td>0.945923</td>\n",
       "      <td>-0.515894</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>1.060809</td>\n",
       "      <td>-0.345297</td>\n",
       "      <td>0.876227</td>\n",
       "      <td>0.065495</td>\n",
       "      <td>-0.253679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037631</td>\n",
       "      <td>-0.307573</td>\n",
       "      <td>-0.794852</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>0.239695</td>\n",
       "      <td>-0.419018</td>\n",
       "      <td>0.112853</td>\n",
       "      <td>0.214251</td>\n",
       "      <td>0.071237</td>\n",
       "      <td>10.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51174</th>\n",
       "      <td>142772.993236</td>\n",
       "      <td>1.420920</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>-0.265795</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>-0.490289</td>\n",
       "      <td>-0.554512</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>-0.031047</td>\n",
       "      <td>-1.000422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>-0.220640</td>\n",
       "      <td>-0.922325</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>-0.578219</td>\n",
       "      <td>0.360725</td>\n",
       "      <td>-0.490796</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>-0.006215</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182463</th>\n",
       "      <td>142772.993236</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>-1.167900</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.262477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172705</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.532645</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>4.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33249</th>\n",
       "      <td>51575.000000</td>\n",
       "      <td>1.056573</td>\n",
       "      <td>0.405634</td>\n",
       "      <td>0.562085</td>\n",
       "      <td>2.550570</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>-0.512296</td>\n",
       "      <td>0.276847</td>\n",
       "      <td>-0.185107</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031035</td>\n",
       "      <td>0.102413</td>\n",
       "      <td>0.145298</td>\n",
       "      <td>-0.139190</td>\n",
       "      <td>0.414416</td>\n",
       "      <td>0.553505</td>\n",
       "      <td>0.058215</td>\n",
       "      <td>-0.021225</td>\n",
       "      <td>0.034642</td>\n",
       "      <td>75.310000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                index        V1        V2        V3        V4        V5  \\\n",
       "45924    71071.000000  1.112634  0.008148  0.000802  2.514559 -0.313505   \n",
       "170525  263822.000000 -0.741988  0.724629  1.105168 -0.015564  0.999639   \n",
       "56081   142772.993236  0.002235  0.008148  0.000802  0.421228 -0.005245   \n",
       "120027  142772.993236  2.282386  0.008148 -0.984139 -1.812335 -1.029801   \n",
       "117252  181424.000000  1.602153 -2.090925 -0.308115 -0.735557 -0.005245   \n",
       "73480   113572.000000 -2.909004  1.469046 -0.919853  1.543391 -1.633809   \n",
       "173065  267768.000000  0.114134  0.945923 -0.515894  0.001551  1.060809   \n",
       "51174   142772.993236  1.420920  0.008148 -0.265795  0.001551 -0.490289   \n",
       "182463  142772.993236  0.002235  0.008148  0.000802  0.001551 -0.005245   \n",
       "33249    51575.000000  1.056573  0.405634  0.562085  2.550570 -0.005245   \n",
       "\n",
       "              V6        V7        V8        V9     ...           V20  \\\n",
       "45924  -0.117546  0.005794  0.056953  0.002071     ...     -0.175697   \n",
       "170525 -0.002993  0.874461 -0.270365 -0.782157     ...     -0.001480   \n",
       "56081  -0.002993 -0.397842  0.002292  0.002071     ...     -0.001480   \n",
       "120027 -0.002993 -1.443545  0.089758  0.002071     ...     -0.401575   \n",
       "117252 -0.002993 -1.280689  0.204274  0.002071     ...      0.425196   \n",
       "73480   2.489002  0.644544  0.002292  0.786929     ...     -0.651084   \n",
       "173065 -0.345297  0.876227  0.065495 -0.253679     ...      0.037631   \n",
       "51174  -0.554512  0.005794 -0.031047 -1.000422     ...      0.007863   \n",
       "182463 -1.167900  0.005794  0.002292  0.262477     ...     -0.172705   \n",
       "33249  -0.512296  0.276847 -0.185107  0.002071     ...      0.031035   \n",
       "\n",
       "             V21       V22       V23       V24       V25       V26       V27  \\\n",
       "45924  -0.045880 -0.127814  0.142695  0.000800  0.000854  0.000036  0.023632   \n",
       "170525  0.002016  0.895954 -0.554227  0.067597  0.000854  0.000036  0.034668   \n",
       "56081  -0.015377  0.000876 -0.000661 -0.412952  0.000854  0.435571 -0.020457   \n",
       "120027 -0.143415  0.000876  0.207734  0.000800 -0.228512 -0.189058  0.013076   \n",
       "117252  0.435741  0.646779 -0.000661  0.000800 -0.593891 -0.248057 -0.018549   \n",
       "73480   0.047422  1.250971  0.007136 -0.868204 -1.286972 -0.256837 -0.614046   \n",
       "173065 -0.307573 -0.794852  0.066814  0.239695 -0.419018  0.112853  0.214251   \n",
       "51174  -0.220640 -0.922325  0.025935 -0.578219  0.360725 -0.490796 -0.001058   \n",
       "182463  0.002016  0.000876 -0.000661  0.000800 -0.532645  0.000036 -0.001058   \n",
       "33249   0.102413  0.145298 -0.139190  0.414416  0.553505  0.058215 -0.021225   \n",
       "\n",
       "             V28      Amount  \n",
       "45924   0.030464    0.770000  \n",
       "170525  0.063369   87.959687  \n",
       "56081  -0.000223   76.460000  \n",
       "120027 -0.054427   19.200000  \n",
       "117252 -0.000223  268.000000  \n",
       "73480  -0.258630  292.170000  \n",
       "173065  0.071237   10.110000  \n",
       "51174  -0.006215   24.000000  \n",
       "182463 -0.000223    4.490000  \n",
       "33249   0.034642   75.310000  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_N = Imputation_Nation(objective=\"binary:logistic\", impute_y=True)\n",
    "X_imputed, y_imputed = I_N.fit_transform(X_train, y_train)\n",
    "print(f'\\033[1m{Fore.RED}Imputed and Encoded X_train:{Style.RESET_ALL}\\033[0m')\n",
    "X_imputed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mImputed X_train missing value count:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index     0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Imputed X_train missing value count:{Style.RESET_ALL}\\033[0m')\n",
    "X_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;font-weight:bold;color:#b22222\"> Filler values collected from X_train:  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fills</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Columns</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>142772.993236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>0.002235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>0.008148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.000802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>0.001551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>-0.005245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>-0.002993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.005794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>0.002292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>0.002071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.002419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>-0.002951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>-0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>-0.003899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>0.004125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>-0.001116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>-0.004007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>-0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>0.000773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>-0.001480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>0.002016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>0.000876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>-0.000661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>0.000854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>-0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>-0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>87.959687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Fills\n",
       "Columns               \n",
       "index    142772.993236\n",
       "V1            0.002235\n",
       "V2            0.008148\n",
       "V3            0.000802\n",
       "V4            0.001551\n",
       "V5           -0.005245\n",
       "V6           -0.002993\n",
       "V7            0.005794\n",
       "V8            0.002292\n",
       "V9            0.002071\n",
       "V10           0.002419\n",
       "V11          -0.002951\n",
       "V12           0.000089\n",
       "V13          -0.000930\n",
       "V14          -0.003899\n",
       "V15           0.004125\n",
       "V16          -0.001116\n",
       "V17          -0.004007\n",
       "V18          -0.000060\n",
       "V19           0.000773\n",
       "V20          -0.001480\n",
       "V21           0.002016\n",
       "V22           0.000876\n",
       "V23          -0.000661\n",
       "V24           0.000800\n",
       "V25           0.000854\n",
       "V26           0.000036\n",
       "V27          -0.001058\n",
       "V28          -0.000223\n",
       "Amount       87.959687"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_N.X_fillers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;font-weight:bold;color:#b22222\"> Collected filler for missing y values:  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_N.y_filler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sample\"></a>\n",
    "\n",
    "![Imgur](https://i.imgur.com/OwNPE6P.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>Dealing with Imbalance: Resampling</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "![Imgur](https://i.imgur.com/YOQuMbt.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\">\n",
    "class&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HyperSampler</span><i><span style=\"font-size:16px;font-weight:bold;color:#3366cc\">(sampler=\"HyperSampler\", best_params={}, booster='gbtree', objective='reg:logistic', scorer='Auto', test_samplers=[], excluded_samplers=[], additional_samplers=[], set_sampler_params={})</span></i>  \n",
    " \n",
    "\"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "        Options for resampling methods methods (parameter sampler='') are the following:\n",
    "            Binary_Undersampler, ClusterCentroids, CondensedNearestNeighbour, EditedNearestNeighbours,\n",
    "            RepeatedEditedNearestNeighbours, AllKNN, InstanceHardnessThreshold, NearMiss, NeighbourhoodCleaningRule, \n",
    "            OneSidedSelection, RandomUnderSampler, TomekLinks, ADASYN, RandomOverSampler, SMOTE, SMOTEENN, \n",
    "            SMOTETomek, BalanceCascade, BalancedBaggingClassifier, EasyEnsemble\n",
    ".\n",
    "        \n",
    "        If sampler = 'HyperSampler',\n",
    "            every resampling method will be applied and tested\n",
    "            to determine which method best contributes to model performance.\n",
    "            \n",
    "            best_params = {dictionary of best parameters for model tuning continuity}\n",
    "            \n",
    "            scorer = pass desired Sklearn.metrics scorer as a string\n",
    "                passing scorer='Auto' entails the use of built-in XGBoost evaluation metrics\n",
    "            \n",
    "            The resampling method that contributes to the best score will be applied to the data to be returned.\n",
    "            \n",
    "            HyperSampler.resampler = chosen resampling class assigned after instantiation \n",
    "                                                                        or after fit() if sampler = 'HyperSampler'\n",
    "        \n",
    "        This feature selection transformer works only with any of the three boosters as base learners:\n",
    "        -gbtree\n",
    "        -gblinear\n",
    "        -dart\n",
    "        \n",
    "        This class requires that X (features or independent variables) has already been encoded and Imputed. \n",
    "        \n",
    "        This class also requires the target (y) to also be passed.\n",
    "        \n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit, sample, and fit_sample methods are given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Parameters:  </span>\n",
    "\n",
    "### sampler: str, default='HyperSampler'\n",
    "* If 'HyperSampler', .sample() will perform tests to determine which imblearn resampling method returns the best performance.  \n",
    "* also used to specify a single imblearn resampling method\n",
    "\n",
    "### best_params: dict, default={}\n",
    "* dictionary of parameters to be used as kwargs for XGBoost models to be used for evaluating performance\n",
    "\n",
    "### booster: str, default='gbtree'\n",
    "* specifies the xgboost booster to be used for the evaluation models ('gbtree','gblinear', or 'dart')  \n",
    "\n",
    "### objective: str, default='reg:logistic'\n",
    "* specifies the xgboost objective to be used for the evaluation models\n",
    "* see XGBoost documentation for more details\n",
    "\n",
    "### scorer: str, default='Auto'\n",
    "* determines the model evaluation metric used\n",
    "* if 'Auto', built-in XGBoost metrics will be used\n",
    "* otherwise, existic sklearn metrics can be passed as strings\n",
    "\n",
    "### test_samplers: list, default=[]\n",
    "* allows user to pass a list of resampling methods that will be tested\n",
    "* if left empty, a default list will be used\n",
    "\n",
    "### excluded_samplers: list, default=[]\n",
    "* allows user to pass a list of resampling methods that will be excluded from testing\n",
    "\n",
    "### set_sampler_params: dictionary, default={}\n",
    "* allows user to pass a dictionary for setting parameters of resampling methods\n",
    "* if left empty, a default dictionary will be used\n",
    "* resampling methods not built into the class can be added this way\n",
    "\n",
    "### additional_samplers: list, default=[]\n",
    "* new resampling methods introduced through set_sampler_params can be added to the list of methods to be tested\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Attributes:  </span>\n",
    "\n",
    "### scoring_functions: dictionary\n",
    "* dictonary of metrics available for use\n",
    "\n",
    "### classification_objectives: list\n",
    "* list of XGBoost objectives considered as for classification\n",
    "\n",
    "### regression_objectives: list\n",
    "* list of XGBoost objectives considered as for regression\n",
    "\n",
    "### the_higher_the_better: list\n",
    "* list of metrics of which an increase means improvement in performance\n",
    "\n",
    "### the_lower_the_better: list\n",
    "* list of metrics of which a decrease means improvement in performance\n",
    "\n",
    "### need_encoded_y: list\n",
    "* list of metrics that require an encoded y\n",
    "* automatically dealt with my the class\n",
    "\n",
    "### the_lower_the_better: list\n",
    "* list of metrics of which a decrease means improvement in performance\n",
    "\n",
    "### sampler_params: dictionary\n",
    "* default dictionary of parameters used for resampling methods\n",
    "\n",
    "### sampler: str\n",
    "* name or determined resampling method\n",
    "\n",
    "### resampler: obj\n",
    "* resampler object\n",
    "* fitted to X (or also y) after .fit()\n",
    "* should NOT be used to transform test set or new data\n",
    "\n",
    "### performance_scores: pandas DataFrame\n",
    "* results of testing\n",
    "\n",
    "### best_perf: pandas DataFrame\n",
    "* highest performer/s\n",
    "\n",
    "### best_sampler: str\n",
    "* name of resampling method deemed best\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Methods:  </span>\n",
    "\n",
    "### .fit(X, y=None, target=None)  \n",
    "Either fits the pre-determined resampling method to the data or performs tests first before fitting the best method to the data.  \n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "* target:*string* (optional)  \n",
    "    * name of target column  \n",
    "\n",
    "### .sample(X, y=None, target=None)  \n",
    "Using statistics learned from fitted data, returns resampled data.\n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "* target:*string* (optional)  \n",
    "    * name of target column  \n",
    "\n",
    "**Returns:**\n",
    "* X_resampled: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * resampled or untouched independent variables / features\n",
    "* y_resampled: *pandas DataFrame, shape (n_samples,)*    \n",
    "    * resampled or untouched dependent variable\n",
    "\n",
    "### .fit_sample(X, y=None, target=None)  \n",
    "Streamlines fit() and sample() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Binary_Undersampler:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "    This class is applicable only on data for binary classification.\n",
    "    This function assumes that 1 is the value of the underrepresented class.\"\"\"\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def sample(self, X, y):\n",
    "        X = pd.DataFrame(X)\n",
    "        y = pd.DataFrame(y)\n",
    "        # Encoding target variables formatted as string\n",
    "        if y.iloc[:,0].dtype == object:\n",
    "            Target_is_string = True\n",
    "        else:\n",
    "            Target_is_string = False\n",
    "\n",
    "        if Target_is_string == True:\n",
    "            L_E = LabelEncoder()\n",
    "            L_E = L_E.fit(y.values.ravel().astype(str))\n",
    "            y.iloc[:,0] = L_E.transform(y.values.ravel().astype(str))\n",
    "    \n",
    "        Positive_indices = np.array(y[y.iloc[:,-1] ==1].index)\n",
    "        Negative_indices = np.array(y[y.iloc[:,-1] == 0].index)\n",
    "        if len(Positive_indices) < len(Negative_indices):\n",
    "            Minority_indices = Positive_indices\n",
    "            Majority_indices = Negative_indices\n",
    "        elif len(Positive_indices) > len(Negative_indices):\n",
    "            Minority_indices = Negative_indices\n",
    "            Majority_indices = Positive_indices\n",
    "        #return X, y\n",
    "        random_Majority_indices = np.array(np.random.choice(Majority_indices, len(Minority_indices), replace = False))\n",
    "        Undersample_indices = np.concatenate([Minority_indices, random_Majority_indices])\n",
    "        \n",
    "        # Restore original labels\n",
    "        if Target_is_string == True:\n",
    "            y.iloc[:,0] = L_E.inverse_transform(y.values.ravel())\n",
    "                \n",
    "        return np.array(X.loc[Undersample_indices,:].reset_index(drop=True)),\\\n",
    "                    np.array(y.loc[Undersample_indices,:].reset_index(drop=True))\n",
    "        \n",
    "    def fit_sample(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.sample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 147296\n",
      "Length of y: 147296\n",
      "\u001b[1m\u001b[31mClass Imbalance:\u001b[0m\u001b[0m\n",
      "Counter({0: 147046, 1: 250})\n"
     ]
    }
   ],
   "source": [
    "# Checking for class imbalance:\n",
    "print(\"Length of X: {}\".format(len(X_imputed)))\n",
    "print(\"Length of y: {}\".format(len(y_imputed)))\n",
    "print(f'\\033[1m{Fore.RED}Class Imbalance:{Style.RESET_ALL}\\033[0m')\n",
    "print(Counter(y_imputed.iloc[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary_Undersampler in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of undersampled X_train: 500\n",
      "Length of undersampled train: 500\n",
      "\u001b[1m\u001b[31mClass Balance:\u001b[0m\u001b[0m\n",
      "Counter({1: 250, 0: 250})\n"
     ]
    }
   ],
   "source": [
    "X_undersampled, y_undersampled = Binary_Undersampler().fit_sample(X_imputed, y_imputed)\n",
    "print(\"Length of undersampled X_train: {}\".format(len(X_undersampled)))\n",
    "print(\"Length of undersampled y_train: {}\".format(len(y_undersampled)))\n",
    "print(f'\\033[1m{Fore.RED}Class Balance:{Style.RESET_ALL}\\033[0m')\n",
    "print(Counter(pd.DataFrame(y_undersampled).iloc[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling Class\n",
    "\n",
    "class HyperSampler:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "        Options for resampling methods methods (parameter sampler='') are the following:\n",
    "            Binary_Undersampler, ClusterCentroids, CondensedNearestNeighbour, EditedNearestNeighbours,\n",
    "            RepeatedEditedNearestNeighbours, AllKNN, InstanceHardnessThreshold, NearMiss, NeighbourhoodCleaningRule, \n",
    "            OneSidedSelection, RandomUnderSampler, TomekLinks, ADASYN, RandomOverSampler, SMOTE, SMOTEENN, \n",
    "            SMOTETomek, BalanceCascade, BalancedBaggingClassifier, EasyEnsemble\n",
    ".\n",
    "        \n",
    "        If sampler = 'HyperSampler',\n",
    "            every resampling method will be applied and tested\n",
    "            to determine which method best contributes to model performance.\n",
    "            \n",
    "            best_params = {dictionary of best parameters for model tuning continuity}\n",
    "            \n",
    "            scorer = pass desired Sklearn.metrics scorer as a string\n",
    "                passing scorer='Auto' entails the use of built-in XGBoost evaluation metrics\n",
    "            \n",
    "            The resampling method that contributes to the best score will be applied to the data to be returned.\n",
    "            \n",
    "            HyperSampler.resampler = chosen resampling class assigned after instantiation \n",
    "                                                                        or after fit() if sampler = 'HyperSampler'\n",
    "        \n",
    "        This feature selection transformer works only with any of the three boosters as base learners:\n",
    "        -gbtree\n",
    "        -gblinear\n",
    "        -dart\n",
    "        \n",
    "        This class requires that X (features or independent variables) has already been encoded and Imputed. \n",
    "        \n",
    "        This class also requires the target (y) to also be passed.\n",
    "        \n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit, sample, and fit_sample methods are given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "    \n",
    "    # Compatible model evaluation metrics: Sklearn metrics\n",
    "    scoring_functions = {'accuracy_score':accuracy_score,'f1_score':f1_score, 'hamming_loss':hamming_loss,\n",
    "        'jaccard_similarity_score':jaccard_similarity_score, 'log_loss':log_loss, 'matthews_corrcoef':matthews_corrcoef,\n",
    "                 'precision_score':precision_score, 'recall_score':recall_score, 'zero_one_loss':zero_one_loss,\n",
    "                'explained_variance_score':explained_variance_score, 'mean_absolute_error':mean_absolute_error,\n",
    "                 'mean_squared_error':mean_squared_error, 'mean_squared_log_error':mean_squared_log_error,\n",
    "                 'median_absolute_error':median_absolute_error, 'r2_score':r2_score}\n",
    "\n",
    "    classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                                'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "    regression_objectives = ['reg:linear','count:poisson','reg:gamma', 'reg:tweedie']\n",
    "    \n",
    "    # metrics with which higher value = higher model performance\n",
    "    the_higher_the_better = ['accuracy_score','f1_score','jaccard_similarity_score',\n",
    "                                  'precision_score','recall_score',\n",
    "                                'explained_variance_score','r2_score','Best test auc',\n",
    "                                'Best test ndcg','Best test map']\n",
    "    # metrics with which lower value = higher model performance\n",
    "    the_lower_the_better = ['hamming_loss', 'log_loss','matthews_corrcoef','zero_one_loss','mean_absolute_error',\n",
    "                           'mean_squared_error','mean_squared_log_error','median_absolute_error',\n",
    "                           'Best test error','Best test rmse','Best test mae','Best test log loss',\n",
    "                           'Best test merror','Best test mlogloss']\n",
    "    \n",
    "    # metrics that require encoded target variable\n",
    "    need_encoded_y = ['recall_score','precision_score','f1_score']\n",
    "\n",
    "    # parameters to inherited from mother class\n",
    "    sampler='N/A'\n",
    "    best_params='N/A'\n",
    "    booster='N/A'\n",
    "    objective='N/A'\n",
    "    scorer='N/A'\n",
    "    test_samplers='N/A'\n",
    "    excluded_samplers='N/A'\n",
    "    additional_samplers='N/A'\n",
    "    set_sampler_params='N/A'\n",
    "    \n",
    "    # arguments for classes\n",
    "    estimator='N/A'\n",
    "    voting='N/A'\n",
    "    n_jobs='N/A'\n",
    "    return_indices='N/A'\n",
    "    n_neighbors='N/A'\n",
    "    n_seeds_S='N/A'\n",
    "    kind_sel='N/A'\n",
    "    max_iter='N/A'\n",
    "    allow_minority='N/A'\n",
    "    ratio='N/A'\n",
    "    cv='N/A'\n",
    "    version='N/A'\n",
    "    ver3_samp_ngh='N/A'\n",
    "    n_neighbors_ver3='N/A'\n",
    "    threshold_cleaning='N/A'\n",
    "    replacement='N/A'\n",
    "    k_neighbors='N/A'\n",
    "    m_neighbors='N/A'\n",
    "    out_step='N/A'\n",
    "    kind='N/A'\n",
    "    svm_estimator='N/A'\n",
    "    smote='N/A'\n",
    "    enn='N/A'\n",
    "    kind_smote='N/A'\n",
    "    kind_enn='N/A'\n",
    "    tomek='N/A'\n",
    "    n_max_subset='N/A'\n",
    "    classifier='N/A'\n",
    "    n_estimators='N/A'\n",
    "    max_samples='N/A'\n",
    "    max_features='N/A'\n",
    "    bootstrap='N/A'\n",
    "    bootstrap_features='N/A'\n",
    "    oob_score='N/A'\n",
    "    warm_start='N/A'\n",
    "    verbose='N/A'\n",
    "    n_subsets='N/A'\n",
    "\n",
    "#     **kwargs\n",
    "    \n",
    "    random_state='N/A'\n",
    "    test_size='N/A'\n",
    "    \n",
    "    def __init__(self, sampler=\"HyperSampler\", best_params={},\n",
    "                 booster='gbtree', objective='reg:logistic', scorer='Auto',\n",
    "                 test_samplers=[],excluded_samplers=[],additional_samplers=[],set_sampler_params={},\n",
    "                 # parameters for samplers\n",
    "                 estimator=None,voting='auto',n_jobs=1, return_indices=False,n_seeds_S=1,\n",
    "                 kind_sel='all',max_iter=100,allow_minority=False,ratio='auto',cv=5,version=1,\n",
    "                 ver3_samp_ngh=None,n_neighbors_ver3=3,threshold_cleaning=0.5,replacement=False,k=None,n_neighbors=5,\n",
    "                 k_neighbors=5,m=None,m_neighbors=10,out_step=0.5,kind='regular',svm_estimator=None,smote=None,\n",
    "                 enn=None,kind_smote=None,kind_enn=None, tomek=None,n_max_subset=None,\n",
    "                 classifier=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,\n",
    "                 oob_score=False,warm_start=False,verbose=0,n_subsets=10,\n",
    "                # parameters for train/test split\n",
    "                 random_state=69, test_size=0.2,\n",
    "                # values passed for these parameters will never be used unless: class.parameter is made None\n",
    "                scoring_functions=None, classification_objectives=None, need_encoded_y=None,\n",
    "                 the_higher_the_better=None, the_lower_the_better=None):\n",
    "        \n",
    "        # assigning parameters as instance variables\n",
    "        varses = list(vars(HyperSampler).keys())\n",
    "        self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "        class_name = \"HyperSampler\"+\".\"\n",
    "        for v in self.variables:\n",
    "            # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "            if eval(\"%s != 'N/A'\" % (class_name+v)) is True:\n",
    "                exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "            # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "            else:\n",
    "                exec(\"self.%s = %s\" % (v, v))\n",
    "        \n",
    "        # determining the model build used for feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            if self.objective in HyperSampler.classification_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBClassifier(**self.best_params)\n",
    "\n",
    "            elif self.objective in HyperSampler.regression_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBRegressor(**self.best_params)\n",
    "        elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "            self.best_params['booster'] = self.booster\n",
    "            self.best_params['objective'] = self.objective\n",
    "            \n",
    "        # ASSIGNING SELF.BASE_ESTIMATOR\n",
    "        self.estimator=None\n",
    "        self.base_estimator=None\n",
    "        self.svm_estimator=None\n",
    "        \n",
    "        self.sampler_params={'Binary_Undersampler':[Binary_Undersampler,{}],\n",
    "                'ClusterCentroids':[ClusterCentroids,{'ratio':self.ratio,'random_state':self.random_state,\n",
    "                                            'estimator':self.estimator,'voting':self.voting,'n_jobs':self.n_jobs}],\n",
    "                  'CondensedNearestNeighbour':[CondensedNearestNeighbour,{'ratio':self.ratio,\n",
    "                                            'return_indices':self.return_indices,'random_state':self.random_state,\n",
    "                                    'n_neighbors':self.n_neighbors,\n",
    "                                    'n_seeds_S':self.n_seeds_S,'n_jobs':self.n_jobs}],\n",
    "            'EditedNearestNeighbours':[EditedNearestNeighbours,{'ratio':self.ratio,'return_indices':self.return_indices,\n",
    "                                    'random_state':self.random_state,\n",
    "                                    'n_neighbors':self.n_neighbors,'kind_sel':self.kind_sel,'n_jobs':self.n_jobs}],\n",
    "            'RepeatedEditedNearestNeighbours':[RepeatedEditedNearestNeighbours,{'ratio':self.ratio,\n",
    "                    'return_indices':self.return_indices,'random_state':self.random_state,\n",
    "                                            'n_neighbors':self.n_neighbors,'max_iter':self.max_iter,\n",
    "                                                'kind_sel':self.kind_sel,'n_jobs':self.n_jobs}],\n",
    "             'AllKNN':[AllKNN,{'ratio':self.ratio,'return_indices':self.return_indices,'random_state':self.random_state,\n",
    "                               'n_neighbors':self.n_neighbors,'kind_sel':self.kind_sel,\n",
    "                               'allow_minority':self.allow_minority,'n_jobs':self.n_jobs}],\n",
    "            'InstanceHardnessThreshold':[InstanceHardnessThreshold,{'estimator':self.estimator,'ratio':self.ratio,\n",
    "                                    'return_indices':self.return_indices,'random_state':self.random_state,\n",
    "                                    'cv':self.cv,'n_jobs':self.n_jobs}],\n",
    "             'NearMiss':[NearMiss,{'ratio':self.ratio,'return_indices':self.return_indices,'random_state':self.random_state,\n",
    "                                   'version':self.version,'n_neighbors':self.n_neighbors,\n",
    "                                    'ver3_samp_ngh':self.ver3_samp_ngh,'n_neighbors_ver3':self.n_neighbors_ver3,\n",
    "                                   'n_jobs':self.n_jobs}],\n",
    "             'NeighbourhoodCleaningRule':[NeighbourhoodCleaningRule,{'ratio':self.ratio,\n",
    "                                    'return_indices':self.return_indices,\n",
    "                                    'random_state':self.random_state,\n",
    "                                    'n_neighbors':self.n_neighbors,'kind_sel':self.kind_sel,\n",
    "                                    'threshold_cleaning':self.threshold_cleaning,'n_jobs':self.n_jobs}],\n",
    "             'OneSidedSelection':[OneSidedSelection,{'ratio':self.ratio,'return_indices':self.return_indices,\n",
    "                                    'random_state':self.random_state,\n",
    "                                    'n_neighbors':self.n_neighbors,'n_seeds_S':self.n_seeds_S,'n_jobs':self.n_jobs}],\n",
    "            'RandomUnderSampler':[RandomUnderSampler,{'ratio':self.ratio,'return_indices':self.return_indices,\n",
    "                                    'random_state':self.random_state,'replacement':self.replacement}],\n",
    "             'TomekLinks':[TomekLinks,{'ratio':self.ratio,'return_indices':self.return_indices,\n",
    "                                    'random_state':self.random_state,'n_jobs':self.n_jobs}],\n",
    "             'ADASYN':[ADASYN,{'ratio':self.ratio,'random_state':self.random_state,\n",
    "                                   'n_neighbors':self.n_neighbors,'n_jobs':self.n_jobs}],\n",
    "            'RandomOverSampler':[RandomOverSampler,{'ratio':self.ratio,'random_state':self.random_state}],\n",
    "             'SMOTE':[SMOTE,{'ratio':self.ratio,'random_state':self.random_state,\n",
    "                                 'k_neighbors':self.k_neighbors,'m_neighbors':self.m_neighbors,\n",
    "                                 'out_step':self.out_step,'kind':self.kind,'svm_estimator':self.svm_estimator,\n",
    "                                 'n_jobs':self.n_jobs}],\n",
    "             'SMOTEENN':[SMOTEENN,{'ratio':self.ratio,'random_state':self.random_state,'smote':self.smote,\n",
    "                                   'enn':self.enn,'out_step':self.out_step,\n",
    "                                   'kind_smote':self.kind_smote,\n",
    "                                   'n_neighbors':self.n_neighbors,'kind_enn':self.kind_enn,'n_jobs':self.n_jobs}],\n",
    "             'SMOTETomek':[SMOTETomek,{'ratio':self.ratio,'random_state':self.random_state,'smote':self.smote,\n",
    "                                    'tomek':self.tomek,'out_step':self.out_step,\n",
    "                                    'kind_smote':self.kind_smote,'n_jobs':self.n_jobs}],\n",
    "            'BalanceCascade':[BalanceCascade,{'ratio':self.ratio,'return_indices':self.return_indices,\n",
    "                                    'random_state':self.random_state,'n_max_subset':self.n_max_subset,\n",
    "                                    'classifier':self.classifier,'estimator':self.estimator}],\n",
    "             'EasyEnsemble':[EasyEnsemble,{'ratio':self.ratio,'return_indices':self.return_indices,\n",
    "                                    'random_state':self.random_state,'replacement':self.replacement,\n",
    "                                    'n_subsets':self.n_subsets}]}\n",
    "        if len(self.set_sampler_params) !=0:\n",
    "            for key,value in self.set_sampler_params.items():\n",
    "                self.sampler_params[key] = [value[0],value[1]]\n",
    "        \n",
    "        # Assigning the resampler if sampler (sampling method) is not \"Hyperscale\"\n",
    "        if self.sampler is not 'HyperSampler':\n",
    "            self.resampler = self.sampler_params[self.sampler][0](**self.sampler_params[self.sampler][1])\n",
    "            self.performance_scores = None\n",
    "            self.best_perf = None\n",
    "            self.best_sampler = None\n",
    "    \n",
    "    def fit(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if self.target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[self.target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        else:\n",
    "            X = pd.DataFrame(X)\n",
    "            y = pd.DataFrame(y)\n",
    "        \n",
    "        if self.sampler == 'HyperSampler':\n",
    "            # Encode labels if string when needed\n",
    "            if ((self.booster == 'dart' or self.booster == 'gblinear')\\\n",
    "                            and self.objective in HyperSampler.classification_objectives)\\\n",
    "                                                                    or self.scorer in self.need_encoded_y:\n",
    "                if y.iloc[:,0].dtype == object:\n",
    "                    Target_is_string = True\n",
    "                else:\n",
    "                    Target_is_string = False\n",
    "\n",
    "                if Target_is_string == True:\n",
    "                    L_E = LabelEncoder()\n",
    "                    L_E = L_E.fit(y.iloc[:,0].astype(str))\n",
    "                    y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))\n",
    "\n",
    "            # fit model on all training data            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "\n",
    "            if len(self.test_samplers) != 0:\n",
    "                samplers = self.test_samplers\n",
    "            else:\n",
    "                samplers = ['None','Binary_Undersampler','ClusterCentroids', 'CondensedNearestNeighbour',\n",
    "                        'EditedNearestNeighbours','RepeatedEditedNearestNeighbours', 'AllKNN',\n",
    "                        'InstanceHardnessThreshold', 'NearMiss', 'NeighbourhoodCleaningRule', 'OneSidedSelection',\n",
    "                        'RandomUnderSampler', 'TomekLinks', 'ADASYN', 'RandomOverSampler', 'SMOTE', 'SMOTEENN', \n",
    "                        'SMOTETomek']#, 'BalanceCascade', 'EasyEnsemble']\n",
    "            \n",
    "            if len(self.additional_samplers) != 0:\n",
    "                samplers = samplers + self.additional_samplers\n",
    "            \n",
    "            if len(self.excluded_samplers) !=0:\n",
    "                for excluded in self.excluded_samplers:\n",
    "                    if excluded in samplers:\n",
    "                        samplers.remove(excluded)\n",
    "            \n",
    "            if self.objective not in ['binary:logistic','binary:logitraw']\\\n",
    "                                and 'Binary_Undersampler' in samplers:\n",
    "                samplers.remove('Binary_Undersampler')\n",
    "            \n",
    "            Scores = []\n",
    "            for sampler in samplers:\n",
    "                if sampler is 'None':\n",
    "                    X_train_resampled, y_train_resampled = X_train, y_train.values.ravel()\n",
    "\n",
    "                else:\n",
    "                    sampler = self.sampler_params[sampler][0](**self.sampler_params[sampler][1])\n",
    "                    X_train_resampled, y_train_resampled = sampler.fit_sample(X_train, y_train.values.ravel())\n",
    "                    X_train_resampled = pd.DataFrame(X_train_resampled, columns=X.columns)\n",
    "                \n",
    "                if self.booster is 'gbtree':\n",
    "                    if self.objective in HyperSampler.classification_objectives:\n",
    "                        estimator = xgb.XGBClassifier\n",
    "                    if self.objective in HyperSampler.regression_objectives:\n",
    "                        estimator = xgb.XGBRegressor\n",
    "                    selection_model = estimator(**self.best_params)\n",
    "                    selection_model.fit(X_train_resampled, y_train_resampled,\n",
    "                                       eval_set=[(X_test,y_test.values.ravel())], verbose=False)\n",
    "                    # eval model\n",
    "                    # using built-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = selection_model.evals_result()\n",
    "                        scorer_used = list(result['validation_0'].keys())[0]\n",
    "                        #score = np.mean(result['validation_1'][scorer_used])\n",
    "                        score = result['validation_0'][scorer_used][-1]\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = HyperSampler.scoring_functions[self.scorer]\n",
    "                        y_pred = selection_model.predict(X_test)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test.values.ravel()]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used = y_test.values.ravel()\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "                        \n",
    "                elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "                    dtrain_resampled = xgb.DMatrix(data=X_train_resampled, label=y_train_resampled)\n",
    "                    dtest = xgb.DMatrix(data=X_test, label=y_test.values.ravel())\n",
    "\n",
    "                    # eval model\n",
    "                    # using built-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = {}\n",
    "                        selection_model = xgb.train(dtrain=dtrain_resampled, params=self.best_params,\n",
    "                                                    evals=[(dtest, 'eval')], evals_result=result,\n",
    "                                                   verbose_eval=False)\n",
    "                        scorer_used = list(result['eval'].keys())[0]\n",
    "                        #score = np.mean(result['eval'][scorer_used])\n",
    "                        score = result['eval'][scorer_used][-1]\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = HyperSampler.scoring_functions[self.scorer]\n",
    "\n",
    "                        selection_model = xgb.train(dtrain=dtrain_resampled, params=self.best_params)\n",
    "                        y_pred = selection_model.predict(dtest)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test.values.ravel()]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used =  y_test.values.ravel()\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "\n",
    "            # building table of performance scores\n",
    "            if self.scorer == 'Auto':\n",
    "                #self.scorer = 'Average test ' + scorer_used\n",
    "                self.scorer = 'Best test ' + scorer_used\n",
    "            self.performance_scores = pd.DataFrame()\n",
    "            self.performance_scores['Samplers'] = samplers\n",
    "            self.performance_scores[self.scorer] = Scores\n",
    "\n",
    "            # Best resampler: returns max possible model performance \n",
    "            if self.scorer in HyperSampler.the_higher_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == max(self.performance_scores[self.scorer])]\n",
    "            elif self.scorer in HyperSampler.the_lower_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == min(self.performance_scores[self.scorer])]\n",
    "                \n",
    "            # assigning the best-performing resampler as self.resampler\n",
    "            self.best_sampler = self.best_perf.iloc[0,0]\n",
    "            if self.best_sampler is not 'None':\n",
    "                self.resampler = self.sampler_params[self.best_sampler][0](**self.sampler_params[self.best_sampler][1])\n",
    "            else:\n",
    "                self.resampler = None\n",
    "            \n",
    "            # reverse label encoding\n",
    "            if ((self.booster == 'dart' or self.booster == 'gblinear')\\\n",
    "                            and self.objective in HyperSampler.classification_objectives)\\\n",
    "                                                                    or self.scorer in self.need_encoded_y:\n",
    "                if Target_is_string == True:\n",
    "                    y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])\n",
    "            \n",
    "            return self\n",
    "            \n",
    "        else:\n",
    "            return self\n",
    "    \n",
    "    def sample(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        else:\n",
    "            X = pd.DataFrame(X)\n",
    "            y = pd.DataFrame(y)\n",
    "\n",
    "        if self.best_sampler is not 'None':\n",
    "            X_resampled, y_resampled = self.resampler.fit_sample(X, y.values.ravel())\n",
    "            return pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=y.columns)\n",
    "        else:\n",
    "            return pd.DataFrame(X), pd.DataFrame(y)\n",
    "        \n",
    "    def fit_sample(self, X, y=None, target=None):\n",
    "        self.fit(X,y, target=target)\n",
    "        return self.sample(X, y, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> HyperSampler in action  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JanErish\\Anaconda3\\lib\\site-packages\\imblearn\\utils\\deprecation.py:50: DeprecationWarning: 'k' is deprecated from 0.2 and will be removed in 0.4. Use 'k_neighbors' instead.\n",
      "  category=DeprecationWarning)\n",
      "C:\\Users\\JanErish\\Anaconda3\\lib\\site-packages\\imblearn\\utils\\deprecation.py:50: DeprecationWarning: 'm' is deprecated from 0.2 and will be removed in 0.4. Use 'm_neighbors' instead.\n",
      "  category=DeprecationWarning)\n",
      "C:\\Users\\JanErish\\Anaconda3\\lib\\site-packages\\imblearn\\utils\\deprecation.py:50: DeprecationWarning: 'k' is deprecated from 0.2 and will be removed in 0.4. Use 'k_neighbors' instead.\n",
      "  category=DeprecationWarning)\n",
      "C:\\Users\\JanErish\\Anaconda3\\lib\\site-packages\\imblearn\\utils\\deprecation.py:50: DeprecationWarning: 'm' is deprecated from 0.2 and will be removed in 0.4. Use 'm_neighbors' instead.\n",
      "  category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of resampled X_train: 500\n",
      "Length of resampled y_train: 500\n",
      "\u001b[1m\u001b[31mResampling Results:\u001b[0m\u001b[0m\n",
      "Counter({0: 250, 1: 250})\n"
     ]
    }
   ],
   "source": [
    "HS = HyperSampler(booster='gbtree',objective='binary:logistic', scorer='recall_score',\n",
    "                  excluded_samplers=[])#,\n",
    "                 #test_samplers=['None','SMOTE','SMOTEENN','SMOTETomek'])\n",
    "HS.fit(X_imputed, y_imputed)\n",
    "X_resampled, y_resampled = HS.sample(X_imputed, y_imputed)\n",
    "print(\"Length of resampled X_train: {}\".format(len(X_resampled)))\n",
    "print(\"Length of resampled y_train: {}\".format(len(y_resampled)))\n",
    "print(f'\\033[1m{Fore.RED}Resampling Results:{Style.RESET_ALL}\\033[0m')\n",
    "print(Counter(y_resampled.values.ravel()))\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mPerformance scores of resampling methods:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Samplers</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Binary_Undersampler</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterCentroids</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CondensedNearestNeighbour</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EditedNearestNeighbours</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RepeatedEditedNearestNeighbours</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AllKNN</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>InstanceHardnessThreshold</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NeighbourhoodCleaningRule</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OneSidedSelection</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomUnderSampler</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TomekLinks</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ADASYN</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RandomOverSampler</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SMOTEENN</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SMOTETomek</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Samplers  recall_score\n",
       "0                              None          0.68\n",
       "1               Binary_Undersampler          0.42\n",
       "2                  ClusterCentroids          0.96\n",
       "3         CondensedNearestNeighbour          0.86\n",
       "4           EditedNearestNeighbours          0.70\n",
       "5   RepeatedEditedNearestNeighbours          0.68\n",
       "6                            AllKNN          0.68\n",
       "7         InstanceHardnessThreshold          0.82\n",
       "8                          NearMiss          0.94\n",
       "9         NeighbourhoodCleaningRule          0.68\n",
       "10                OneSidedSelection          0.64\n",
       "11               RandomUnderSampler          0.92\n",
       "12                       TomekLinks          0.64\n",
       "13                           ADASYN          0.82\n",
       "14                RandomOverSampler          0.88\n",
       "15                            SMOTE          0.88\n",
       "16                         SMOTEENN          0.88\n",
       "17                       SMOTETomek          0.88"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Performance scores of resampling methods:{Style.RESET_ALL}\\033[0m')\n",
    "HS.performance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mBest Resampling Method:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Samplers</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterCentroids</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Samplers  recall_score\n",
       "2  ClusterCentroids          0.96"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Best Resampling Method:{Style.RESET_ALL}\\033[0m')\n",
    "HS.best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scale\"></a>\n",
    "\n",
    "![Imgur](https://i.imgur.com/TTh0Ez6.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>Feature Scaling</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "![Imgur](https://i.imgur.com/qe9mKrF.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\">\n",
    "class&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Feature_Hyperscaler</span><i><span style=\"font-size:16px;font-weight:bold;color:#3366cc\">(best_params={}, booster='gbtree', objective='reg:logistic', scorer='accuracy_score', set_scaler_params={})</span></i>  \n",
    " \n",
    "\"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "        Options for scaling methods (parameter \"scale=\") are the following:\n",
    "        Normal, Standard, MinMax, Normal, Robust, MaxAbs, and Quantile.\n",
    "        \n",
    "        If scale = 'Hyperscale',\n",
    "            every scaling method will be applied and tested\n",
    "            to determine which method best contributes to model performance.\n",
    "            \n",
    "            best_params = {dictionary of best parameters for model tuning continuity}\n",
    "            \n",
    "            scorer = pass desired Sklearn.metrics scorer as a string\n",
    "                passing scorer='Auto' entails the use of built-in XGBoost evaluation metrics\n",
    "            \n",
    "            The scaling method that contributes to the best score will be applied to the data to be returned.\n",
    "        \n",
    "        This feature scaling transformer works with any of the three boosters as base learners:\n",
    "        -gbtree\n",
    "        -gblinear\n",
    "        -dart\n",
    "        \n",
    "        This class requires that X (features or independent variables) has already been encoded and Imputed. \n",
    "        \n",
    "        Tuning requires the target (y) to also be passed.\n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit_transform method is given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Parameters:  </span>\n",
    "\n",
    "### scale: str, default='Hyperscale'\n",
    "* If 'Hyperscale', fit() will perform tests to determine which sklearn feature scaling method returns the best performance.  \n",
    "* also used to specify a single sklearn feature scaling method\n",
    "\n",
    "### best_params: dict, default={}\n",
    "* dictionary of parameters to be used as kwargs for XGBoost models to be used for evaluating performance\n",
    "\n",
    "### booster: str, default='gbtree'\n",
    "* specifies the xgboost booster to be used for the evaluation models ('gbtree','gblinear', or 'dart')  \n",
    "\n",
    "### objective: str, default='reg:logistic'\n",
    "* specifies the xgboost objective to be used for the evaluation models\n",
    "* see XGBoost documentation for more details\n",
    "\n",
    "### scorer: str, default='Auto'\n",
    "* determines the model evaluation metric used\n",
    "* if 'Auto', built-in XGBoost metrics will be used\n",
    "* otherwise, existic sklearn metrics can be passed as strings\n",
    "\n",
    "### set_scaler_params: dictionary, default={}\n",
    "* allows user to pass a dictionary for setting parameters of feature scaling methods\n",
    "* if left empty, a default dictionary will be used\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Attributes:  </span>\n",
    "\n",
    "### scoring_functions: dictionary\n",
    "* dictonary of metrics available for use\n",
    "\n",
    "### classification_objectives: list\n",
    "* list of XGBoost objectives considered as for classification\n",
    "\n",
    "### regression_objectives: list\n",
    "* list of XGBoost objectives considered as for regression\n",
    "\n",
    "### the_higher_the_better: list\n",
    "* list of metrics of which an increase means improvement in performance\n",
    "\n",
    "### the_lower_the_better: list\n",
    "* list of metrics of which a decrease means improvement in performance\n",
    "\n",
    "### need_encoded_y: list\n",
    "* list of metrics that require an encoded y\n",
    "* automatically dealt with my the class\n",
    "\n",
    "### the_lower_the_better: list\n",
    "* list of metrics of which a decrease means improvement in performance\n",
    "\n",
    "### scaler_params: dictionary\n",
    "* default dictionary of parameters used for feature scaling methods\n",
    "\n",
    "### scale: str\n",
    "* name or determined feature scaling method\n",
    "\n",
    "### scaler: obj\n",
    "* scaler object\n",
    "* fitted to X after .fit()\n",
    "* should be used later to also transform test set or new data\n",
    "\n",
    "### performance_scores: pandas DataFrame\n",
    "* results of testing\n",
    "\n",
    "### best_perf: pandas DataFrame\n",
    "* highest performer/s\n",
    "\n",
    "### best_sampler: str\n",
    "* name of feature scaling method deemed best\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Methods:  </span>\n",
    "\n",
    "### .fit(X, y=None, target=None)  \n",
    "Either fits the pre-determined scaling method to the data or performs tests first before fitting the best method to the data.  \n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "* target:*string* (optional)  \n",
    "    * name of target column  \n",
    "\n",
    "### .transform(X, y=None, target=None)  \n",
    "Using statistics learned from fitted data, returns scaled data.\n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "* target:*string* (optional)  \n",
    "    * name of target column  \n",
    "\n",
    "**Returns:**\n",
    "* X_resampled: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * rescaled or untouched independent variables / features\n",
    "* y_resampled: *pandas DataFrame, shape (n_samples,)*    \n",
    "    * untouched dependent variable\n",
    "\n",
    "### .fit_transform(X, y=None, target=None)  \n",
    "Streamlines fit() and transform() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaler\n",
    "\n",
    "class Feature_Hyperscaler:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "        Options for scaling methods (parameter \"scale=\") are the following:\n",
    "        Normal, Standard, MinMax, Normal, Robust, MaxAbs, and Quantile.\n",
    "        \n",
    "        If scale = 'Hyperscale',\n",
    "            every scaling method will be applied and tested\n",
    "            to determine which method best contributes to model performance.\n",
    "            \n",
    "            best_params = {dictionary of best parameters for model tuning continuity}\n",
    "            \n",
    "            scorer = pass desired Sklearn.metrics scorer as a string\n",
    "                passing scorer='Auto' entails the use of built-in XGBoost evaluation metrics\n",
    "            \n",
    "            The scaling method that contributes to the best score will be applied to the data to be returned.\n",
    "        \n",
    "        This feature selection transformer works only with any of the three boosters as base learners:\n",
    "        -gbtree\n",
    "        -gblinear\n",
    "        -dart\n",
    "        \n",
    "        This class requires that X (features or independent variables) has already been encoded and Imputed. \n",
    "        \n",
    "        Tuning requires the target (y) to also be passed.\n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit_transform method is given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "    \n",
    "    # Compatible model evaluation metrics: Sklearn metrics\n",
    "    scoring_functions = {'accuracy_score':accuracy_score,'f1_score':f1_score, 'hamming_loss':hamming_loss,\n",
    "        'jaccard_similarity_score':jaccard_similarity_score, 'log_loss':log_loss, 'matthews_corrcoef':matthews_corrcoef,\n",
    "                 'precision_score':precision_score, 'recall_score':recall_score, 'zero_one_loss':zero_one_loss,\n",
    "                'explained_variance_score':explained_variance_score, 'mean_absolute_error':mean_absolute_error,\n",
    "                 'mean_squared_error':mean_squared_error, 'mean_squared_log_error':mean_squared_log_error,\n",
    "                 'median_absolute_error':median_absolute_error, 'r2_score':r2_score}\n",
    "\n",
    "    classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                                'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "    regression_objectives = ['reg:linear','count:poisson','reg:gamma', 'reg:tweedie']\n",
    "    \n",
    "    # metrics with which higher value = higher model performance\n",
    "    the_higher_the_better = ['accuracy_score','f1_score','jaccard_similarity_score',\n",
    "                                  'precision_score','recall_score',\n",
    "                                'explained_variance_score','r2_score','Best test auc',\n",
    "                                'Best test ndcg','Best test map']\n",
    "    # metrics with which lower value = higher model performance\n",
    "    the_lower_the_better = ['hamming_loss', 'log_loss','matthews_corrcoef','zero_one_loss','mean_absolute_error',\n",
    "                           'mean_squared_error','mean_squared_log_error','median_absolute_error',\n",
    "                           'Best test error','Best test rmse','Best test mae','Best test log loss',\n",
    "                           'Best test merror','Best test mlogloss']\n",
    "\n",
    "    # metrics that require encoded target variable\n",
    "    need_encoded_y = ['recall_score','precision_score','f1_score']\n",
    "    \n",
    "    # parameters to inherited from mother class\n",
    "    set_scaler_params='N/A'\n",
    "    scale='N/A'\n",
    "    best_params='N/A'\n",
    "    booster='N/A'\n",
    "    objective='N/A'\n",
    "    scorer='N/A'\n",
    "    copy='N/A'\n",
    "    with_mean='N/A'\n",
    "    with_std='N/A'\n",
    "    feature_range='N/A'\n",
    "    norm='N/A'\n",
    "    with_centering='N/A'\n",
    "    with_scaling='N/A'\n",
    "    quantile_range='N/A'\n",
    "    n_quantiles='N/A'\n",
    "    output_distribution='N/A'\n",
    "    ignore_implicit_zeros='N/A'\n",
    "    subsample='N/A'\n",
    "    random_state='N/A'\n",
    "    test_size='N/A'\n",
    "    \n",
    "    def __init__(self, scale=\"Hyperscale\", best_params={},\n",
    "                 booster='gbtree', objective='reg:logistic', scorer='accuracy_score',\n",
    "                 set_scaler_params={},\n",
    "                 copy=True, with_mean=True, with_std=True,\n",
    "                feature_range=(0, 1),\n",
    "                 norm='l2',\n",
    "                 with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0),\n",
    "                 n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False,\n",
    "                 subsample=100000, random_state=69, test_size=0.2,\n",
    "                # values passed for these parameters will never be used unless: class.parameter is made None\n",
    "                scoring_functions=None, classification_objectives=None, need_encoded_y=None,\n",
    "                 the_higher_the_better=None, the_lower_the_better=None):\n",
    "        \n",
    "        # assigning parameters as instance variables\n",
    "        varses = list(vars(Feature_Hyperscaler).keys())\n",
    "        self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "        class_name = \"Feature_Hyperscaler\"+\".\"\n",
    "        for v in self.variables:\n",
    "            # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "            if eval(\"%s != 'N/A'\" % (class_name+v)) is True:\n",
    "                exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "            # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "            else:\n",
    "                exec(\"self.%s = %s\" % (v, v))\n",
    "        \n",
    "        # determining the model build used for feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            if self.objective in Feature_Hyperscaler.classification_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBClassifier(**self.best_params)\n",
    "            elif self.objective in Feature_Hyperscaler.regression_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBRegressor(**self.best_params)\n",
    "        elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "            self.best_params['booster'] = self.booster\n",
    "            self.best_params['objective'] = self.objective\n",
    "        \n",
    "        self.scaler_params={'StandardScaler':[StandardScaler,{'copy':self.copy, \n",
    "                                              'with_mean':self.with_mean, 'with_std':self.with_std}],\n",
    "                          'MinMaxScaler':[MinMaxScaler,{'feature_range':self.feature_range, 'copy':self.copy}],\n",
    "                          'Normalizer':[Normalizer,{'norm':self.norm, 'copy':self.copy}],\n",
    "                            'RobustScaler':[RobustScaler,{'with_centering':self.with_centering,\n",
    "                                            'with_scaling':self.with_scaling,\n",
    "                                             'quantile_range':self.quantile_range, 'copy':self.copy}],\n",
    "                           'MaxAbsScaler':[MaxAbsScaler,{'copy':self.copy}],\n",
    "                           'QuantileTransformer':[QuantileTransformer,{'n_quantiles':self.n_quantiles, \n",
    "                                                  'output_distribution':self.output_distribution,\n",
    "                                                  'ignore_implicit_zeros':self.ignore_implicit_zeros,\n",
    "                                                  'subsample':self.subsample,'random_state':self.random_state}]}\n",
    "        if len(self.set_scaler_params) !=0:\n",
    "            for key,value in self.set_scaler_params.items():\n",
    "                self.scaler_params[key] = [value[0],value[1]]\n",
    "                \n",
    "        # Assigning the scaler if scale is not \"Hyperscale\"\n",
    "        if self.scale is not 'Hyperscale':\n",
    "            self.scaler = self.scaler_params[self.scale][0](**self.scaler_params[self.scale][1])\n",
    "            self.performance_scores = None\n",
    "            self.best_perf = None\n",
    "            self.best_scaler = None\n",
    "    \n",
    "    def fit(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if self.target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[self.target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        \n",
    "        if self.scale == 'Hyperscale':\n",
    "            # Encode labels if string when needed\n",
    "            if ((self.booster == 'dart' or self.booster == 'gblinear')\\\n",
    "                            and self.objective in Feature_Hyperscaler.classification_objectives)\\\n",
    "                                                                    or self.scorer in self.need_encoded_y:\n",
    "                if y.iloc[:,0].dtype == object:\n",
    "                    Target_is_string = True\n",
    "                else:\n",
    "                    Target_is_string = False\n",
    "\n",
    "                if Target_is_string == True:\n",
    "                    L_E = LabelEncoder()\n",
    "                    L_E = L_E.fit(y.iloc[:,0].astype(str))\n",
    "                    y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))\n",
    "\n",
    "            # fit model on all training data            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "            \n",
    "            scales = ['None','StandardScaler','MinMaxScaler',#'Normalizer',\n",
    "                      'RobustScaler','MaxAbsScaler','QuantileTransformer']\n",
    "            Scores = []\n",
    "            for scale in scales:\n",
    "                if scale is 'None':\n",
    "                    X_train_scaled = X_train\n",
    "                    X_test_scaled = X_test\n",
    "                else:\n",
    "                    scaler = self.scaler_params[scale][0](**self.scaler_params[scale][1])\n",
    "                    scaler.fit(X_train)\n",
    "                    X_train_scaled = scaler.transform(X_train)\n",
    "                    X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                if self.booster is 'gbtree':\n",
    "                    if self.objective in Feature_Hyperscaler.classification_objectives:\n",
    "                        estimator = xgb.XGBClassifier\n",
    "                    if self.objective in Feature_Hyperscaler.regression_objectives:\n",
    "                        estimator = xgb.XGBRegressor\n",
    "                    selection_model = estimator(**self.best_params)\n",
    "                    selection_model.fit(X_train_scaled, y_train.values.ravel(),\n",
    "                                       eval_set=[(X_test_scaled,y_test.values.ravel())], verbose=False)\n",
    "                    # eval model\n",
    "                    # using built-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = selection_model.evals_result()\n",
    "                        scorer_used = list(result['validation_0'].keys())[0]\n",
    "                        #score = np.mean(result['validation_1'][scorer_used])\n",
    "                        score = result['validation_0'][scorer_used][-1]\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = Feature_Hyperscaler.scoring_functions[self.scorer]\n",
    "                        y_pred = selection_model.predict(X_test_scaled)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used = y_test.iloc[:,0]\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "                        \n",
    "                elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "                    dtrain_scaled = xgb.DMatrix(data=X_train_scaled, label=y_train)\n",
    "                    dtest_scaled = xgb.DMatrix(data=X_test_scaled, label=y_test)\n",
    "\n",
    "                    # eval model\n",
    "                    # using built-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = {}\n",
    "                        selection_model = xgb.train(dtrain=dtrain_scaled, params=self.best_params,\n",
    "                                                    evals=[(dtest_scaled, 'eval')], evals_result=result,\n",
    "                                                   verbose_eval=False)\n",
    "                        scorer_used = list(result['eval'].keys())[0]\n",
    "                        #score = np.mean(result['eval'][scorer_used])\n",
    "                        score = result['eval'][scorer_used][-1]\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = Feature_Hyperscaler.scoring_functions[self.scorer]\n",
    "\n",
    "                        selection_model = xgb.train(dtrain=dtrain_scaled, params=self.best_params)\n",
    "                        y_pred = selection_model.predict(dtest_scaled)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used = y_test.iloc[:,0]\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "\n",
    "            # building table of performance scores\n",
    "            if self.scorer == 'Auto':\n",
    "                self.scorer = 'Best test ' + scorer_used\n",
    "            self.performance_scores = pd.DataFrame()\n",
    "            self.performance_scores['Scalers'] = scales\n",
    "            self.performance_scores[self.scorer] = Scores\n",
    "\n",
    "            # Best scaler: returns max possible model performance \n",
    "            if self.scorer in Feature_Hyperscaler.the_higher_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == max(self.performance_scores[self.scorer])]\n",
    "            elif self.scorer in Feature_Hyperscaler.the_lower_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == min(self.performance_scores[self.scorer])]\n",
    "                \n",
    "            # assigning the best-performing scaler as scaler\n",
    "            self.best_scaler = self.best_perf.iloc[0,0]\n",
    "            if self.best_scaler is not 'None':\n",
    "                self.scaler = self.scaler_params[self.best_scaler][0](**self.scaler_params[self.best_scaler][1])\n",
    "                # Fitting the scaler\n",
    "                self.scaler.fit(X)\n",
    "            else:\n",
    "                self.scaler = None\n",
    "            \n",
    "            # reverse label encoding\n",
    "            if ((self.booster == 'dart' or self.booster == 'gblinear')\\\n",
    "                            and self.objective in Feature_Hyperscaler.classification_objectives)\\\n",
    "                                                                    or self.scorer in self.need_encoded_y:\n",
    "                if Target_is_string == True:\n",
    "                    y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])\n",
    "            \n",
    "            return self\n",
    "            \n",
    "        else:\n",
    "            # Fitting the scaler\n",
    "            self.scaler.fit(X)\n",
    "            return self\n",
    "    \n",
    "    def transform(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = pd.DataFrame(X.drop(target, axis =1))\n",
    "        if self.best_scaler is not 'None':\n",
    "            X_scaled = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n",
    "            return X_scaled, pd.DataFrame(y)\n",
    "        else:\n",
    "            return pd.DataFrame(X), pd.DataFrame(y)\n",
    "        \n",
    "    def fit_transform(self, X, y=None, target=None):\n",
    "        self.fit(X,y, target=target)\n",
    "        return self.transform(X, y, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Feature_Hyperscaler in action  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mScaled X_train:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152930.253012</td>\n",
       "      <td>0.552952</td>\n",
       "      <td>0.036295</td>\n",
       "      <td>-0.152578</td>\n",
       "      <td>0.273752</td>\n",
       "      <td>0.205888</td>\n",
       "      <td>0.057598</td>\n",
       "      <td>-0.145254</td>\n",
       "      <td>-0.114393</td>\n",
       "      <td>1.242381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106660</td>\n",
       "      <td>-0.007431</td>\n",
       "      <td>0.124388</td>\n",
       "      <td>0.048337</td>\n",
       "      <td>-0.030963</td>\n",
       "      <td>-0.172096</td>\n",
       "      <td>0.118541</td>\n",
       "      <td>-0.024249</td>\n",
       "      <td>-0.020175</td>\n",
       "      <td>56.313754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43355.565217</td>\n",
       "      <td>-0.382524</td>\n",
       "      <td>-0.008179</td>\n",
       "      <td>0.350821</td>\n",
       "      <td>0.061924</td>\n",
       "      <td>-0.290693</td>\n",
       "      <td>0.177480</td>\n",
       "      <td>-0.043012</td>\n",
       "      <td>0.096769</td>\n",
       "      <td>-0.076569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076488</td>\n",
       "      <td>0.033170</td>\n",
       "      <td>-0.096371</td>\n",
       "      <td>-0.025172</td>\n",
       "      <td>0.011808</td>\n",
       "      <td>0.112465</td>\n",
       "      <td>0.018228</td>\n",
       "      <td>-0.007483</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>113.313522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253331.879464</td>\n",
       "      <td>0.060237</td>\n",
       "      <td>0.021565</td>\n",
       "      <td>-0.511447</td>\n",
       "      <td>-0.236409</td>\n",
       "      <td>0.169967</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.094860</td>\n",
       "      <td>0.020066</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052852</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>0.011813</td>\n",
       "      <td>0.051461</td>\n",
       "      <td>0.025971</td>\n",
       "      <td>-0.114771</td>\n",
       "      <td>-0.018566</td>\n",
       "      <td>-0.054684</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>82.084517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81717.660622</td>\n",
       "      <td>-0.035027</td>\n",
       "      <td>-0.066331</td>\n",
       "      <td>0.510752</td>\n",
       "      <td>0.131712</td>\n",
       "      <td>-0.212058</td>\n",
       "      <td>-0.021352</td>\n",
       "      <td>-0.086360</td>\n",
       "      <td>0.030674</td>\n",
       "      <td>-0.037935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047827</td>\n",
       "      <td>-0.022004</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>-0.069526</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>0.111146</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>-0.014216</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>81.272664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>197407.898123</td>\n",
       "      <td>0.202502</td>\n",
       "      <td>-0.064094</td>\n",
       "      <td>-0.535840</td>\n",
       "      <td>-0.181412</td>\n",
       "      <td>0.192768</td>\n",
       "      <td>-0.036185</td>\n",
       "      <td>0.118597</td>\n",
       "      <td>-0.042756</td>\n",
       "      <td>-0.066639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044944</td>\n",
       "      <td>0.041093</td>\n",
       "      <td>0.100095</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.017195</td>\n",
       "      <td>-0.080370</td>\n",
       "      <td>-0.034930</td>\n",
       "      <td>0.022542</td>\n",
       "      <td>-0.017435</td>\n",
       "      <td>97.967130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18027.370370</td>\n",
       "      <td>-0.106030</td>\n",
       "      <td>-0.016584</td>\n",
       "      <td>0.494407</td>\n",
       "      <td>0.076138</td>\n",
       "      <td>-0.325488</td>\n",
       "      <td>0.079424</td>\n",
       "      <td>-0.126039</td>\n",
       "      <td>0.081186</td>\n",
       "      <td>-0.190041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069033</td>\n",
       "      <td>0.039784</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>-0.035586</td>\n",
       "      <td>0.021745</td>\n",
       "      <td>0.119676</td>\n",
       "      <td>-0.019521</td>\n",
       "      <td>-0.011712</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>105.614976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>142772.716562</td>\n",
       "      <td>0.030177</td>\n",
       "      <td>0.080735</td>\n",
       "      <td>0.040203</td>\n",
       "      <td>-0.012049</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>-0.021354</td>\n",
       "      <td>-0.021753</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019619</td>\n",
       "      <td>-0.006042</td>\n",
       "      <td>-0.000722</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>-0.000622</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>59.971786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>233146.284653</td>\n",
       "      <td>0.227286</td>\n",
       "      <td>-0.052158</td>\n",
       "      <td>-0.564621</td>\n",
       "      <td>-0.161485</td>\n",
       "      <td>0.184585</td>\n",
       "      <td>-0.131005</td>\n",
       "      <td>-0.047801</td>\n",
       "      <td>-0.040014</td>\n",
       "      <td>-0.013413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011769</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>0.068010</td>\n",
       "      <td>0.034763</td>\n",
       "      <td>-0.029526</td>\n",
       "      <td>-0.120826</td>\n",
       "      <td>-0.002214</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>79.378424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>111023.021898</td>\n",
       "      <td>-0.286304</td>\n",
       "      <td>0.121140</td>\n",
       "      <td>0.416509</td>\n",
       "      <td>0.116303</td>\n",
       "      <td>-0.199018</td>\n",
       "      <td>0.059974</td>\n",
       "      <td>-0.061782</td>\n",
       "      <td>0.071597</td>\n",
       "      <td>-0.191857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008555</td>\n",
       "      <td>-0.036340</td>\n",
       "      <td>-0.123752</td>\n",
       "      <td>-0.027434</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>0.071632</td>\n",
       "      <td>0.049134</td>\n",
       "      <td>-0.002538</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>83.565483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>277146.500000</td>\n",
       "      <td>0.066163</td>\n",
       "      <td>0.296455</td>\n",
       "      <td>-0.586701</td>\n",
       "      <td>-0.089188</td>\n",
       "      <td>0.278897</td>\n",
       "      <td>-0.104586</td>\n",
       "      <td>0.092952</td>\n",
       "      <td>-0.013246</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>-0.016126</td>\n",
       "      <td>0.075743</td>\n",
       "      <td>0.027539</td>\n",
       "      <td>-0.020466</td>\n",
       "      <td>-0.129376</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>-0.020405</td>\n",
       "      <td>-0.012373</td>\n",
       "      <td>69.956420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index        V1        V2        V3        V4        V5        V6  \\\n",
       "0  152930.253012  0.552952  0.036295 -0.152578  0.273752  0.205888  0.057598   \n",
       "1   43355.565217 -0.382524 -0.008179  0.350821  0.061924 -0.290693  0.177480   \n",
       "2  253331.879464  0.060237  0.021565 -0.511447 -0.236409  0.169967  0.006083   \n",
       "3   81717.660622 -0.035027 -0.066331  0.510752  0.131712 -0.212058 -0.021352   \n",
       "4  197407.898123  0.202502 -0.064094 -0.535840 -0.181412  0.192768 -0.036185   \n",
       "5   18027.370370 -0.106030 -0.016584  0.494407  0.076138 -0.325488  0.079424   \n",
       "6  142772.716562  0.030177  0.080735  0.040203 -0.012049  0.033646 -0.021354   \n",
       "7  233146.284653  0.227286 -0.052158 -0.564621 -0.161485  0.184585 -0.131005   \n",
       "8  111023.021898 -0.286304  0.121140  0.416509  0.116303 -0.199018  0.059974   \n",
       "9  277146.500000  0.066163  0.296455 -0.586701 -0.089188  0.278897 -0.104586   \n",
       "\n",
       "         V7        V8        V9     ...           V20       V21       V22  \\\n",
       "0 -0.145254 -0.114393  1.242381     ...     -0.106660 -0.007431  0.124388   \n",
       "1 -0.043012  0.096769 -0.076569     ...      0.076488  0.033170 -0.096371   \n",
       "2  0.094860  0.020066 -0.015625     ...     -0.052852  0.012124  0.011813   \n",
       "3 -0.086360  0.030674 -0.037935     ...      0.047827 -0.022004  0.001229   \n",
       "4  0.118597 -0.042756 -0.066639     ...     -0.044944  0.041093  0.100095   \n",
       "5 -0.126039  0.081186 -0.190041     ...      0.069033  0.039784  0.032000   \n",
       "6 -0.021753  0.011134  0.010417     ...     -0.019619 -0.006042 -0.000722   \n",
       "7 -0.047801 -0.040014 -0.013413     ...     -0.011769  0.009494  0.068010   \n",
       "8 -0.061782  0.071597 -0.191857     ...      0.008555 -0.036340 -0.123752   \n",
       "9  0.092952 -0.013246  0.019366     ...      0.009869 -0.016126  0.075743   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28      Amount  \n",
       "0  0.048337 -0.030963 -0.172096  0.118541 -0.024249 -0.020175   56.313754  \n",
       "1 -0.025172  0.011808  0.112465  0.018228 -0.007483  0.001840  113.313522  \n",
       "2  0.051461  0.025971 -0.114771 -0.018566 -0.054684  0.004101   82.084517  \n",
       "3 -0.069526  0.049565  0.111146  0.030800 -0.014216  0.016189   81.272664  \n",
       "4  0.043100  0.017195 -0.080370 -0.034930  0.022542 -0.017435   97.967130  \n",
       "5 -0.035586  0.021745  0.119676 -0.019521 -0.011712  0.007288  105.614976  \n",
       "6  0.001220  0.001536  0.004685 -0.000676 -0.000622  0.000895   59.971786  \n",
       "7  0.034763 -0.029526 -0.120826 -0.002214  0.000408  0.000178   79.378424  \n",
       "8 -0.027434  0.021983  0.071632  0.049134 -0.002538  0.006140   83.565483  \n",
       "9  0.027539 -0.020466 -0.129376  0.012070 -0.020405 -0.012373   69.956420  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FHS = Feature_Hyperscaler(booster='gbtree',objective='binary:logistic', scorer='recall_score')\n",
    "FHS.fit(X_resampled, y_resampled)\n",
    "X_scaled, y_scaled = FHS.transform(X_resampled, y_resampled)\n",
    "print(f'\\033[1m{Fore.RED}Scaled X_train:{Style.RESET_ALL}\\033[0m')\n",
    "X_scaled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mPerformance scores of scaling methods:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scalers</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MaxAbsScaler</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QuantileTransformer</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Scalers  recall_score\n",
       "0                 None          0.96\n",
       "1       StandardScaler          0.96\n",
       "2         MinMaxScaler          0.96\n",
       "3         RobustScaler          0.96\n",
       "4         MaxAbsScaler          0.96\n",
       "5  QuantileTransformer          0.96"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Performance scores of scaling methods:{Style.RESET_ALL}\\033[0m')\n",
    "FHS.performance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mBest scaling method:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scalers</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MaxAbsScaler</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QuantileTransformer</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Scalers  recall_score\n",
       "0                 None          0.96\n",
       "1       StandardScaler          0.96\n",
       "2         MinMaxScaler          0.96\n",
       "3         RobustScaler          0.96\n",
       "4         MaxAbsScaler          0.96\n",
       "5  QuantileTransformer          0.96"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Best scaling method:{Style.RESET_ALL}\\033[0m')\n",
    "FHS.best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extract\"></a>\n",
    "\n",
    "![Imgur](https://i.imgur.com/3Mk1UST.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>Feature Extraction</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "![Imgur](https://i.imgur.com/K19NTwq.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\">\n",
    "class&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Feature_Hyperextractor</span><i><span style=\"font-size:16px;font-weight:bold;color:#3366cc\">(extract=\"Hyperextractor\", best_params={}, booster='gbtree', objective='reg:logistic', scorer='Auto', test_extractors=[], excluded_extractors=[], additional_extractors=[], set_extract_params={}, NLP=False, Sparse=False)</span></i>  \n",
    " \n",
    "\"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "        Options for feature extraction methods (parameter extract='') are the following:\n",
    "            FactorAnalysis, FastICA, IncrementalPCA, KernelPCA, LatentDirichletAllocation, \n",
    "            MiniBatchDictionaryLearning, MiniBatchSparsePCA, NMF, PCA, SparsePCA,\n",
    "            TruncatedSVD, LinearDiscriminantAnalysis\n",
    "\n",
    ".\n",
    "        \n",
    "        If extract = 'Hyperextractor',\n",
    "            every extraction method will be applied and tested\n",
    "            to determine which method best contributes to model performance.\n",
    "            \n",
    "            best_params = {dictionary of best parameters for model tuning continuity}\n",
    "            \n",
    "            scorer = pass desired Sklearn.metrics scorer as a string\n",
    "                passing scorer='Auto' entails the use of built-in XGBoost evaluation metrics\n",
    "            \n",
    "            The extraction method that contributes to the best score will be applied to the data to be returned.\n",
    "            \n",
    "            Feature_Hyperextractor.extractor = chosen resampling class assigned after instantiation \n",
    "                                                                        or after fit() if extract = 'Feature_Hyperextractor'\n",
    "        \n",
    "        This feature extraction transformer works with any of the three boosters as base learners:\n",
    "        -gbtree\n",
    "        -gblinear\n",
    "        -dart\n",
    "        \n",
    "        This class requires that X (features or independent variables) has already been encoded and Imputed. \n",
    "        \n",
    "        This class also requires the target (y) to also be passed.\n",
    "        \n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit, transform, and fit_transform methods are given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Parameters:  </span>\n",
    "\n",
    "### extract: str, default='Hyperextractor'\n",
    "* If 'Hyperextractor', .transform() will perform tests to determine which sklearn feature extraction method returns the best performance.  \n",
    "* also used to specify a single sklearn feature extraction method\n",
    "\n",
    "### best_params: dict, default={}\n",
    "* dictionary of parameters to be used as kwargs for XGBoost models to be used for evaluating performance\n",
    "\n",
    "### booster: str, default='gbtree'\n",
    "* specifies the xgboost booster to be used for the evaluation models ('gbtree','gblinear', or 'dart')  \n",
    "\n",
    "### objective: str, default='reg:logistic'\n",
    "* specifies the xgboost objective to be used for the evaluation models\n",
    "* see XGBoost documentation for more details\n",
    "\n",
    "### scorer: str, default='Auto'\n",
    "* determines the model evaluation metric used\n",
    "* if 'Auto', built-in XGBoost metrics will be used\n",
    "* otherwise, existic sklearn metrics can be passed as strings\n",
    "\n",
    "### test_extractors: list, default=[]\n",
    "* allows user to pass a list of feature extraction methods that will be tested\n",
    "* if left empty, a default list will be used\n",
    "\n",
    "### excluded_extractors: list, default=[]\n",
    "* allows user to pass a list of feature extraction methods that will be excluded from testing\n",
    "\n",
    "### set_extract_params: dictionary, default={}\n",
    "* allows user to pass a dictionary for setting parameters of feature extraction methods\n",
    "* if left empty, a default dictionary will be used\n",
    "* feature extraction methods not built into the class can be added this way\n",
    "\n",
    "### additional_extractors: list, default=[]\n",
    "* new feature extraction methods introduced through set_extract_params can be added to the list of methods to be tested\n",
    "\n",
    "### NLP: boolean, default=False\n",
    "* if True, adds methods used for Natural Language Processing (NLP) to list of methods to be tested\n",
    "\n",
    "### Sparse: boolean, default=False\n",
    "* if True, adds methods used for processing sparse data to list of methods to be tested\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Attributes:  </span>\n",
    "\n",
    "### scoring_functions: dictionary\n",
    "* dictonary of metrics available for use\n",
    "\n",
    "### classification_objectives: list\n",
    "* list of XGBoost objectives considered as for classification\n",
    "\n",
    "### regression_objectives: list\n",
    "* list of XGBoost objectives considered as for regression\n",
    "\n",
    "### the_higher_the_better: list\n",
    "* list of metrics of which an increase means improvement in performance\n",
    "\n",
    "### the_lower_the_better: list\n",
    "* list of metrics of which a decrease means improvement in performance\n",
    "\n",
    "### need_encoded_y: list\n",
    "* list of metrics that require an encoded y\n",
    "* automatically dealt with my the class\n",
    "\n",
    "### the_lower_the_better: list\n",
    "* list of metrics of which a decrease means improvement in performance\n",
    "\n",
    "### extract_params: dictionary\n",
    "* default dictionary of parameters used for feature extraction methods\n",
    "\n",
    "### extract: str\n",
    "* name or determined feature extraction method\n",
    "\n",
    "### extractor: obj\n",
    "* extractor object\n",
    "* fitted to X (or also y) after .fit()\n",
    "* should used later to transform test set or new data\n",
    "\n",
    "### performance_scores: pandas DataFrame\n",
    "* results of testing\n",
    "\n",
    "### best_perf: pandas DataFrame\n",
    "* highest performer/s\n",
    "\n",
    "### best_extract: str\n",
    "* name of feature extraction method deemed best\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Methods:  </span>\n",
    "\n",
    "### .fit(X, y=None, target=None)  \n",
    "Either fits the pre-determined feature extraction method to the data or performs tests first before fitting the best method to the data.  \n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "* target:*string* (optional)  \n",
    "    * name of target column  \n",
    "\n",
    "### .transform(X, y=None, target=None)  \n",
    "Using statistics learned from fitted data, returns extracted data.\n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "* target:*string* (optional)  \n",
    "    * name of target column  \n",
    "\n",
    "**Returns:**\n",
    "* X_extracted: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * extracted or untouched independent variables / features\n",
    "* y_extracted: *pandas DataFrame, shape (n_samples,)*    \n",
    "    * untouched dependent variable\n",
    "\n",
    "### .fit_transform(X, y=None, target=None)  \n",
    "Streamlines fit() and transform() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Class\n",
    "\n",
    "class Feature_Hyperextractor:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "        Options for feature extraction methods (parameter extract='') are the following:\n",
    "            FactorAnalysis, FastICA, IncrementalPCA, KernelPCA, LatentDirichletAllocation, \n",
    "            MiniBatchDictionaryLearning, MiniBatchSparsePCA, NMF, PCA, SparsePCA,\n",
    "            TruncatedSVD, LinearDiscriminantAnalysis\n",
    "\n",
    ".\n",
    "        \n",
    "        If extractor = 'Hyperextractor',\n",
    "            every extraction method will be applied and tested\n",
    "            to determine which method best contributes to model performance.\n",
    "            \n",
    "            best_params = {dictionary of best parameters for model tuning continuity}\n",
    "            \n",
    "            scorer = pass desired Sklearn.metrics scorer as a string\n",
    "                passing scorer='Auto' entails the use of built-in XGBoost evaluation metrics\n",
    "            \n",
    "            The extraction method that contributes to the best score will be applied to the data to be returned.\n",
    "            \n",
    "            Feature_Hyperextractor.extractor = chosen resampling class assigned after instantiation \n",
    "                                                                        or after fit() if extract = 'Feature_Hyperextractor'\n",
    "        \n",
    "        This feature extraction transformer works with any of the three boosters as base learners:\n",
    "        -gbtree\n",
    "        -gblinear\n",
    "        -dart\n",
    "        \n",
    "        This class requires that X (features or independent variables) has already been encoded and Imputed. \n",
    "        \n",
    "        This class also requires the target (y) to also be passed.\n",
    "        \n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit, transform, and fit_transform methods are given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "    \n",
    "    # Compatible model evaluation metrics: Sklearn metrics\n",
    "    scoring_functions = {'accuracy_score':accuracy_score,'f1_score':f1_score, 'hamming_loss':hamming_loss,\n",
    "        'jaccard_similarity_score':jaccard_similarity_score, 'log_loss':log_loss, 'matthews_corrcoef':matthews_corrcoef,\n",
    "                 'precision_score':precision_score, 'recall_score':recall_score, 'zero_one_loss':zero_one_loss,\n",
    "                'explained_variance_score':explained_variance_score, 'mean_absolute_error':mean_absolute_error,\n",
    "                 'mean_squared_error':mean_squared_error, 'mean_squared_log_error':mean_squared_log_error,\n",
    "                 'median_absolute_error':median_absolute_error, 'r2_score':r2_score}\n",
    "\n",
    "    classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                                'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "    regression_objectives = ['reg:linear','count:poisson','reg:gamma', 'reg:tweedie']\n",
    "    \n",
    "    # metrics with which higher value = higher model performance\n",
    "    the_higher_the_better = ['accuracy_score','f1_score','jaccard_similarity_score',\n",
    "                                  'precision_score','recall_score',\n",
    "                                'explained_variance_score','r2_score','Best test auc',\n",
    "                                'Best test ndcg','Best test map']\n",
    "    # metrics with which lower value = higher model performance\n",
    "    the_lower_the_better = ['hamming_loss', 'log_loss','matthews_corrcoef','zero_one_loss','mean_absolute_error',\n",
    "                           'mean_squared_error','mean_squared_log_error','median_absolute_error',\n",
    "                           'Best test error','Best test rmse','Best test mae','Best test log loss',\n",
    "                           'Best test merror','Best test mlogloss']\n",
    "    \n",
    "    # metrics that require encoded target variable\n",
    "    need_encoded_y = ['recall_score','precision_score','f1_score']\n",
    "    \n",
    "    \n",
    "    # parameters to inherited from mother class\n",
    "    extract='N/A'\n",
    "    best_params='N/A'\n",
    "    booster='N/A'\n",
    "    objective='N/A'\n",
    "    scorer='N/A'\n",
    "    test_extractors='N/A'\n",
    "    excluded_extractors='N/A'\n",
    "    additional_extractors='N/A'\n",
    "    set_extract_params='N/A'\n",
    "    NLP='N/A'\n",
    "    Sparse='N/A'\n",
    "    \n",
    "    # arguments for classes\n",
    "    Dirichlet_n_components='N/A'\n",
    "    TruncatedSVD_n_components='N/A'\n",
    "    n_components='N/A'\n",
    "    tol='N/A'\n",
    "    copy='N/A'\n",
    "    max_iter='N/A'\n",
    "    noise_variance_init='N/A'\n",
    "    svd_method='N/A'\n",
    "    iterated_power='N/A'\n",
    "    FastICA_algorithm='N/A'\n",
    "    TruncatedSVD_algorithm='N/A'\n",
    "    whiten='N/A'\n",
    "    fun='N/A'\n",
    "    fun_args='N/A'\n",
    "    w_init='N/A'\n",
    "    batch_size='N/A'\n",
    "    kernel='N/A'\n",
    "    gamma='N/A'\n",
    "    degree='N/A'\n",
    "    coef0='N/A'\n",
    "    kernel_params='N/A'\n",
    "    alpha='N/A'\n",
    "    fit_inverse_transform='N/A'\n",
    "    eigen_solver='N/A'\n",
    "    remove_zero_eig='N/A'\n",
    "    copy_X='N/A'\n",
    "    n_jobs='N/A'\n",
    "    doc_topic_prior='N/A'\n",
    "    topic_word_prior='N/A'\n",
    "    learning_method='N/A'\n",
    "    learning_decay='N/A'\n",
    "    learning_offset='N/A'\n",
    "    evaluate_every='N/A'\n",
    "    total_samples='N/A'\n",
    "    perp_tol='N/A'\n",
    "    mean_change_tol='N/A'\n",
    "    max_doc_update_iter='N/A'\n",
    "    verbose='N/A'\n",
    "    n_topics='N/A'\n",
    "    n_iter='N/A'\n",
    "    fit_algorithm='N/A'\n",
    "    shuffle='N/A'\n",
    "    dict_init='N/A'\n",
    "    transform_algorithm='N/A'\n",
    "    transform_n_nonzero_coefs='N/A'\n",
    "    transform_alpha='N/A'\n",
    "    split_sign='N/A'\n",
    "    ridge_alpha='N/A'\n",
    "    callback='N/A'\n",
    "    method='N/A'\n",
    "    init='N/A'\n",
    "    NMF_solver='N/A'\n",
    "    LDA_solver='N/A'\n",
    "    beta_loss='N/A'\n",
    "    l1_ratio='N/A'\n",
    "    svd_solver='N/A'\n",
    "    U_init='N/A'\n",
    "    V_init='N/A'\n",
    "    shrinkage='N/A'\n",
    "    priors='N/A'\n",
    "    store_covariance='N/A'\n",
    "\n",
    "#     **kwargs\n",
    "    \n",
    "    random_state='N/A'\n",
    "    test_size='N/A'\n",
    "    \n",
    "    def __init__(self, extract=\"Feature_Hyperextractor\", best_params={},\n",
    "                 booster='gbtree', objective='reg:logistic', scorer='Auto',\n",
    "                 test_extractors=[], excluded_extractors=[],additional_extractors=[],set_extract_params={},\n",
    "                 NLP=False,Sparse=False,\n",
    "                 \n",
    "                 # parameters for extractors\n",
    "                Dirichlet_n_components=10, TruncatedSVD_n_components=2,TruncatedSVD_algorithm='randomized',\n",
    "                 n_components=None,tol=0.01,copy=True,max_iter=1000,noise_variance_init=None,svd_method='randomized',\n",
    "                iterated_power=3,FastICA_algorithm='parallel',whiten=True,fun='logcosh',fun_args=None,w_init=None,\n",
    "                batch_size=None,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None,alpha=1,\n",
    "                fit_inverse_transform=False,eigen_solver='auto',remove_zero_eig=False,copy_X=True,n_jobs=1,\n",
    "                doc_topic_prior=None,topic_word_prior=None,learning_method=None,learning_decay=0.7,\n",
    "                learning_offset=10,evaluate_every=-1,total_samples=1000000,perp_tol=0.1,mean_change_tol=0.001,\n",
    "                max_doc_update_iter=100,verbose=0,n_topics=None,n_iter=1000,fit_algorithm='lars',shuffle=True,\n",
    "                dict_init=None,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,\n",
    "                split_sign=False,ridge_alpha=0.01,callback=None,method='lars',init=None,NMF_solver='cd',\n",
    "                beta_loss='frobenius',l1_ratio=0,svd_solver='auto',U_init=None,V_init=None,LDA_solver='svd',\n",
    "                shrinkage=None,priors=None,store_covariance=False,\n",
    "\n",
    "                # parameters for train/test split\n",
    "                 random_state=69, test_size=0.2,\n",
    "                # values passed for these parameters will never be used unless: class.parameter is made None\n",
    "                scoring_functions=None, classification_objectives=None, need_encoded_y=None,\n",
    "                 the_higher_the_better=None, the_lower_the_better=None):\n",
    "        \n",
    "        # assigning parameters as instance variables\n",
    "        varses = list(vars(Feature_Hyperextractor).keys())\n",
    "        self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "        class_name = \"Feature_Hyperextractor\"+\".\"\n",
    "        for v in self.variables:\n",
    "            # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "            if eval(\"%s != 'N/A'\" % (class_name+v)) is True:\n",
    "                exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "            # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "            else:\n",
    "                exec(\"self.%s = %s\" % (v, v))\n",
    "        \n",
    "        # determining the model build used for feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            if self.objective in Feature_Hyperextractor.classification_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBClassifier(**self.best_params)\n",
    "\n",
    "            elif self.objective in Feature_Hyperextractor.regression_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBRegressor(**self.best_params)\n",
    "        elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "            self.best_params['booster'] = self.booster\n",
    "            self.best_params['objective'] = self.objective\n",
    "            \n",
    "        \n",
    "        self.extract_params={'FactorAnalysis':[FactorAnalysis,{'n_components':self.n_components, 'tol':self.tol,\n",
    "                                        'copy':self.copy,'max_iter':self.max_iter,'random_state':self.random_state,\n",
    "                                        'noise_variance_init':self.noise_variance_init,\n",
    "                                        'svd_method':self.svd_method,'iterated_power':self.iterated_power}],\n",
    "                'FastICA':[FastICA,{'n_components':self.n_components,'algorithm':self.FastICA_algorithm,\n",
    "                                    'whiten':self.whiten,'fun':self.fun,'fun_args':self.fun_args,\n",
    "                                    'max_iter':self.max_iter,'tol':self.tol,'w_init':self.w_init,\n",
    "                                    'random_state':self.random_state}],\n",
    "                  'IncrementalPCA':[IncrementalPCA,{'n_components':self.n_components,'whiten':self.whiten,\n",
    "                                    'copy':self.copy,'batch_size':self.batch_size}],\n",
    "            'KernelPCA':[KernelPCA,{'n_components':self.n_components,'kernel':self.kernel,'gamma':self.gamma,\n",
    "                                    'degree':self.degree,'coef0':self.coef0,'kernel_params':self.kernel_params,\n",
    "                                    'alpha':self.alpha,'fit_inverse_transform':self.fit_inverse_transform,\n",
    "                                    'eigen_solver':self.eigen_solver,'tol':self.tol,'max_iter':self.max_iter,\n",
    "                                    'remove_zero_eig':self.remove_zero_eig,'random_state':self.random_state,\n",
    "                                    'copy_X':self.copy_X,'n_jobs':self.n_jobs}],\n",
    "            'LatentDirichletAllocation':[LatentDirichletAllocation,{'n_components':self.Dirichlet_n_components,\n",
    "                                'doc_topic_prior':self.doc_topic_prior,'topic_word_prior':self.topic_word_prior,\n",
    "                                'learning_method':self.learning_method,'learning_decay':self.learning_decay,\n",
    "                                'learning_offset':self.learning_offset,'max_iter':self.max_iter,\n",
    "                                'batch_size':self.batch_size,'evaluate_every':self.evaluate_every,\n",
    "                                'total_samples':self.total_samples,'perp_tol':self.perp_tol,\n",
    "                                'mean_change_tol':self.mean_change_tol,\n",
    "                                'max_doc_update_iter':self.max_doc_update_iter,\n",
    "                                'n_jobs':self.n_jobs,'verbose':self.verbose,\n",
    "                                'random_state':self.random_state,'n_topics':self.n_topics}],\n",
    "             'MiniBatchDictionaryLearning':[MiniBatchDictionaryLearning,{'n_components':self.n_components,\n",
    "                                'alpha':self.alpha,'n_iter':self.n_iter,'fit_algorithm':self.fit_algorithm,\n",
    "                                'n_jobs':self.n_jobs,'batch_size':self.batch_size,'shuffle':self.shuffle,\n",
    "                                'dict_init':self.dict_init,'transform_algorithm':self.transform_algorithm,\n",
    "                                'transform_n_nonzero_coefs':self.transform_n_nonzero_coefs,\n",
    "                                'transform_alpha':self.transform_alpha,'verbose':self.verbose,\n",
    "                                'split_sign':self.split_sign,'random_state':self.random_state}],\n",
    "            'MiniBatchSparsePCA':[MiniBatchSparsePCA,{'n_components':self.n_components,'alpha':self.alpha,\n",
    "                                'ridge_alpha':self.ridge_alpha,'n_iter':self.n_iter,'callback':self.callback,\n",
    "                                'batch_size':self.batch_size,'verbose':self.verbose,'shuffle':self.shuffle,\n",
    "                                'n_jobs':self.n_jobs,'method':self.method,'random_state':self.random_state}],\n",
    "             'NMF':[NMF,{'n_components':self.n_components,'init':self.init,'solver':self.NMF_solver,\n",
    "                            'beta_loss':self.beta_loss,'tol':self.tol,'max_iter':self.max_iter,\n",
    "                         'random_state':self.random_state,'alpha':self.alpha,'l1_ratio':self.l1_ratio,\n",
    "                         'verbose':self.verbose,'shuffle':self.shuffle}],\n",
    "             'PCA':[PCA,{'n_components':self.n_components,'copy':self.copy,'whiten':self.whiten,\n",
    "                        'svd_solver':self.svd_solver,'tol':self.tol,'iterated_power':self.iterated_power,\n",
    "                         'random_state':self.random_state}],\n",
    "             'SparsePCA':[SparsePCA,{'n_components':self.n_components,'alpha':self.alpha,\n",
    "                        'ridge_alpha':self.ridge_alpha,'max_iter':self.max_iter,'tol':self.tol,\n",
    "                        'method':self.method,'n_jobs':self.n_jobs,'U_init':self.U_init,'V_init':self.V_init,\n",
    "                        'verbose':self.verbose,'random_state':self.random_state}],\n",
    "            'TruncatedSVD':[TruncatedSVD,{'n_components':self.TruncatedSVD_n_components,\n",
    "                                          'algorithm':self.TruncatedSVD_algorithm,\n",
    "                            'n_iter':self.n_iter,'random_state':self.random_state,'tol':self.tol}],\n",
    "             'LinearDiscriminantAnalysis':[LinearDiscriminantAnalysis,{'solver':self.LDA_solver,\n",
    "                        'shrinkage':self.shrinkage,'priors':self.priors,'n_components':self.n_components,\n",
    "                        'store_covariance':self.store_covariance,'tol':self.tol}]}\n",
    "        \n",
    "        if len(self.set_extract_params) !=0:\n",
    "            for key,value in self.set_extract_params.items():\n",
    "                self.extract_params[key] = [value[0],value[1]]\n",
    "        \n",
    "        # Assigning the extractor if extract (sampling method) is not \"Hyperscale\"\n",
    "        if self.extract is not 'Feature_Hyperextractor':\n",
    "            self.extractor = self.extract_params[self.extract][0](**self.extract_params[self.extract][1])\n",
    "            self.performance_scores = None\n",
    "            self.best_perf = None\n",
    "            self.best_extract = None\n",
    "    \n",
    "    def fit(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if self.target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[self.target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        \n",
    "        if self.extract == 'Feature_Hyperextractor':\n",
    "            # Encode labels if string when needed\n",
    "            if ((self.booster == 'dart' or self.booster == 'gblinear')\\\n",
    "                            and self.objective in Feature_Hyperextractor.classification_objectives)\\\n",
    "                                                                    or self.scorer in self.need_encoded_y:\n",
    "                if y.iloc[:,0].dtype == object:\n",
    "                    Target_is_string = True\n",
    "                else:\n",
    "                    Target_is_string = False\n",
    "\n",
    "                if Target_is_string == True:\n",
    "                    L_E = LabelEncoder()\n",
    "                    L_E = L_E.fit(y.iloc[:,0].astype(str))\n",
    "                    y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))\n",
    "\n",
    "            # fit model on all training data            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "            \n",
    "            if len(self.test_extractors) != 0:\n",
    "                extractors = self.test_extractors\n",
    "            else:\n",
    "                extractors = ['None','FactorAnalysis', 'FastICA', 'IncrementalPCA', 'KernelPCA',\n",
    "                              'LatentDirichletAllocation','MiniBatchDictionaryLearning','MiniBatchSparsePCA',\n",
    "                              'NMF','PCA', 'SparsePCA','TruncatedSVD', 'LinearDiscriminantAnalysis']\n",
    "            \n",
    "            if len(self.additional_extractors) != 0:\n",
    "                extractors = extractors + self.additional_extractors\n",
    "                \n",
    "            if len(pd.DataFrame(X).columns) == 2:\n",
    "                self.extract_params['TruncatedSVD'][1]['n_components'] = 1\n",
    "            if self.NLP is not True:\n",
    "                extractors.remove('LatentDirichletAllocation')\n",
    "                extractors.remove('NMF')\n",
    "            if self.Sparse is not True:\n",
    "                extractors.remove('MiniBatchDictionaryLearning')\n",
    "                extractors.remove('MiniBatchSparsePCA')\n",
    "                extractors.remove('SparsePCA')\n",
    "            \n",
    "            if len(self.excluded_extractors) !=0:\n",
    "                for excluded in self.excluded_extractors:\n",
    "                    if excluded in extractors:\n",
    "                        extractors.remove(excluded)\n",
    "            \n",
    "            \n",
    "            Scores = []\n",
    "            for extract in extractors:\n",
    "                if extract is 'None':\n",
    "                    X_train_extracted, y_train_extracted = X_train, y_train.values.ravel()\n",
    "                    X_test_extracted, y_test_extracted = X_test, y_test.values.ravel()\n",
    "                elif extract is 'LinearDiscriminantAnalysis':\n",
    "                    extract = self.extract_params[extract][0](**self.extract_params[extract][1])\n",
    "                    extract.fit(X_train, y_train.values.ravel())\n",
    "                    X_train_extracted, y_train_extracted = extract.transform(X_train), y_train.values.ravel()\n",
    "                    X_test_extracted, y_test_extracted = extract.transform(X_test), y_test.values.ravel()                    \n",
    "                else:\n",
    "                    extract = self.extract_params[extract][0](**self.extract_params[extract][1])\n",
    "                    extract.fit(X_train)\n",
    "                    X_train_extracted, y_train_extracted = extract.transform(X_train), y_train.values.ravel()\n",
    "                    X_test_extracted, y_test_extracted = extract.transform(X_test), y_test.values.ravel()\n",
    "                \n",
    "                if self.booster is 'gbtree':\n",
    "                    if self.objective in Feature_Hyperextractor.classification_objectives:\n",
    "                        estimator = xgb.XGBClassifier\n",
    "                    if self.objective in Feature_Hyperextractor.regression_objectives:\n",
    "                        estimator = xgb.XGBRegressor\n",
    "                    selection_model = estimator(**self.best_params)\n",
    "                    selection_model.fit(X_train_extracted, y_train_extracted,\n",
    "                                       eval_set=[(X_test_extracted,y_test_extracted)], verbose=False)\n",
    "                    # eval model\n",
    "                    # using built-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = selection_model.evals_result()\n",
    "                        scorer_used = list(result['validation_0'].keys())[0]\n",
    "                        #score = np.mean(result['validation_1'][scorer_used])\n",
    "                        score = result['validation_0'][scorer_used][-1]\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = Feature_Hyperextractor.scoring_functions[self.scorer]\n",
    "                        y_pred = selection_model.predict(X_test_extracted)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test_extracted]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used = y_test_extracted\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "                        \n",
    "                elif self.booster == 'dart' or self.booster == 'gblinear':\n",
    "                    dtrain_extracted = xgb.DMatrix(data=X_train_extracted, label=y_train_extracted)\n",
    "                    dtest_extracted = xgb.DMatrix(data=X_test_extracted, label=y_test_extracted)\n",
    "\n",
    "                    # eval model\n",
    "                    # using built-in evaluation metrics automatically matched with objective\n",
    "                    if self.scorer == 'Auto':\n",
    "                        result = {}\n",
    "                        selection_model = xgb.train(dtrain=dtrain_extracted, params=self.best_params,\n",
    "                                                    evals=[(dtest_extracted, 'eval')], evals_result=result,\n",
    "                                                   verbose_eval=False)\n",
    "                        scorer_used = list(result['eval'].keys())[0]\n",
    "                        #score = np.mean(result['eval'][scorer_used])\n",
    "                        score = result['eval'][scorer_used][-1]\n",
    "                        Scores.append(score)\n",
    "\n",
    "                    # using Sklearn metrics\n",
    "                    else:\n",
    "                        scoring_function = Feature_Hyperextractor.scoring_functions[self.scorer]\n",
    "\n",
    "                        selection_model = xgb.train(dtrain=dtrain_extracted, params=self.best_params)\n",
    "                        y_pred = selection_model.predict(dtest_extracted)\n",
    "                        if y_pred.dtype != object:\n",
    "                            predictions = [round(value) for value in y_pred]\n",
    "                            y_test_used = [round(value) for value in y_test_extracted]\n",
    "                        else:\n",
    "                            predictions = y_pred\n",
    "                            y_test_used =  y_test_extracted\n",
    "                        score = scoring_function(y_test_used, predictions)\n",
    "                        if self.scorer == 'accuracy_score':\n",
    "                            score = score*100.00\n",
    "                        Scores.append(score)\n",
    "\n",
    "            # building table of performance scores\n",
    "            if self.scorer == 'Auto':\n",
    "                #self.scorer = 'Average test ' + scorer_used\n",
    "                self.scorer = 'Best test ' + scorer_used\n",
    "            self.performance_scores = pd.DataFrame()\n",
    "            self.performance_scores['Extractors'] = extractors\n",
    "            self.performance_scores[self.scorer] = Scores\n",
    "\n",
    "            # Best extract: returns max possible model performance \n",
    "            if self.scorer in Feature_Hyperextractor.the_higher_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == max(self.performance_scores[self.scorer])]\n",
    "            elif self.scorer in Feature_Hyperextractor.the_lower_the_better:\n",
    "                self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                         == min(self.performance_scores[self.scorer])]\n",
    "                \n",
    "            # assigning the best-performing extractor as self.extractor\n",
    "            self.best_extract = self.best_perf.iloc[0,0]\n",
    "            if self.best_extract is not 'None':\n",
    "                self.extractor = self.extract_params[self.best_extract][0](**self.extract_params[self.best_extract][1])\n",
    "                # fitting the extractor\n",
    "                if self.best_extract is 'LinearDiscriminantAnalysis':\n",
    "                    self.extractor.fit(X, y.values.ravel())\n",
    "                else:\n",
    "                    self.extractor.fit(X)\n",
    "            else:\n",
    "                self.extractor = None\n",
    "            \n",
    "            # reverse label encoding\n",
    "            if ((self.booster == 'dart' or self.booster == 'gblinear')\\\n",
    "                            and self.objective in Feature_Hyperextractor.classification_objectives)\\\n",
    "                                                                    or self.scorer in self.need_encoded_y:\n",
    "                if Target_is_string == True:\n",
    "                    y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])\n",
    "            \n",
    "            return self\n",
    "            \n",
    "        else:\n",
    "            if self.extract is 'LinearDiscriminantAnalysis':\n",
    "                self.extractor.fit(X, y.values.ravel())\n",
    "            else:\n",
    "                self.extractor.fit(X)\n",
    "            return self\n",
    "    \n",
    "    def transform(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = pd.DataFrame(X.drop(target, axis =1))\n",
    "        if self.best_extract is not 'None':\n",
    "            X_extracted = pd.DataFrame(self.extractor.transform(X))\n",
    "            X_extracted.columns = ['F'+str(col) for col in X_extracted.columns]\n",
    "            return X_extracted, pd.DataFrame(y)\n",
    "        else:\n",
    "            return pd.DataFrame(X), pd.DataFrame(y)\n",
    "        \n",
    "    def fit_transform(self, X, y=None, target=None):\n",
    "        self.fit(X,y, target=target)\n",
    "        return self.transform(X, y, target=target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Feature_Hyperextractor in action  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mFeatures Engineered from X_train:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F0</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "      <th>F29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266251</td>\n",
       "      <td>-0.118402</td>\n",
       "      <td>-0.547101</td>\n",
       "      <td>-0.007242</td>\n",
       "      <td>0.208805</td>\n",
       "      <td>0.033638</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.086557</td>\n",
       "      <td>0.709156</td>\n",
       "      <td>-0.002445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242455</td>\n",
       "      <td>0.370284</td>\n",
       "      <td>-0.141288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.170784</td>\n",
       "      <td>-0.092613</td>\n",
       "      <td>-0.797599</td>\n",
       "      <td>0.117353</td>\n",
       "      <td>0.172764</td>\n",
       "      <td>0.246944</td>\n",
       "      <td>0.162560</td>\n",
       "      <td>-0.137314</td>\n",
       "      <td>-0.024216</td>\n",
       "      <td>0.037240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034937</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.006718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.582985</td>\n",
       "      <td>-0.063014</td>\n",
       "      <td>-0.298098</td>\n",
       "      <td>-0.058132</td>\n",
       "      <td>0.179771</td>\n",
       "      <td>0.096694</td>\n",
       "      <td>0.272291</td>\n",
       "      <td>-0.044316</td>\n",
       "      <td>0.058963</td>\n",
       "      <td>-0.089999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028158</td>\n",
       "      <td>0.036751</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.667678</td>\n",
       "      <td>-0.113884</td>\n",
       "      <td>-0.722696</td>\n",
       "      <td>0.074993</td>\n",
       "      <td>0.163021</td>\n",
       "      <td>0.253945</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>-0.163521</td>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.073749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043993</td>\n",
       "      <td>-0.034997</td>\n",
       "      <td>-0.002822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.849560</td>\n",
       "      <td>-0.063234</td>\n",
       "      <td>-0.435702</td>\n",
       "      <td>-0.031894</td>\n",
       "      <td>0.166287</td>\n",
       "      <td>0.081211</td>\n",
       "      <td>0.265267</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.061249</td>\n",
       "      <td>-0.013678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025152</td>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.502955</td>\n",
       "      <td>-0.107799</td>\n",
       "      <td>-0.866191</td>\n",
       "      <td>0.114362</td>\n",
       "      <td>0.169591</td>\n",
       "      <td>0.249154</td>\n",
       "      <td>0.113528</td>\n",
       "      <td>-0.145230</td>\n",
       "      <td>-0.020457</td>\n",
       "      <td>0.161684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021707</td>\n",
       "      <td>-0.058405</td>\n",
       "      <td>-0.002340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.133038</td>\n",
       "      <td>-0.117657</td>\n",
       "      <td>-0.567499</td>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.176908</td>\n",
       "      <td>0.163342</td>\n",
       "      <td>0.187957</td>\n",
       "      <td>-0.089073</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>0.014318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.025551</td>\n",
       "      <td>-0.009429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.318257</td>\n",
       "      <td>-0.071643</td>\n",
       "      <td>-0.345541</td>\n",
       "      <td>-0.052795</td>\n",
       "      <td>0.179769</td>\n",
       "      <td>0.083850</td>\n",
       "      <td>0.293792</td>\n",
       "      <td>-0.042039</td>\n",
       "      <td>0.080866</td>\n",
       "      <td>-0.025244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027648</td>\n",
       "      <td>0.034718</td>\n",
       "      <td>-0.009345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.283348</td>\n",
       "      <td>-0.103015</td>\n",
       "      <td>-0.641795</td>\n",
       "      <td>0.073479</td>\n",
       "      <td>0.182611</td>\n",
       "      <td>0.246341</td>\n",
       "      <td>0.157299</td>\n",
       "      <td>-0.171478</td>\n",
       "      <td>-0.005351</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087518</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.013193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.895305</td>\n",
       "      <td>-0.068355</td>\n",
       "      <td>-0.230015</td>\n",
       "      <td>-0.083006</td>\n",
       "      <td>0.186769</td>\n",
       "      <td>0.061792</td>\n",
       "      <td>0.261119</td>\n",
       "      <td>-0.055599</td>\n",
       "      <td>0.060888</td>\n",
       "      <td>-0.086904</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020194</td>\n",
       "      <td>0.054259</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.514299</td>\n",
       "      <td>-0.072816</td>\n",
       "      <td>-0.498048</td>\n",
       "      <td>-0.011688</td>\n",
       "      <td>0.164375</td>\n",
       "      <td>0.082324</td>\n",
       "      <td>0.277102</td>\n",
       "      <td>0.022101</td>\n",
       "      <td>0.037191</td>\n",
       "      <td>0.030170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061364</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.873551</td>\n",
       "      <td>-0.097399</td>\n",
       "      <td>-0.759809</td>\n",
       "      <td>0.108238</td>\n",
       "      <td>0.160049</td>\n",
       "      <td>0.263482</td>\n",
       "      <td>0.140431</td>\n",
       "      <td>-0.141002</td>\n",
       "      <td>-0.018383</td>\n",
       "      <td>0.058167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076400</td>\n",
       "      <td>-0.031953</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.075483</td>\n",
       "      <td>-0.115029</td>\n",
       "      <td>-0.611411</td>\n",
       "      <td>0.055081</td>\n",
       "      <td>0.165639</td>\n",
       "      <td>0.235318</td>\n",
       "      <td>0.158196</td>\n",
       "      <td>-0.187898</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.022736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067217</td>\n",
       "      <td>-0.068659</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.075357</td>\n",
       "      <td>-0.063546</td>\n",
       "      <td>-0.385750</td>\n",
       "      <td>-0.023755</td>\n",
       "      <td>0.183372</td>\n",
       "      <td>0.097742</td>\n",
       "      <td>0.263307</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>0.064037</td>\n",
       "      <td>-0.049009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022090</td>\n",
       "      <td>0.018572</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.477601</td>\n",
       "      <td>-0.115623</td>\n",
       "      <td>-0.682548</td>\n",
       "      <td>0.073435</td>\n",
       "      <td>0.172622</td>\n",
       "      <td>0.236267</td>\n",
       "      <td>0.157051</td>\n",
       "      <td>-0.185262</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>0.071048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042749</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>-0.002777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.654899</td>\n",
       "      <td>-0.146220</td>\n",
       "      <td>-0.894734</td>\n",
       "      <td>0.166227</td>\n",
       "      <td>0.230432</td>\n",
       "      <td>0.174525</td>\n",
       "      <td>-0.116998</td>\n",
       "      <td>-0.018955</td>\n",
       "      <td>0.629269</td>\n",
       "      <td>-0.087207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255559</td>\n",
       "      <td>0.261679</td>\n",
       "      <td>-0.122350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.336595</td>\n",
       "      <td>-0.133057</td>\n",
       "      <td>-0.846287</td>\n",
       "      <td>0.132478</td>\n",
       "      <td>0.160553</td>\n",
       "      <td>0.270009</td>\n",
       "      <td>0.135434</td>\n",
       "      <td>-0.134084</td>\n",
       "      <td>-0.020106</td>\n",
       "      <td>0.086339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055631</td>\n",
       "      <td>-0.039642</td>\n",
       "      <td>0.014067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.683373</td>\n",
       "      <td>-0.057007</td>\n",
       "      <td>-0.466296</td>\n",
       "      <td>-0.012828</td>\n",
       "      <td>0.156769</td>\n",
       "      <td>0.102835</td>\n",
       "      <td>0.290548</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.052088</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015408</td>\n",
       "      <td>0.021866</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.476871</td>\n",
       "      <td>-0.064685</td>\n",
       "      <td>-0.315155</td>\n",
       "      <td>-0.066798</td>\n",
       "      <td>0.175437</td>\n",
       "      <td>0.067806</td>\n",
       "      <td>0.280387</td>\n",
       "      <td>0.007585</td>\n",
       "      <td>0.074943</td>\n",
       "      <td>-0.035172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054066</td>\n",
       "      <td>0.054247</td>\n",
       "      <td>-0.003528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.749273</td>\n",
       "      <td>-0.057578</td>\n",
       "      <td>-0.269722</td>\n",
       "      <td>-0.083657</td>\n",
       "      <td>0.163028</td>\n",
       "      <td>0.080777</td>\n",
       "      <td>0.257926</td>\n",
       "      <td>-0.052367</td>\n",
       "      <td>0.057484</td>\n",
       "      <td>-0.079618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>0.042431</td>\n",
       "      <td>-0.013424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.397125</td>\n",
       "      <td>-0.097912</td>\n",
       "      <td>-0.524106</td>\n",
       "      <td>-0.012754</td>\n",
       "      <td>0.154751</td>\n",
       "      <td>0.096114</td>\n",
       "      <td>0.263920</td>\n",
       "      <td>-0.019422</td>\n",
       "      <td>0.048040</td>\n",
       "      <td>0.084348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>-0.003889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.013776</td>\n",
       "      <td>-0.081470</td>\n",
       "      <td>-0.787294</td>\n",
       "      <td>0.100861</td>\n",
       "      <td>0.160227</td>\n",
       "      <td>0.241446</td>\n",
       "      <td>0.147231</td>\n",
       "      <td>-0.126325</td>\n",
       "      <td>-0.014480</td>\n",
       "      <td>0.095005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017579</td>\n",
       "      <td>-0.022721</td>\n",
       "      <td>-0.002477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.030701</td>\n",
       "      <td>-0.110971</td>\n",
       "      <td>-0.583655</td>\n",
       "      <td>0.047569</td>\n",
       "      <td>0.179863</td>\n",
       "      <td>0.235871</td>\n",
       "      <td>0.152594</td>\n",
       "      <td>-0.200561</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>-0.020010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053130</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>-0.001963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.254558</td>\n",
       "      <td>-0.071353</td>\n",
       "      <td>-0.360542</td>\n",
       "      <td>-0.038882</td>\n",
       "      <td>0.165067</td>\n",
       "      <td>0.093027</td>\n",
       "      <td>0.272733</td>\n",
       "      <td>-0.011493</td>\n",
       "      <td>0.067027</td>\n",
       "      <td>-0.027146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040285</td>\n",
       "      <td>0.022669</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.933700</td>\n",
       "      <td>-0.068137</td>\n",
       "      <td>-0.418969</td>\n",
       "      <td>-0.045687</td>\n",
       "      <td>0.173460</td>\n",
       "      <td>0.084107</td>\n",
       "      <td>0.290338</td>\n",
       "      <td>0.007140</td>\n",
       "      <td>0.055540</td>\n",
       "      <td>-0.044768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026204</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>-0.003757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.767947</td>\n",
       "      <td>-0.102713</td>\n",
       "      <td>-0.737680</td>\n",
       "      <td>0.097074</td>\n",
       "      <td>0.168506</td>\n",
       "      <td>0.287392</td>\n",
       "      <td>0.132456</td>\n",
       "      <td>-0.162675</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.052658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051350</td>\n",
       "      <td>-0.009788</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.387837</td>\n",
       "      <td>-0.116199</td>\n",
       "      <td>-0.665917</td>\n",
       "      <td>0.073970</td>\n",
       "      <td>0.170476</td>\n",
       "      <td>0.239490</td>\n",
       "      <td>0.139755</td>\n",
       "      <td>-0.190270</td>\n",
       "      <td>0.012911</td>\n",
       "      <td>0.041860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095873</td>\n",
       "      <td>-0.010359</td>\n",
       "      <td>-0.010611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.190268</td>\n",
       "      <td>-0.067688</td>\n",
       "      <td>-0.363618</td>\n",
       "      <td>-0.051090</td>\n",
       "      <td>0.195520</td>\n",
       "      <td>0.082093</td>\n",
       "      <td>0.283177</td>\n",
       "      <td>-0.025215</td>\n",
       "      <td>0.067247</td>\n",
       "      <td>-0.042811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007806</td>\n",
       "      <td>0.037989</td>\n",
       "      <td>-0.003941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.163095</td>\n",
       "      <td>-0.106356</td>\n",
       "      <td>-0.622636</td>\n",
       "      <td>0.052519</td>\n",
       "      <td>0.168164</td>\n",
       "      <td>0.254853</td>\n",
       "      <td>0.141528</td>\n",
       "      <td>-0.201869</td>\n",
       "      <td>-0.003489</td>\n",
       "      <td>-0.017821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051644</td>\n",
       "      <td>-0.004569</td>\n",
       "      <td>-0.004444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.253315</td>\n",
       "      <td>-0.103127</td>\n",
       "      <td>-0.830322</td>\n",
       "      <td>0.107261</td>\n",
       "      <td>0.166819</td>\n",
       "      <td>0.279360</td>\n",
       "      <td>0.145312</td>\n",
       "      <td>-0.166145</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.123472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027335</td>\n",
       "      <td>-0.018509</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.133042</td>\n",
       "      <td>-0.089310</td>\n",
       "      <td>1.067425</td>\n",
       "      <td>-0.789059</td>\n",
       "      <td>-2.160616</td>\n",
       "      <td>-0.014968</td>\n",
       "      <td>-2.293228</td>\n",
       "      <td>0.109642</td>\n",
       "      <td>0.870958</td>\n",
       "      <td>0.167895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607804</td>\n",
       "      <td>-0.263787</td>\n",
       "      <td>-0.200525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>-1.656375</td>\n",
       "      <td>-0.217188</td>\n",
       "      <td>-0.853413</td>\n",
       "      <td>-0.176066</td>\n",
       "      <td>0.804638</td>\n",
       "      <td>-2.502862</td>\n",
       "      <td>-1.974881</td>\n",
       "      <td>1.368791</td>\n",
       "      <td>-0.592442</td>\n",
       "      <td>0.859528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172206</td>\n",
       "      <td>0.186720</td>\n",
       "      <td>0.038297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>-1.623418</td>\n",
       "      <td>-0.217465</td>\n",
       "      <td>0.069607</td>\n",
       "      <td>-0.311179</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>-1.092627</td>\n",
       "      <td>-1.355949</td>\n",
       "      <td>1.383900</td>\n",
       "      <td>2.458397</td>\n",
       "      <td>-1.046876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036605</td>\n",
       "      <td>-0.390334</td>\n",
       "      <td>0.038601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>-1.596546</td>\n",
       "      <td>-0.025615</td>\n",
       "      <td>2.556524</td>\n",
       "      <td>0.502733</td>\n",
       "      <td>-0.080013</td>\n",
       "      <td>0.880506</td>\n",
       "      <td>2.698349</td>\n",
       "      <td>-0.878477</td>\n",
       "      <td>-3.003623</td>\n",
       "      <td>-0.187019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726811</td>\n",
       "      <td>0.155596</td>\n",
       "      <td>-0.108954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.133041</td>\n",
       "      <td>0.175215</td>\n",
       "      <td>-0.152392</td>\n",
       "      <td>0.009210</td>\n",
       "      <td>0.210931</td>\n",
       "      <td>-0.017501</td>\n",
       "      <td>-1.035457</td>\n",
       "      <td>-0.254390</td>\n",
       "      <td>-0.753335</td>\n",
       "      <td>0.701747</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414808</td>\n",
       "      <td>-0.062982</td>\n",
       "      <td>0.187933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.133042</td>\n",
       "      <td>-0.165930</td>\n",
       "      <td>-0.241991</td>\n",
       "      <td>0.706867</td>\n",
       "      <td>1.056090</td>\n",
       "      <td>-0.069680</td>\n",
       "      <td>-0.117728</td>\n",
       "      <td>-1.324710</td>\n",
       "      <td>-0.637359</td>\n",
       "      <td>-1.161618</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.493787</td>\n",
       "      <td>0.864701</td>\n",
       "      <td>-0.217579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.133042</td>\n",
       "      <td>-0.089308</td>\n",
       "      <td>-0.434903</td>\n",
       "      <td>-0.094711</td>\n",
       "      <td>0.069790</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>-0.528807</td>\n",
       "      <td>0.353971</td>\n",
       "      <td>0.968753</td>\n",
       "      <td>0.391811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281382</td>\n",
       "      <td>0.184815</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>-1.650539</td>\n",
       "      <td>-0.217058</td>\n",
       "      <td>0.664486</td>\n",
       "      <td>-0.503465</td>\n",
       "      <td>-0.798782</td>\n",
       "      <td>-0.749863</td>\n",
       "      <td>-0.878547</td>\n",
       "      <td>0.188310</td>\n",
       "      <td>0.727441</td>\n",
       "      <td>0.665524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892049</td>\n",
       "      <td>0.522632</td>\n",
       "      <td>-0.103583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1.273218</td>\n",
       "      <td>-0.063951</td>\n",
       "      <td>0.265955</td>\n",
       "      <td>-0.357085</td>\n",
       "      <td>0.073657</td>\n",
       "      <td>-1.246993</td>\n",
       "      <td>-0.626836</td>\n",
       "      <td>0.454708</td>\n",
       "      <td>-1.789744</td>\n",
       "      <td>0.653012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>0.048091</td>\n",
       "      <td>-0.075814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.133039</td>\n",
       "      <td>0.758946</td>\n",
       "      <td>-0.131285</td>\n",
       "      <td>0.266408</td>\n",
       "      <td>-0.301009</td>\n",
       "      <td>0.511986</td>\n",
       "      <td>-0.325256</td>\n",
       "      <td>-0.183448</td>\n",
       "      <td>0.682492</td>\n",
       "      <td>-0.128210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018010</td>\n",
       "      <td>0.940527</td>\n",
       "      <td>-0.007533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.283350</td>\n",
       "      <td>-0.174020</td>\n",
       "      <td>5.047391</td>\n",
       "      <td>1.796492</td>\n",
       "      <td>4.968090</td>\n",
       "      <td>3.787573</td>\n",
       "      <td>0.532960</td>\n",
       "      <td>0.076475</td>\n",
       "      <td>1.816624</td>\n",
       "      <td>1.550758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333568</td>\n",
       "      <td>-0.218529</td>\n",
       "      <td>0.232078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>1.860270</td>\n",
       "      <td>-0.099581</td>\n",
       "      <td>0.278167</td>\n",
       "      <td>-0.377719</td>\n",
       "      <td>-0.189201</td>\n",
       "      <td>-0.555210</td>\n",
       "      <td>-0.016222</td>\n",
       "      <td>0.167106</td>\n",
       "      <td>-0.245095</td>\n",
       "      <td>-0.543269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024851</td>\n",
       "      <td>-0.230061</td>\n",
       "      <td>0.027755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.133042</td>\n",
       "      <td>-0.089308</td>\n",
       "      <td>0.292005</td>\n",
       "      <td>-0.574496</td>\n",
       "      <td>-0.664346</td>\n",
       "      <td>-0.874841</td>\n",
       "      <td>-2.467029</td>\n",
       "      <td>-0.513609</td>\n",
       "      <td>-1.179852</td>\n",
       "      <td>0.589447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.470555</td>\n",
       "      <td>-0.054498</td>\n",
       "      <td>-0.085535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>-1.621713</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>-0.122838</td>\n",
       "      <td>0.749354</td>\n",
       "      <td>-0.613611</td>\n",
       "      <td>0.300368</td>\n",
       "      <td>0.080455</td>\n",
       "      <td>0.091149</td>\n",
       "      <td>1.720344</td>\n",
       "      <td>-1.284149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064893</td>\n",
       "      <td>-0.300260</td>\n",
       "      <td>-0.073631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1.350883</td>\n",
       "      <td>-0.150563</td>\n",
       "      <td>0.275447</td>\n",
       "      <td>-0.610988</td>\n",
       "      <td>-1.187957</td>\n",
       "      <td>0.322767</td>\n",
       "      <td>-0.132914</td>\n",
       "      <td>-0.735561</td>\n",
       "      <td>-1.135795</td>\n",
       "      <td>-0.447462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.412983</td>\n",
       "      <td>0.405982</td>\n",
       "      <td>0.102469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>-1.180575</td>\n",
       "      <td>-0.123152</td>\n",
       "      <td>2.907867</td>\n",
       "      <td>-1.115742</td>\n",
       "      <td>-2.693739</td>\n",
       "      <td>0.648096</td>\n",
       "      <td>-0.331383</td>\n",
       "      <td>-0.528247</td>\n",
       "      <td>1.214082</td>\n",
       "      <td>0.255205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905014</td>\n",
       "      <td>0.657256</td>\n",
       "      <td>0.038321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.133042</td>\n",
       "      <td>0.032885</td>\n",
       "      <td>-0.526973</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>0.147283</td>\n",
       "      <td>0.118151</td>\n",
       "      <td>-0.019811</td>\n",
       "      <td>-0.049573</td>\n",
       "      <td>0.227504</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148554</td>\n",
       "      <td>0.137716</td>\n",
       "      <td>-0.029818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.236701</td>\n",
       "      <td>-0.145396</td>\n",
       "      <td>2.238088</td>\n",
       "      <td>0.853976</td>\n",
       "      <td>-1.014450</td>\n",
       "      <td>2.206291</td>\n",
       "      <td>1.014719</td>\n",
       "      <td>-4.675390</td>\n",
       "      <td>-2.119757</td>\n",
       "      <td>-1.689108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356346</td>\n",
       "      <td>-1.057496</td>\n",
       "      <td>-0.161412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>-1.583575</td>\n",
       "      <td>-0.215535</td>\n",
       "      <td>3.598550</td>\n",
       "      <td>0.134556</td>\n",
       "      <td>1.208280</td>\n",
       "      <td>-1.422176</td>\n",
       "      <td>-0.989019</td>\n",
       "      <td>1.451999</td>\n",
       "      <td>-1.053684</td>\n",
       "      <td>-0.454761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.805099</td>\n",
       "      <td>0.651075</td>\n",
       "      <td>-0.026061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.221553</td>\n",
       "      <td>-0.123056</td>\n",
       "      <td>0.213674</td>\n",
       "      <td>0.207099</td>\n",
       "      <td>0.100949</td>\n",
       "      <td>-0.911325</td>\n",
       "      <td>-0.411949</td>\n",
       "      <td>0.945019</td>\n",
       "      <td>-1.997326</td>\n",
       "      <td>-0.646015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602880</td>\n",
       "      <td>0.328858</td>\n",
       "      <td>-0.203946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1.596258</td>\n",
       "      <td>-0.056764</td>\n",
       "      <td>-0.297435</td>\n",
       "      <td>-0.128968</td>\n",
       "      <td>0.146292</td>\n",
       "      <td>-0.085855</td>\n",
       "      <td>-0.136808</td>\n",
       "      <td>0.316778</td>\n",
       "      <td>-0.401985</td>\n",
       "      <td>-0.118658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432595</td>\n",
       "      <td>0.208880</td>\n",
       "      <td>-0.150806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.222274</td>\n",
       "      <td>-0.087316</td>\n",
       "      <td>-0.551679</td>\n",
       "      <td>0.119204</td>\n",
       "      <td>1.091613</td>\n",
       "      <td>-1.031132</td>\n",
       "      <td>-1.243840</td>\n",
       "      <td>1.300044</td>\n",
       "      <td>0.746313</td>\n",
       "      <td>-0.198830</td>\n",
       "      <td>...</td>\n",
       "      <td>1.172896</td>\n",
       "      <td>-0.584876</td>\n",
       "      <td>0.062018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.133042</td>\n",
       "      <td>-0.176036</td>\n",
       "      <td>3.534098</td>\n",
       "      <td>2.904141</td>\n",
       "      <td>7.325182</td>\n",
       "      <td>4.621767</td>\n",
       "      <td>-4.455538</td>\n",
       "      <td>-5.128106</td>\n",
       "      <td>-1.575461</td>\n",
       "      <td>3.183713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.591470</td>\n",
       "      <td>1.323733</td>\n",
       "      <td>0.386247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.133042</td>\n",
       "      <td>-0.089296</td>\n",
       "      <td>-0.315396</td>\n",
       "      <td>0.102468</td>\n",
       "      <td>-0.159061</td>\n",
       "      <td>-0.342280</td>\n",
       "      <td>-0.759242</td>\n",
       "      <td>-0.008324</td>\n",
       "      <td>-1.646095</td>\n",
       "      <td>-1.496326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579557</td>\n",
       "      <td>-1.251087</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.133042</td>\n",
       "      <td>-0.177343</td>\n",
       "      <td>4.127070</td>\n",
       "      <td>3.944479</td>\n",
       "      <td>5.557127</td>\n",
       "      <td>0.264480</td>\n",
       "      <td>-2.368385</td>\n",
       "      <td>1.658897</td>\n",
       "      <td>0.789951</td>\n",
       "      <td>0.457322</td>\n",
       "      <td>...</td>\n",
       "      <td>1.185804</td>\n",
       "      <td>-0.650613</td>\n",
       "      <td>0.087155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.750809</td>\n",
       "      <td>-0.163656</td>\n",
       "      <td>0.196711</td>\n",
       "      <td>1.930867</td>\n",
       "      <td>-0.938710</td>\n",
       "      <td>-1.082400</td>\n",
       "      <td>1.238830</td>\n",
       "      <td>0.546870</td>\n",
       "      <td>0.746122</td>\n",
       "      <td>-1.064266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279536</td>\n",
       "      <td>-0.088865</td>\n",
       "      <td>-0.092801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-1.176929</td>\n",
       "      <td>-0.169713</td>\n",
       "      <td>2.788176</td>\n",
       "      <td>-0.376023</td>\n",
       "      <td>0.468134</td>\n",
       "      <td>2.742898</td>\n",
       "      <td>3.194430</td>\n",
       "      <td>-0.733956</td>\n",
       "      <td>0.102665</td>\n",
       "      <td>-0.463179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.874690</td>\n",
       "      <td>-0.696640</td>\n",
       "      <td>-0.231348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-1.178477</td>\n",
       "      <td>-0.087725</td>\n",
       "      <td>3.052644</td>\n",
       "      <td>-1.166082</td>\n",
       "      <td>-1.542982</td>\n",
       "      <td>0.692733</td>\n",
       "      <td>-0.100497</td>\n",
       "      <td>0.173832</td>\n",
       "      <td>0.197855</td>\n",
       "      <td>-0.847791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721038</td>\n",
       "      <td>-0.792625</td>\n",
       "      <td>-0.053725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.243507</td>\n",
       "      <td>-0.086838</td>\n",
       "      <td>-0.371057</td>\n",
       "      <td>-0.078158</td>\n",
       "      <td>0.385142</td>\n",
       "      <td>-0.233247</td>\n",
       "      <td>0.121167</td>\n",
       "      <td>0.230343</td>\n",
       "      <td>-0.480261</td>\n",
       "      <td>0.147080</td>\n",
       "      <td>...</td>\n",
       "      <td>1.493117</td>\n",
       "      <td>0.263444</td>\n",
       "      <td>-0.215467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-1.623378</td>\n",
       "      <td>-0.186678</td>\n",
       "      <td>1.094633</td>\n",
       "      <td>-0.336821</td>\n",
       "      <td>-1.288285</td>\n",
       "      <td>-0.309214</td>\n",
       "      <td>-0.827695</td>\n",
       "      <td>0.232114</td>\n",
       "      <td>0.887893</td>\n",
       "      <td>-0.902763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113802</td>\n",
       "      <td>0.161544</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           F0        F1        F2        F3        F4        F5        F6  \\\n",
       "0    0.266251 -0.118402 -0.547101 -0.007242  0.208805  0.033638  0.027600   \n",
       "1   -1.170784 -0.092613 -0.797599  0.117353  0.172764  0.246944  0.162560   \n",
       "2    1.582985 -0.063014 -0.298098 -0.058132  0.179771  0.096694  0.272291   \n",
       "3   -0.667678 -0.113884 -0.722696  0.074993  0.163021  0.253945  0.136000   \n",
       "4    0.849560 -0.063234 -0.435702 -0.031894  0.166287  0.081211  0.265267   \n",
       "5   -1.502955 -0.107799 -0.866191  0.114362  0.169591  0.249154  0.113528   \n",
       "6    0.133038 -0.117657 -0.567499  0.027753  0.176908  0.163342  0.187957   \n",
       "7    1.318257 -0.071643 -0.345541 -0.052795  0.179769  0.083850  0.293792   \n",
       "8   -0.283348 -0.103015 -0.641795  0.073479  0.182611  0.246341  0.157299   \n",
       "9    1.895305 -0.068355 -0.230015 -0.083006  0.186769  0.061792  0.261119   \n",
       "10   0.514299 -0.072816 -0.498048 -0.011688  0.164375  0.082324  0.277102   \n",
       "11  -0.873551 -0.097399 -0.759809  0.108238  0.160049  0.263482  0.140431   \n",
       "12  -0.075483 -0.115029 -0.611411  0.055081  0.165639  0.235318  0.158196   \n",
       "13   1.075357 -0.063546 -0.385750 -0.023755  0.183372  0.097742  0.263307   \n",
       "14  -0.477601 -0.115623 -0.682548  0.073435  0.172622  0.236267  0.157051   \n",
       "15  -1.654899 -0.146220 -0.894734  0.166227  0.230432  0.174525 -0.116998   \n",
       "16  -1.336595 -0.133057 -0.846287  0.132478  0.160553  0.270009  0.135434   \n",
       "17   0.683373 -0.057007 -0.466296 -0.012828  0.156769  0.102835  0.290548   \n",
       "18   1.476871 -0.064685 -0.315155 -0.066798  0.175437  0.067806  0.280387   \n",
       "19   1.749273 -0.057578 -0.269722 -0.083657  0.163028  0.080777  0.257926   \n",
       "20   0.397125 -0.097912 -0.524106 -0.012754  0.154751  0.096114  0.263920   \n",
       "21  -1.013776 -0.081470 -0.787294  0.100861  0.160227  0.241446  0.147231   \n",
       "22   0.030701 -0.110971 -0.583655  0.047569  0.179863  0.235871  0.152594   \n",
       "23   1.254558 -0.071353 -0.360542 -0.038882  0.165067  0.093027  0.272733   \n",
       "24   0.933700 -0.068137 -0.418969 -0.045687  0.173460  0.084107  0.290338   \n",
       "25  -0.767947 -0.102713 -0.737680  0.097074  0.168506  0.287392  0.132456   \n",
       "26  -0.387837 -0.116199 -0.665917  0.073970  0.170476  0.239490  0.139755   \n",
       "27   1.190268 -0.067688 -0.363618 -0.051090  0.195520  0.082093  0.283177   \n",
       "28  -0.163095 -0.106356 -0.622636  0.052519  0.168164  0.254853  0.141528   \n",
       "29  -1.253315 -0.103127 -0.830322  0.107261  0.166819  0.279360  0.145312   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "470  0.133042 -0.089310  1.067425 -0.789059 -2.160616 -0.014968 -2.293228   \n",
       "471 -1.656375 -0.217188 -0.853413 -0.176066  0.804638 -2.502862 -1.974881   \n",
       "472 -1.623418 -0.217465  0.069607 -0.311179 -0.052063 -1.092627 -1.355949   \n",
       "473 -1.596546 -0.025615  2.556524  0.502733 -0.080013  0.880506  2.698349   \n",
       "474  0.133041  0.175215 -0.152392  0.009210  0.210931 -0.017501 -1.035457   \n",
       "475  0.133042 -0.165930 -0.241991  0.706867  1.056090 -0.069680 -0.117728   \n",
       "476  0.133042 -0.089308 -0.434903 -0.094711  0.069790  0.001116 -0.528807   \n",
       "477 -1.650539 -0.217058  0.664486 -0.503465 -0.798782 -0.749863 -0.878547   \n",
       "478  1.273218 -0.063951  0.265955 -0.357085  0.073657 -1.246993 -0.626836   \n",
       "479  0.133039  0.758946 -0.131285  0.266408 -0.301009  0.511986 -0.325256   \n",
       "480  0.283350 -0.174020  5.047391  1.796492  4.968090  3.787573  0.532960   \n",
       "481  1.860270 -0.099581  0.278167 -0.377719 -0.189201 -0.555210 -0.016222   \n",
       "482  0.133042 -0.089308  0.292005 -0.574496 -0.664346 -0.874841 -2.467029   \n",
       "483 -1.621713 -0.035429 -0.122838  0.749354 -0.613611  0.300368  0.080455   \n",
       "484  1.350883 -0.150563  0.275447 -0.610988 -1.187957  0.322767 -0.132914   \n",
       "485 -1.180575 -0.123152  2.907867 -1.115742 -2.693739  0.648096 -0.331383   \n",
       "486  0.133042  0.032885 -0.526973  0.007483  0.147283  0.118151 -0.019811   \n",
       "487  0.236701 -0.145396  2.238088  0.853976 -1.014450  2.206291  1.014719   \n",
       "488 -1.583575 -0.215535  3.598550  0.134556  1.208280 -1.422176 -0.989019   \n",
       "489  0.221553 -0.123056  0.213674  0.207099  0.100949 -0.911325 -0.411949   \n",
       "490  1.596258 -0.056764 -0.297435 -0.128968  0.146292 -0.085855 -0.136808   \n",
       "491  0.222274 -0.087316 -0.551679  0.119204  1.091613 -1.031132 -1.243840   \n",
       "492  0.133042 -0.176036  3.534098  2.904141  7.325182  4.621767 -4.455538   \n",
       "493  0.133042 -0.089296 -0.315396  0.102468 -0.159061 -0.342280 -0.759242   \n",
       "494  0.133042 -0.177343  4.127070  3.944479  5.557127  0.264480 -2.368385   \n",
       "495  0.750809 -0.163656  0.196711  1.930867 -0.938710 -1.082400  1.238830   \n",
       "496 -1.176929 -0.169713  2.788176 -0.376023  0.468134  2.742898  3.194430   \n",
       "497 -1.178477 -0.087725  3.052644 -1.166082 -1.542982  0.692733 -0.100497   \n",
       "498  0.243507 -0.086838 -0.371057 -0.078158  0.385142 -0.233247  0.121167   \n",
       "499 -1.623378 -0.186678  1.094633 -0.336821 -1.288285 -0.309214 -0.827695   \n",
       "\n",
       "           F7        F8        F9 ...        F20       F21       F22  F23  \\\n",
       "0    0.086557  0.709156 -0.002445 ...   0.242455  0.370284 -0.141288  0.0   \n",
       "1   -0.137314 -0.024216  0.037240 ...  -0.034937  0.014576  0.006718  0.0   \n",
       "2   -0.044316  0.058963 -0.089999 ...   0.028158  0.036751  0.003005  0.0   \n",
       "3   -0.163521  0.024748  0.073749 ...  -0.043993 -0.034997 -0.002822  0.0   \n",
       "4    0.000376  0.061249 -0.013678 ...   0.025152 -0.004624  0.001506  0.0   \n",
       "5   -0.145230 -0.020457  0.161684 ...  -0.021707 -0.058405 -0.002340  0.0   \n",
       "6   -0.089073  0.076717  0.014318 ...   0.003420  0.025551 -0.009429  0.0   \n",
       "7   -0.042039  0.080866 -0.025244 ...   0.027648  0.034718 -0.009345  0.0   \n",
       "8   -0.171478 -0.005351  0.005604 ...  -0.087518 -0.000048 -0.013193  0.0   \n",
       "9   -0.055599  0.060888 -0.086904 ...  -0.020194  0.054259  0.004991  0.0   \n",
       "10   0.022101  0.037191  0.030170 ...   0.061364  0.015064  0.007478  0.0   \n",
       "11  -0.141002 -0.018383  0.058167 ...  -0.076400 -0.031953  0.002849  0.0   \n",
       "12  -0.187898  0.042133  0.022736 ...  -0.067217 -0.068659  0.002509  0.0   \n",
       "13  -0.013021  0.064037 -0.049009 ...   0.022090  0.018572  0.002935  0.0   \n",
       "14  -0.185262  0.010568  0.071048 ...  -0.042749 -0.000314 -0.002777  0.0   \n",
       "15  -0.018955  0.629269 -0.087207 ...   0.255559  0.261679 -0.122350  0.0   \n",
       "16  -0.134084 -0.020106  0.086339 ...  -0.055631 -0.039642  0.014067  0.0   \n",
       "17   0.027775  0.052088  0.004298 ...   0.015408  0.021866  0.004519  0.0   \n",
       "18   0.007585  0.074943 -0.035172 ...  -0.054066  0.054247 -0.003528  0.0   \n",
       "19  -0.052367  0.057484 -0.079618 ...   0.020532  0.042431 -0.013424  0.0   \n",
       "20  -0.019422  0.048040  0.084348 ...   0.009481  0.010672 -0.003889  0.0   \n",
       "21  -0.126325 -0.014480  0.095005 ...  -0.017579 -0.022721 -0.002477  0.0   \n",
       "22  -0.200561 -0.005159 -0.020010 ...  -0.053130  0.011596 -0.001963  0.0   \n",
       "23  -0.011493  0.067027 -0.027146 ...  -0.040285  0.022669  0.001072  0.0   \n",
       "24   0.007140  0.055540 -0.044768 ...   0.026204  0.009309 -0.003757  0.0   \n",
       "25  -0.162675  0.003984  0.052658 ...  -0.051350 -0.009788 -0.001402  0.0   \n",
       "26  -0.190270  0.012911  0.041860 ...  -0.095873 -0.010359 -0.010611  0.0   \n",
       "27  -0.025215  0.067247 -0.042811 ...  -0.007806  0.037989 -0.003941  0.0   \n",
       "28  -0.201869 -0.003489 -0.017821 ...  -0.051644 -0.004569 -0.004444  0.0   \n",
       "29  -0.166145  0.002279  0.123472 ...  -0.027335 -0.018509  0.007984  0.0   \n",
       "..        ...       ...       ... ...        ...       ...       ...  ...   \n",
       "470  0.109642  0.870958  0.167895 ...   0.607804 -0.263787 -0.200525  0.0   \n",
       "471  1.368791 -0.592442  0.859528 ...   0.172206  0.186720  0.038297  0.0   \n",
       "472  1.383900  2.458397 -1.046876 ...  -0.036605 -0.390334  0.038601  0.0   \n",
       "473 -0.878477 -3.003623 -0.187019 ...  -0.726811  0.155596 -0.108954  0.0   \n",
       "474 -0.254390 -0.753335  0.701747 ...  -0.414808 -0.062982  0.187933  0.0   \n",
       "475 -1.324710 -0.637359 -1.161618 ...  -1.493787  0.864701 -0.217579  0.0   \n",
       "476  0.353971  0.968753  0.391811 ...  -0.281382  0.184815  0.002441  0.0   \n",
       "477  0.188310  0.727441  0.665524 ...  -0.892049  0.522632 -0.103583  0.0   \n",
       "478  0.454708 -1.789744  0.653012 ...   0.012233  0.048091 -0.075814  0.0   \n",
       "479 -0.183448  0.682492 -0.128210 ...  -0.018010  0.940527 -0.007533  0.0   \n",
       "480  0.076475  1.816624  1.550758 ...   0.333568 -0.218529  0.232078  0.0   \n",
       "481  0.167106 -0.245095 -0.543269 ...  -0.024851 -0.230061  0.027755  0.0   \n",
       "482 -0.513609 -1.179852  0.589447 ...  -0.470555 -0.054498 -0.085535  0.0   \n",
       "483  0.091149  1.720344 -1.284149 ...   1.064893 -0.300260 -0.073631  0.0   \n",
       "484 -0.735561 -1.135795 -0.447462 ...  -0.412983  0.405982  0.102469  0.0   \n",
       "485 -0.528247  1.214082  0.255205 ...   0.905014  0.657256  0.038321  0.0   \n",
       "486 -0.049573  0.227504  0.001409 ...   0.148554  0.137716 -0.029818  0.0   \n",
       "487 -4.675390 -2.119757 -1.689108 ...   0.356346 -1.057496 -0.161412  0.0   \n",
       "488  1.451999 -1.053684 -0.454761 ...  -0.805099  0.651075 -0.026061  0.0   \n",
       "489  0.945019 -1.997326 -0.646015 ...   0.602880  0.328858 -0.203946  0.0   \n",
       "490  0.316778 -0.401985 -0.118658 ...   0.432595  0.208880 -0.150806  0.0   \n",
       "491  1.300044  0.746313 -0.198830 ...   1.172896 -0.584876  0.062018  0.0   \n",
       "492 -5.128106 -1.575461  3.183713 ...  -0.591470  1.323733  0.386247  0.0   \n",
       "493 -0.008324 -1.646095 -1.496326 ...  -0.579557 -1.251087  0.043203  0.0   \n",
       "494  1.658897  0.789951  0.457322 ...   1.185804 -0.650613  0.087155  0.0   \n",
       "495  0.546870  0.746122 -1.064266 ...   0.279536 -0.088865 -0.092801  0.0   \n",
       "496 -0.733956  0.102665 -0.463179 ...  -0.874690 -0.696640 -0.231348  0.0   \n",
       "497  0.173832  0.197855 -0.847791 ...   0.721038 -0.792625 -0.053725  0.0   \n",
       "498  0.230343 -0.480261  0.147080 ...   1.493117  0.263444 -0.215467  0.0   \n",
       "499  0.232114  0.887893 -0.902763 ...   0.113802  0.161544  0.009760  0.0   \n",
       "\n",
       "     F24  F25  F26  F27  F28  F29  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9    0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "11   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "12   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "13   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "14   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "15   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "16   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "17   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "18   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "19   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "20   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "22   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "23   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "24   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "25   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "26   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "27   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "28   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "29   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  \n",
       "470  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "471  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "472  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "473  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "474  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "475  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "476  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "477  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "478  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "479  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "480  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "481  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "482  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "483  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "484  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "485  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "486  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "487  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "488  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "489  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "490  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "491  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "492  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "493  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "494  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "495  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "496  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "497  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "498  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "499  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[500 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_E = Feature_Hyperextractor(booster='gbtree',objective='binary:logistic',scorer='recall_score',# extract='IncrementalPCA',\n",
    "                            excluded_extractors=['KernelPCA'])\n",
    "F_E.fit(X_scaled, y_scaled)\n",
    "X_extracted, y_extracted = F_E.transform(X_scaled, y_scaled)\n",
    "# X_extracted = pd.DataFrame(F_E.extractor.fit_transform(X_scaled))\n",
    "print(f'\\033[1m{Fore.RED}Features Engineered from X_train:{Style.RESET_ALL}\\033[0m')\n",
    "X_extracted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mPerformance scores of Feature Extraction methods:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractors</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FactorAnalysis</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FastICA</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IncrementalPCA</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PCA</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TruncatedSVD</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Extractors  recall_score\n",
       "0                        None          0.96\n",
       "1              FactorAnalysis          0.98\n",
       "2                     FastICA          0.98\n",
       "3              IncrementalPCA          0.96\n",
       "4                         PCA          0.96\n",
       "5                TruncatedSVD          0.82\n",
       "6  LinearDiscriminantAnalysis          0.90"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Performance scores of Feature Extraction methods:{Style.RESET_ALL}\\033[0m')\n",
    "F_E.performance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mBest Feature Extraction Method:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractors</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FactorAnalysis</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FastICA</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Extractors  recall_score\n",
       "1  FactorAnalysis          0.98\n",
       "2         FastICA          0.98"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Best Feature Extraction Method:{Style.RESET_ALL}\\033[0m')\n",
    "F_E.best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"select\"></a>\n",
    "\n",
    "![Imgur](https://i.imgur.com/JTRAAnP.png)\n",
    "<center>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>Feature Selection</tt></span></h1>\n",
    "</center>\n",
    "<br>\n",
    "![Imgur](https://i.imgur.com/qmnAFp0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\">\n",
    "class&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Feature_Selector</span><i><span style=\"font-size:16px;font-weight:bold;color:#3366cc\">(booster='gbtree', objective='reg:logistic', random_state=69, test_size=0.2, best_params={}, scorer='Auto')</span></i>  \n",
    " \n",
    "\"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "    This feature selection transformer works only with the following boosters as base learners:\n",
    "        -gbtree\n",
    "        -dart\n",
    "                                                                            \n",
    "    Sklearn.metrics scoring functions can be passed to parameter:\n",
    "        scorer = 'scoring_function'\n",
    "    \n",
    "    Built-in XGBoost evaluation metrics will be used by passing:\n",
    "        scorer = 'Auto'                                                    \n",
    "    \n",
    "    This class requires that X (features or independent variables) has already been encoded and Imputed.\n",
    "    \n",
    "    Tuning requires the target (y) to also be passed.\n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit_transform method is given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Parameters:  </span>\n",
    "\n",
    "\n",
    "### best_params: dict, default={}\n",
    "* dictionary of parameters to be used as kwargs for XGBoost models to be used for evaluating performance\n",
    "\n",
    "### booster: str, default='gbtree'\n",
    "* specifies the xgboost booster to be used for the evaluation models ('gbtree','gblinear', or 'dart')  \n",
    "\n",
    "### objective: str, default='reg:logistic'\n",
    "* specifies the xgboost objective to be used for the evaluation models\n",
    "* see XGBoost documentation for more details\n",
    "\n",
    "### scorer: str, default='Auto'\n",
    "* determines the model evaluation metric used\n",
    "* if 'Auto', built-in XGBoost metrics will be used\n",
    "* otherwise, existic sklearn metrics can be passed as strings\n",
    "\n",
    "### random_state: int, default=69\n",
    "* random_state for train_test_split\n",
    "\n",
    "### test_size: int, default=0.2\n",
    "* test_size for train_test_split\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Attributes:  </span>\n",
    "\n",
    "### scoring_functions: dictionary\n",
    "* dictonary of metrics available for use\n",
    "\n",
    "### classification_objectives: list\n",
    "* list of XGBoost objectives considered as for classification\n",
    "\n",
    "### regression_objectives: list\n",
    "* list of XGBoost objectives considered as for regression\n",
    "\n",
    "### the_higher_the_better: list\n",
    "* list of metrics of which an increase means improvement in performance\n",
    "\n",
    "### the_lower_the_better: list\n",
    "* list of metrics of which a decrease means improvement in performance\n",
    "\n",
    "### need_encoded_y: list\n",
    "* list of metrics that require an encoded y\n",
    "* automatically dealt with my the class\n",
    "\n",
    "### the_lower_the_better: list\n",
    "* list of metrics of which a decrease means improvement in performance\n",
    "\n",
    "### performance_scores: pandas DataFrame\n",
    "* results of testing\n",
    "\n",
    "### best_perf: pandas DataFrame\n",
    "* highest performer/s\n",
    "\n",
    "### best_n: int\n",
    "* number of top features deemed best\n",
    "\n",
    "### selected_columns: list of str\n",
    "* list of names of features deemed best\n",
    "* used later for feature selection on test set or new data\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Methods:  </span>\n",
    "\n",
    "### .fit(X, y=None, target=None)  \n",
    "Performs tests to determine which features to keep.  \n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "* target:*string* (optional)  \n",
    "    * name of target column  \n",
    "\n",
    "### .transform(X, y=None, target=None)  \n",
    "Using results of tests, returns selected features.\n",
    "**Parameters:**\n",
    "* X: *array-like or pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * independent variables / features \n",
    "* y: *array-like or pandas DataFrame, shape (n_samples,)*    \n",
    "    * dependent variable / target  \n",
    "* target:*string* (optional)  \n",
    "    * name of target column  \n",
    "\n",
    "**Returns:**\n",
    "* X_extracted: *pandas DataFrame, shape (n_samples, n_features)*  \n",
    "    * selected independent variables / features\n",
    "* y_extracted: *pandas DataFrame, shape (n_samples,)*    \n",
    "    * untouched dependent variable\n",
    "\n",
    "### .fit_transform(X, y=None, target=None)  \n",
    "Streamlines fit() and transform() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Feature Selection Class # #\n",
    "\n",
    "class Feature_Selector:\n",
    "    \"\"\"This transformer is not compatible with the current sklearn Pipeline.\n",
    "    \n",
    "    This feature selection transformer works only with the following boosters as base learners:\n",
    "        -gbtree\n",
    "        -dart\n",
    "                                                                            \n",
    "    Sklearn.metrics scoring functions can be passed to parameter:\n",
    "        scorer = 'scoring_function'\n",
    "    \n",
    "    Built-in XGBoost evaluation metrics will be used by passing:\n",
    "        scorer = 'Auto'                                                    \n",
    "    \n",
    "    This class requires that X (features or independent variables) has already been encoded and Imputed.\n",
    "    \n",
    "    Tuning requires the target (y) to also be passed.\n",
    "        If target = 'insert target name' is passed, X = dataframe disregarding position of target column.\n",
    "        If the fit_transform method is given one positional argument (X) and target=None, \n",
    "            the class assumes that it is a DataFrame with the last\n",
    "            column as the target variable and the rest as the features.\"\"\"\n",
    "    \n",
    "    # Compatible model evaluation metrics: Sklearn metrics\n",
    "    scoring_functions = {'accuracy_score':accuracy_score,'f1_score':f1_score, 'hamming_loss':hamming_loss,\n",
    "        'jaccard_similarity_score':jaccard_similarity_score, 'log_loss':log_loss, 'matthews_corrcoef':matthews_corrcoef,\n",
    "                 'precision_score':precision_score, 'recall_score':recall_score, 'zero_one_loss':zero_one_loss,\n",
    "                'explained_variance_score':explained_variance_score, 'mean_absolute_error':mean_absolute_error,\n",
    "                 'mean_squared_error':mean_squared_error, 'mean_squared_log_error':mean_squared_log_error,\n",
    "                 'median_absolute_error':median_absolute_error, 'r2_score':r2_score}\n",
    "\n",
    "    classification_objectives = ['reg:logistic','binary:logistic', 'binary:logitraw',\n",
    "                                'multi:softmax', 'multi:softprob', 'rank:pairwise' ]\n",
    "    regression_objectives = ['reg:linear','count:poisson','reg:gamma', 'reg:tweedie']\n",
    "    \n",
    "    # metrics with which higher value = higher model performance\n",
    "    the_higher_the_better = ['accuracy_score','f1_score','jaccard_similarity_score',\n",
    "                                  'precision_score','recall_score',\n",
    "                                'explained_variance_score','r2_score','Best test auc',\n",
    "                                'Best test ndcg','Best test map']\n",
    "    # metrics with which lower value = higher model performance\n",
    "    the_lower_the_better = ['hamming_loss', 'log_loss','matthews_corrcoef','zero_one_loss','mean_absolute_error',\n",
    "                           'mean_squared_error','mean_squared_log_error','median_absolute_error',\n",
    "                           'Best test error','Best test rmse','Best test mae','Best test log loss',\n",
    "                           'Best test merror','Best test mlogloss']\n",
    "\n",
    "    # metrics that require encoded target variable\n",
    "    need_encoded_y = ['recall_score','precision_score','f1_score']\n",
    "    \n",
    "    booster='N/A'\n",
    "    best_params='N/A'\n",
    "    objective='N/A'\n",
    "    random_state='N/A'\n",
    "    test_size='N/A'\n",
    "    scorer='N/A'\n",
    "\n",
    "    def __init__(self, booster='gbtree', objective='reg:logistic', \n",
    "                 random_state=69, test_size=0.2, best_params={}, scorer='Auto',\n",
    "                \n",
    "                 # values passed for these parameters will never be used unless: class.parameter is made None\n",
    "                 scoring_functions=None, classification_objectives=None, regression_objectives=None,\n",
    "                the_higher_the_better=None, the_lower_the_better=None, need_encoded_y=None):\n",
    "        \n",
    "        \n",
    "        # assigning parameters as instance variables\n",
    "        varses = list(vars(Feature_Selector).keys())\n",
    "        self.variables = varses[len(varses) - varses[::-1].index('__doc__') : varses.index('__init__')]\n",
    "        class_name = \"Feature_Selector\"+\".\"\n",
    "        for v in self.variables:\n",
    "            # if the class variable for the argument is not empty, assign its value as the instance variable\n",
    "            if eval(\"%s != 'N/A'\" % (class_name+v)) is True:\n",
    "                exec(\"self.%s = %s\" % (v,class_name+v))\n",
    "            # if the class variable is empty, assign to instance the value passed as argument during instantiation\n",
    "            else:\n",
    "                exec(\"self.%s = %s\" % (v, v))\n",
    "        \n",
    "    \n",
    "        # overriding colsample parameters\n",
    "        self.best_params['colsample_bytree'] = 1\n",
    "        self.best_params['colsample_bylevel'] = 1\n",
    "        \n",
    "        # determining the model build used for feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            if self.objective in Feature_Selector.classification_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBClassifier(**self.best_params)\n",
    "            elif self.objective in Feature_Selector.regression_objectives:\n",
    "                self.best_params['objective'] = self.objective\n",
    "                if 'booster' in list(self.best_params.keys()):\n",
    "                    del self.best_params['booster']\n",
    "                self.model = xgb.XGBRegressor(**self.best_params)\n",
    "        elif self.booster == 'dart':\n",
    "            self.best_params['booster'] = self.booster\n",
    "            self.best_params['objective'] = self.objective\n",
    "        \n",
    "        \n",
    "        # making sure there is a scorer\n",
    "        if self.scorer not in Feature_Selector.the_higher_the_better\\\n",
    "                            and scorer not in Feature_Selector.the_lower_the_better:\n",
    "            self.scorer = 'Auto'\n",
    "        else:\n",
    "            self.performance_scores = None\n",
    "            self.best_perf = None\n",
    "            self.best_threshold = None\n",
    "    \n",
    "    def fit(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = X.drop(self.target, axis =1)\n",
    "        \n",
    "        # Encode labels if string when needed\n",
    "        if ((self.booster == 'dart' or self.booster == 'gblinear')\\\n",
    "                        and self.objective in Feature_Selector.classification_objectives)\\\n",
    "                                                                or self.scorer in self.need_encoded_y:\n",
    "            if y.iloc[:,0].dtype == object:\n",
    "                Target_is_string = True\n",
    "            else:\n",
    "                Target_is_string = False\n",
    "            \n",
    "            if Target_is_string == True:\n",
    "                L_E = LabelEncoder()\n",
    "                L_E = L_E.fit(y.iloc[:,0].astype(str))\n",
    "                y.iloc[:,0] = L_E.transform(y.iloc[:,0].astype(str))\n",
    "\n",
    "        # fit model on all training data            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                            test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "        \n",
    "        # Initialize lists of building performance metrics table\n",
    "        Threshold = []\n",
    "        ns = []\n",
    "        Scores = []\n",
    "        \n",
    "        # tree boosters: feature selection\n",
    "        if self.booster == 'gbtree':\n",
    "            self.test_model = self.model\n",
    "            self.test_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "            # Fit model using each importance as a threshold\n",
    "            thresholds = sorted(self.test_model.feature_importances_)\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                # select features using threshold\n",
    "                selection = SelectFromModel(self.test_model, threshold=thresh, prefit=True)\n",
    "                select_X_train = selection.transform(X_train)\n",
    "                select_X_test = selection.transform(X_test)\n",
    "                # train model\n",
    "                if self.objective in Feature_Selector.classification_objectives:\n",
    "                    estimator = xgb.XGBClassifier\n",
    "                if self.objective in Feature_Selector.regression_objectives:\n",
    "                    estimator = xgb.XGBRegressor\n",
    "                selection_model = estimator(**self.best_params)\n",
    "                selection_model.fit(select_X_train, y_train.values.ravel(),\n",
    "                                   eval_set=[(select_X_test,y_test.values.ravel())], verbose=False)\n",
    "                \n",
    "                # eval model\n",
    "                # using built-in evaluation metrics automatically matched with objective\n",
    "                if self.scorer == 'Auto':\n",
    "                    result = selection_model.evals_result()\n",
    "                    scorer_used = list(result['validation_0'].keys())[0]\n",
    "                    #score = np.mean(result['validation_1'][scorer_used])\n",
    "                    score = result['validation_0'][scorer_used][-1]\n",
    "                    Scores.append(score)\n",
    "                    ns.append(len(pd.DataFrame(select_X_train).columns))\n",
    "                    Threshold.append(thresh)\n",
    "\n",
    "                # using Sklearn metrics\n",
    "                else:\n",
    "                    scoring_function = Feature_Selector.scoring_functions[self.scorer]\n",
    "                    select_X_test = selection.transform(X_test)\n",
    "                    y_pred = selection_model.predict(select_X_test)\n",
    "                    if y_pred.dtype != object:\n",
    "                        predictions = [round(value) for value in y_pred]\n",
    "                        y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                    else:\n",
    "                        predictions = y_pred\n",
    "                        y_test_used = y_test.iloc[:,0]\n",
    "                    score = scoring_function(y_test_used, predictions)\n",
    "                    if self.scorer == 'accuracy_score':\n",
    "                        score = score*100.00\n",
    "                    Scores.append(score)\n",
    "                    Threshold.append(thresh)\n",
    "                    ns.append(select_X_train.shape[1])\n",
    "        \n",
    "        # dart boosters: feature selection\n",
    "        elif self.booster == 'dart':\n",
    "            dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "            \n",
    "            # train the dart booster model\n",
    "            xg_reg = xgb.train(dtrain=dtrain, params=self.best_params, num_boost_round=10)\n",
    "            importance = xg_reg.get_fscore()\n",
    "            importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            importance = pd.DataFrame(importance)\n",
    "        \n",
    "            for value in range(1, len(importance)):\n",
    "                columns = importance.iloc[:,0][:value]\n",
    "                dtrain_trim = xgb.DMatrix(data=X_train.loc[:,columns], label=y_train)\n",
    "                dtest_trim = xgb.DMatrix(data=X_test.loc[:,columns], label=y_test)\n",
    "                \n",
    "                # eval model\n",
    "                # using built-in evaluation metrics automatically matched with objective\n",
    "                if self.scorer == 'Auto':\n",
    "                    result = {}\n",
    "                    selection_model = xgb.train(dtrain=dtrain_trim, params=self.best_params,\n",
    "                                                evals=[(dtest_trim, 'eval')], evals_result=result,\n",
    "                                               verbose_eval=False)\n",
    "                    scorer_used = list(result['eval'].keys())[0]\n",
    "                    #score = np.mean(result['eval'][scorer_used])\n",
    "                    score = result['eval'][scorer_used][-1]\n",
    "                    Scores.append(score)\n",
    "                    ns.append(len(columns))\n",
    "                    Threshold.append(importance.iloc[value-1,1])\n",
    "                \n",
    "                # using Sklearn metrics\n",
    "                else:\n",
    "                    scoring_function = Feature_Selector.scoring_functions[self.scorer]\n",
    "\n",
    "                    selection_model = xgb.train(dtrain=dtrain_trim, params=self.best_params)\n",
    "                    y_pred = selection_model.predict(dtest_trim)\n",
    "                    if y_pred.dtype != object:\n",
    "                        predictions = [round(value) for value in y_pred]\n",
    "                        y_test_used = [round(value) for value in y_test.iloc[:,0]]\n",
    "                    else:\n",
    "                        predictions = y_pred\n",
    "                        y_test_used = y_test.iloc[:,0]\n",
    "                    score = scoring_function(y_test_used, predictions)\n",
    "                    if self.scorer == 'accuracy_score':\n",
    "                        score = score*100.00\n",
    "                    Scores.append(score)\n",
    "                    ns.append(len(columns))\n",
    "                    Threshold.append(importance.iloc[value-1,1])\n",
    "        \n",
    "        # building table of performance scores\n",
    "        if self.scorer == 'Auto':\n",
    "            self.scorer = 'Best test ' + scorer_used\n",
    "        self.performance_scores = pd.DataFrame()\n",
    "        self.performance_scores['Threshold'] = Threshold\n",
    "        self.performance_scores['n'] = ns\n",
    "        self.performance_scores[self.scorer] = Scores\n",
    "\n",
    "        # Best cut-off of top features: minumum number giving the max possible model performance \n",
    "        if self.scorer in Feature_Selector.the_higher_the_better:\n",
    "            self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                     == max(self.performance_scores[self.scorer])]\n",
    "            self.best_threshold = self.best_perf[(self.best_perf['n'] == min(self.best_perf['n']))].drop_duplicates()\n",
    "        elif self.scorer in Feature_Selector.the_lower_the_better:\n",
    "            self.best_perf = self.performance_scores[self.performance_scores[self.scorer]\\\n",
    "                                                     == min(self.performance_scores[self.scorer])]\n",
    "            self.best_threshold = self.best_perf[(self.best_perf['n'] == min(self.best_perf['n']))].drop_duplicates()\n",
    "        \n",
    "        # store the best number of features\n",
    "        self.best_n = self.best_threshold.n.iloc[0]\n",
    "        \n",
    "        # saving selected columns for use on .predict()\n",
    "        if self.booster == 'gbtree':\n",
    "            self.selected_columns = X.columns[np.argsort(self.test_model.feature_importances_)\\\n",
    "                                          [-(self.best_n):]]\n",
    "        if self.booster == 'dart':\n",
    "            self.selected_columns = list(columns[:self.best_n])   \n",
    "        \n",
    "        # reverse label encoding\n",
    "        if ((self.booster == 'dart' or self.booster == 'gblinear')\\\n",
    "                        and self.objective in Feature_Selector.classification_objectives)\\\n",
    "                                                                or self.scorer in self.need_encoded_y:\n",
    "            if Target_is_string == True:\n",
    "                y.iloc[:,0] = L_E.inverse_transform(y.iloc[:,0])\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None, target=None):\n",
    "        if y is None:\n",
    "            if target is None:\n",
    "                y = pd.DataFrame(X.iloc[:,-1])\n",
    "                X = pd.DataFrame(X.drop(X.columns[[-1,]], axis=1))\n",
    "            else:\n",
    "                y = pd.DataFrame(X[target])\n",
    "                X = X.drop(target, axis =1)\n",
    "        # Drop unwanted features\n",
    "        X = X[self.selected_columns]\n",
    "        \n",
    "        return pd.DataFrame(X), pd.DataFrame(y)\n",
    "    \n",
    "    def fit_transform(self, X, y=None, target=None):\n",
    "        self.fit(X, y, target)\n",
    "        return self.transform(X, y, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#b22222\"> Feature_Selector in action  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mFeatures Selected from X_train:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F12</th>\n",
       "      <th>F2</th>\n",
       "      <th>F13</th>\n",
       "      <th>F8</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141758</td>\n",
       "      <td>-0.547101</td>\n",
       "      <td>0.118107</td>\n",
       "      <td>0.709156</td>\n",
       "      <td>-0.118402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122173</td>\n",
       "      <td>-0.797599</td>\n",
       "      <td>-0.088937</td>\n",
       "      <td>-0.024216</td>\n",
       "      <td>-0.092613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044194</td>\n",
       "      <td>-0.298098</td>\n",
       "      <td>-0.092114</td>\n",
       "      <td>0.058963</td>\n",
       "      <td>-0.063014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085675</td>\n",
       "      <td>-0.722696</td>\n",
       "      <td>-0.045364</td>\n",
       "      <td>0.024748</td>\n",
       "      <td>-0.113884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073172</td>\n",
       "      <td>-0.435702</td>\n",
       "      <td>-0.139703</td>\n",
       "      <td>0.061249</td>\n",
       "      <td>-0.063234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.206897</td>\n",
       "      <td>-0.866191</td>\n",
       "      <td>-0.125036</td>\n",
       "      <td>-0.020457</td>\n",
       "      <td>-0.107799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.567499</td>\n",
       "      <td>-0.066664</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>-0.117657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.050382</td>\n",
       "      <td>-0.345541</td>\n",
       "      <td>-0.113288</td>\n",
       "      <td>0.080866</td>\n",
       "      <td>-0.071643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.033116</td>\n",
       "      <td>-0.641795</td>\n",
       "      <td>-0.015484</td>\n",
       "      <td>-0.005351</td>\n",
       "      <td>-0.103015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.089322</td>\n",
       "      <td>-0.230015</td>\n",
       "      <td>-0.110771</td>\n",
       "      <td>0.060888</td>\n",
       "      <td>-0.068355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        F12        F2       F13        F8        F1\n",
       "0  0.141758 -0.547101  0.118107  0.709156 -0.118402\n",
       "1  0.122173 -0.797599 -0.088937 -0.024216 -0.092613\n",
       "2  0.044194 -0.298098 -0.092114  0.058963 -0.063014\n",
       "3  0.085675 -0.722696 -0.045364  0.024748 -0.113884\n",
       "4  0.073172 -0.435702 -0.139703  0.061249 -0.063234\n",
       "5  0.206897 -0.866191 -0.125036 -0.020457 -0.107799\n",
       "6  0.041691 -0.567499 -0.066664  0.076717 -0.117657\n",
       "7  0.050382 -0.345541 -0.113288  0.080866 -0.071643\n",
       "8  0.033116 -0.641795 -0.015484 -0.005351 -0.103015\n",
       "9 -0.089322 -0.230015 -0.110771  0.060888 -0.068355"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_selector = Feature_Selector(booster='gbtree',objective='binary:logistic', scorer='recall_score')\n",
    "feat_selector = feat_selector.fit(X_extracted, y_extracted)\n",
    "X_selected, y_selected = feat_selector.transform(X_extracted, y_extracted)\n",
    "print(f'\\033[1m{Fore.RED}Features Selected from X_train:{Style.RESET_ALL}\\033[0m')\n",
    "X_selected.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mPerformance Scores for different feature selections:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>n</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008547</td>\n",
       "      <td>22</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.014245</td>\n",
       "      <td>21</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.014245</td>\n",
       "      <td>21</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.014245</td>\n",
       "      <td>21</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.017094</td>\n",
       "      <td>18</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.019943</td>\n",
       "      <td>17</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.022792</td>\n",
       "      <td>16</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.025641</td>\n",
       "      <td>15</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.028490</td>\n",
       "      <td>14</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.031339</td>\n",
       "      <td>13</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.034188</td>\n",
       "      <td>12</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.037037</td>\n",
       "      <td>11</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.051282</td>\n",
       "      <td>10</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.056980</td>\n",
       "      <td>9</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.059829</td>\n",
       "      <td>8</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.065527</td>\n",
       "      <td>7</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.071225</td>\n",
       "      <td>6</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.082621</td>\n",
       "      <td>4</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.085470</td>\n",
       "      <td>3</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.088319</td>\n",
       "      <td>2</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.094017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold   n  recall_score\n",
       "0    0.000000  30          0.96\n",
       "1    0.000000  30          0.96\n",
       "2    0.000000  30          0.96\n",
       "3    0.000000  30          0.96\n",
       "4    0.000000  30          0.96\n",
       "5    0.000000  30          0.96\n",
       "6    0.000000  30          0.96\n",
       "7    0.000000  30          0.96\n",
       "8    0.008547  22          0.96\n",
       "9    0.014245  21          0.96\n",
       "10   0.014245  21          0.96\n",
       "11   0.014245  21          0.96\n",
       "12   0.017094  18          0.96\n",
       "13   0.019943  17          0.96\n",
       "14   0.022792  16          0.96\n",
       "15   0.025641  15          0.96\n",
       "16   0.028490  14          0.96\n",
       "17   0.031339  13          0.96\n",
       "18   0.034188  12          0.96\n",
       "19   0.037037  11          0.96\n",
       "20   0.051282  10          0.96\n",
       "21   0.056980   9          0.96\n",
       "22   0.059829   8          0.96\n",
       "23   0.065527   7          0.96\n",
       "24   0.071225   6          0.96\n",
       "25   0.076923   5          0.98\n",
       "26   0.082621   4          0.96\n",
       "27   0.085470   3          0.96\n",
       "28   0.088319   2          0.92\n",
       "29   0.094017   1          0.80"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Performance Scores for different feature selections:{Style.RESET_ALL}\\033[0m')\n",
    "feat_selector.performance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mBest number of top features:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>n</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  n  recall_score\n",
       "25   0.076923  5          0.98"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'\\033[1m{Fore.RED}Best number of top features:{Style.RESET_ALL}\\033[0m')\n",
    "feat_selector.best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"verdict\"></a>\n",
    "<h1><span style=\"font-size:48px;font-family:Times New Roman,Times,serif\"><tt>VERDICT:</tt></span></h1>\n",
    "\n",
    "\n",
    "<span style=\"font-size:30px;font-weight:bold;color:#3366cc\">The final tests reached a </span><span style=\"font-size:30px;font-weight:bold;color:#b22222\"> recall score of 98% </span><span style=\"font-size:30px;font-weight:bold;color:#3366cc\"> even before Hyperparameter Tuning has been performed!!! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>\n",
    "<h1><span style=\"font-size:35px;font-family:Times New Roman,Times,serif\"><tt>Applying preprocessing statistics learned from X_train to X_test::</tt></span></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to view/hide the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mPreprocessed X_test ready to use for prediction:\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F12</th>\n",
       "      <th>F2</th>\n",
       "      <th>F13</th>\n",
       "      <th>F8</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.498119</td>\n",
       "      <td>-0.563105</td>\n",
       "      <td>-0.771356</td>\n",
       "      <td>0.407072</td>\n",
       "      <td>-0.155116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.562073</td>\n",
       "      <td>-0.549863</td>\n",
       "      <td>0.188044</td>\n",
       "      <td>-0.152795</td>\n",
       "      <td>-0.173757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.340447</td>\n",
       "      <td>-0.257558</td>\n",
       "      <td>0.095124</td>\n",
       "      <td>0.213554</td>\n",
       "      <td>-0.136760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.239110</td>\n",
       "      <td>-0.264196</td>\n",
       "      <td>-0.030727</td>\n",
       "      <td>0.146990</td>\n",
       "      <td>-0.139136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.016318</td>\n",
       "      <td>-0.885908</td>\n",
       "      <td>0.016873</td>\n",
       "      <td>-0.164912</td>\n",
       "      <td>-0.130859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.087781</td>\n",
       "      <td>-0.901105</td>\n",
       "      <td>-0.661568</td>\n",
       "      <td>-0.200278</td>\n",
       "      <td>-0.208367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.521040</td>\n",
       "      <td>-0.733932</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>0.116841</td>\n",
       "      <td>-0.193123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.606422</td>\n",
       "      <td>-0.519089</td>\n",
       "      <td>-0.831701</td>\n",
       "      <td>-0.466483</td>\n",
       "      <td>-0.090066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.384194</td>\n",
       "      <td>-0.570086</td>\n",
       "      <td>-0.881533</td>\n",
       "      <td>0.310844</td>\n",
       "      <td>-0.147488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.699253</td>\n",
       "      <td>-0.452530</td>\n",
       "      <td>-0.248756</td>\n",
       "      <td>0.314546</td>\n",
       "      <td>0.045898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        F12        F2       F13        F8        F1\n",
       "0  1.498119 -0.563105 -0.771356  0.407072 -0.155116\n",
       "1 -0.562073 -0.549863  0.188044 -0.152795 -0.173757\n",
       "2 -0.340447 -0.257558  0.095124  0.213554 -0.136760\n",
       "3  0.239110 -0.264196 -0.030727  0.146990 -0.139136\n",
       "4 -0.016318 -0.885908  0.016873 -0.164912 -0.130859\n",
       "5 -0.087781 -0.901105 -0.661568 -0.200278 -0.208367\n",
       "6 -0.521040 -0.733932  0.106796  0.116841 -0.193123\n",
       "7 -0.606422 -0.519089 -0.831701 -0.466483 -0.090066\n",
       "8  0.384194 -0.570086 -0.881533  0.310844 -0.147488\n",
       "9  0.699253 -0.452530 -0.248756  0.314546  0.045898"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imputing X_test with the same statistics\n",
    "X_test_processing = Imputation_Nation(X_fillers=I_N.X_fillers).fit_transform(X_test)\n",
    "\n",
    "# scaling X_test with the same statistics\n",
    "if FHS.scaler is not None:\n",
    "    X_test_processing = FHS.scaler.transform(X_test_processing)\n",
    "    \n",
    "# extracting the same features from X_test\n",
    "if F_E.extractor is not None:\n",
    "    X_test_processing = pd.DataFrame(F_E.extractor.transform(X_test_processing))\n",
    "    X_test_processing.columns = ['F'+str(col) for col in X_test_processing.columns]\n",
    "\n",
    "# selecting the same features from X_test\n",
    "X_test_processing = pd.DataFrame(X_test_processing).loc[:,feat_selector.selected_columns]\n",
    "print(f'\\033[1m{Fore.RED}Preprocessed X_test ready to use for prediction:{Style.RESET_ALL}\\033[0m')\n",
    "X_test_processing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Sources:  \n",
    "* [Impputation and Encoding Header](https://healthunlimitedbiz.files.wordpress.com/2015/07/puzzle-w-missing-pieces.jpg)  \n",
    "* [Resampling Header](http://www.guoguiyan.com/laboratory-wallpapers/68519704.html)  \n",
    "- [Feature Scaling Header](https://www.wzwlh.com/wealth-builder/)  \n",
    "- [Feature Extraction Header](https://dop4.deviantart.com/art/Human-Transmutation-Circle-299877050)  \n",
    "- [Feature Selection Header](https://www.bayarea.com/uncategorized/5869/)  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
